{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# CME538 - Introduction to Data Science\n",
    "## Assignment 7 - Email Spam Classification\n",
    "\n",
    "### Learning Objectives\n",
    "After completing this assignment, you should be comfortable:\n",
    "\n",
    "- Feature engineering\n",
    "- Using sklearn to build simple and more complex linear models\n",
    "- Identifying informative variables through EDA\n",
    "- Classification using logistic regression\n",
    "- Classification metrics\n",
    "\n",
    "### Marking Breakdown\n",
    "\n",
    "Question | Points\n",
    "--- | ---\n",
    "Question 1a | 1\n",
    "Question 1b | 1\n",
    "Question 1c | 1\n",
    "Question 2a | 1\n",
    "Question 2b | 1\n",
    "Question 2c | 1\n",
    "Question 2d | 1\n",
    "Question 2e | 1\n",
    "Question 2f | 1\n",
    "Question 2g | 1\n",
    "Question 2h | 1\n",
    "Question 2i | 1\n",
    "Question 2j | 1\n",
    "Question 3a | 1\n",
    "Question 3b | 1\n",
    "Total | 15\n",
    "\n",
    "One of the following marks below will be added to the **Total** above.\n",
    "\n",
    "### Code Quality\n",
    "\n",
    "| Rank | Points | Description |\n",
    "| :-- | :-- | :-- |\n",
    "| Youngling | 1 | Code is unorganized, variables names are not descriptive, redundant, memory-intensive, computationally-intensive, uncommented, error-prone, difficult to understand. |\n",
    "| Padawan | 2 | Code is organized, variables names are descriptive, satisfactory utilization of memory and computational resources, satisfactory commenting, readable. |\n",
    "| Jedi | 3 | Code is organized, easy to understand, efficient, clean, a pleasure to read. #cleancode |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Setup Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Import 3rd party libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import local libraries\n",
    "from threshold_prediction_plot import threshold_prediction_plot, sigmoid\n",
    "\n",
    "# Configure Notebook\n",
    "%matplotlib inline\n",
    "plt.style.use('fivethirtyeight')\n",
    "sns.set_context(\"notebook\")\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Have you ever wondered how Google intercepts all of those spam emails so they don't end up in your inbox? In this assignment, you will build a model to classify emails as `spam` or `not-spam`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# The Data\n",
    "The data can be found in the file `emails.csv` in the root director. The `csv` contains the following columns.\n",
    "\n",
    "Columns of `emails.csv` include:\n",
    "- `id`: An identifier for the training example\n",
    "- `subject`: The subject of the email\n",
    "- `email`:  The text of the email\n",
    "- `label`: 1 if the email is `spam`, 0 if the email is `not-spam`\n",
    "\n",
    "The file `emails.csv` contains 7513 rows where each row is an email.\n",
    "\n",
    "First, let's import `emails.csv` to a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "email_data = pd.read_csv('emails.csv', index_col=0)\n",
    "email_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Data Cleaning and Pre-processing\n",
    "First, let's convert the email and subject text to lower case. This is based on the assumption that what differentiates `spam` emails from `not-spam` emails is case-insensitive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "email_data['subject'] = email_data['subject'].str.lower()\n",
    "email_data['email'] = email_data['email'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Next, let's check if our data contains any missing values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print('Missingness before imputation:')\n",
    "print(email_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can see there are 6 missing values for `'subject'`. This means that a subject was not included in the email and therefore, we can we replace missing values with appropriate filler values (i.e., NaN values in the subject or email columns will be replaced with empty strings `''`). Note that while there are no missing values in the `'label'` column, we should be careful when replacing missing value labels. Doing so without consideration may introduce significant bias into our model when fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "email_data[['subject', 'email']] = email_data[['subject', 'email']].fillna('')\n",
    "print('Missingness after imputation:')\n",
    "print(email_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Lastly, let's split `email_data` into training and validation datasets. We'll need this validation data to assess the performance of our classifier once we are finished training. Note that we have set the seed (random_state) to `0`. This will produce a pseudo-random sequence of random numbers that is the same for everyone. Do not modify this in the following questions.\n",
    "\n",
    "You'll notice that we've added an argument for `'stratify'` and set it equal to our training target `'label'`. This is done to maintain the proportion of each class in `train` and `val`. If 20% of the emails are `spam` in the dataset, then we want 20% of the emails in the `train` and `val` datasets to be also be `spam`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, val = train_test_split(email_data, test_size=0.1, random_state=0, stratify=email_data['label'])\n",
    "print('Train:', train.shape, 'Test:', val.shape)\n",
    "print('Train spam proportion: {:.2f} %'.format(train['label'][train['label'] == 1].shape[0] / train.shape[0] * 100))\n",
    "print('Test spam proportion: {:.2f} %'.format(val['label'][val['label'] == 1].shape[0] / val.shape[0] * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 1. Feature Engineering\n",
    "We would like to take the text of an email and predict whether the email is `spam` or `not-spam`. This is a classification problem and we will be using logistic regression to train a classifier. Recall that to train a logistic regression model, we need an `nD` numeric feature array `X` and a `1D` array of corresponding binary labels `y`. Unfortunately, our data are text, not numbers. Therefore, we will need to extract some numeric features from the raw email text. Each row of `X` is an email and each column of `X` contains one feature for all the emails. We'll guide you through creating a simple feature, and you'll create more interesting ones as you try to increase the accuracy of your model during the `Project 1`. \n",
    "\n",
    "## Question 1a\n",
    "In the cell below, we have printed the text of the `'email'` column for a `not-spam` and a `spam` email in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "index = 1  # You can change this to see other examples\n",
    "not_spam = train.loc[train['label'] == 0, 'email'].iloc[index]\n",
    "spam = train.loc[train['label'] == 1, 'email'].iloc[index]\n",
    "print('\\n--------\\nNOT SPAM\\n--------')\n",
    "print(not_spam)\n",
    "print('\\n----\\nSPAM\\n----')\n",
    "print(spam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Discuss one thing you notice that is different between `spam` emails and `not-spam` emails that might help with the identification of `spam`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "*Type your answer here, replacing this text.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Question 1b\n",
    "Create a function called `word_detector` that takes in a list of words and a pandas Series of email texts. It should output a DataFrame containing one row for each email text. The row should contain either a 0 or a 1 for each word in the list: 0 if the word doesn't appear in the text and 1 if the word does.\n",
    "\n",
    "The following code\n",
    "```python \n",
    "word_detector(['hello', 'bye', 'world'], \n",
    "              pd.Series(['hello', 'hello worldhello']))\n",
    "```\n",
    "should output the following DataFrame.\n",
    "<br>\n",
    "<img src=\"images/q1b.png\" alt=\"drawing\" width=\"150\"/>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def word_detector(words, texts):\n",
    "    \n",
    "    \"\"\"\n",
    "    Returns a DataFrame with detections of words.\n",
    "\n",
    "    Parameters:\n",
    "        words (list): A list of words to look for.\n",
    "        texts (Series): A series of strings to search in.\n",
    "\n",
    "    Returns:\n",
    "        (DataFrame): A DataFrame with len(words) columns and texts.shape[0] rows.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Write your code here\n",
    "    ...\n",
    "                                  \n",
    "    return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's check it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "word_detector(['hello', 'bye', 'world'], \n",
    "              pd.Series(['hello', 'hello worldhello']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "And, we can check if it works for our `'email'` column in `train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "word_detector(['hello', 'bye', 'world'], train['email']).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Is is important that the index values output from `word_detector()` match with the indices in `train`. Let's check quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Question 1c\n",
    "We need to identify some features that allow us to distinguish `spam` emails from `not-spam` emails. One idea is to compare the distribution of a single feature in `spam` emails to the distribution of the same feature in `not-spam` emails. If the feature is itself a binary indicator, such as whether a certain word occurs in the text, this amounts to comparing the proportion of `spam` emails with the word to the proportion of `not-spam` emails with the word.\n",
    "\n",
    "The following plot (which was created using `sns.barplot`) compares the proportion of emails in each class containing a particular set of words.\n",
    "```python\n",
    "words = ['body', 'business', 'html', 'money', 'offer', 'please']\n",
    "```\n",
    "<br>\n",
    "<img src=\"images/q1c.png\" alt=\"drawing\" width=\"500\"/>\n",
    "<br>\n",
    "\n",
    "Create a bar chart like the one above comparing the proportion of `spam` and `not-spam` emails containing certain words. Choose a set of **six** words that are different from the ones above, but also have different proportions for the two classes. We'll help you get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "First let's create a list of the words we're interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "words = ['body', 'business', 'html', 'money', 'offer', 'please']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Next, let's use our function `word_detector()` to detect the words in `words`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_q1c = pd.concat((word_detector(words, train['email']), \n",
    "                    train[['label']]), axis=1)\n",
    "df_q1c.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "use the `.melt()` method to \"unpivot\" the DataFrame `df_q1c`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_q1c = df_q1c.melt('label')\n",
    "df_q1c.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "You can text your code with these words as a quick sanity check.\n",
    "```python\n",
    "words = ['body', 'business', 'html', 'money', 'offer', 'please']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 2. Model Building\n",
    "Our new function `word_detector(['hello', 'bye', 'world'], train['email'])` outputs a numeric DataFrame containing features for each email. This means we can use it as input to train a classifier.\n",
    "\n",
    "## Question 2a\n",
    "Let's consider 5 words that might be useful as features to distinguish `spam` from `not-spam` emails. \n",
    "```python\n",
    "words = ['drug', 'bank', 'prescription', 'memo', 'private']\n",
    "```\n",
    "Use `words` as well as the `train` DataFrame to create two DataFrames: `X_train` and `y_train`.\n",
    "\n",
    "`X_train` should be a DataFrame of 0s and 1s created by using `word_detector()` on all the emails in `train`.\n",
    "\n",
    "`y_train` should be a 1D DataFrane of the correct labels for each email in `train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "words = ['drug', 'bank', 'prescription', 'memo', 'private']\n",
    "\n",
    "# Write your code here\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Check `X_train`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "and check y_train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Question 2b\n",
    "Now that we have `X_train` and `y_train`, we can build a model with `scikit-learn`. Using the `LogisticRegression()` classifier, train a logistic regression model using `X_train` and `y_train`. Then, output the model's training accuracy below. You should get an accuracy of around `0.75`. \n",
    "\n",
    "You must also create a function `accuracy()` that takes two 1D numpy arrays (`y_true` and `y_pred`) and outputs an accuracy score. `y_true` and `y_pred` arrays can only contain **0**'s `not-spam` or **1**'s `spam`.\n",
    "\n",
    "Do **NOT** use the `sklearn` function `accuracy_score()` from `sklearn.metrics` to compute the accuracy.\n",
    "\n",
    "Hint: ```np.equal(y_true, y_pred)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def accuracy(y_true, y_pred):\n",
    "    \n",
    "    \"\"\"\n",
    "    Returns a accuracy score for two 1D numpy array of the same length.\n",
    "\n",
    "    Parameters:\n",
    "        y_true (1D numpy array): 1D array of true binary labels \n",
    "                                 np.array([0, 1, 0, 0, ..]).\n",
    "        y_pred (1D numpy array): 1D array of predicted binary labels \n",
    "                                 np.array([1, 0, 0, 1, ..]).\n",
    "\n",
    "    Returns:\n",
    "        (float): Accuracy score of y_true and y_pred.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Write your code here\n",
    "    ...\n",
    "    \n",
    "    return ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Write your code here\n",
    "model = ...\n",
    "y_true = ...\n",
    "training_accuracy = accuracy(y_true, ...)\n",
    "\n",
    "# Print training accuracy\n",
    "print('Training Accuracy: {:.3f}'.format(training_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Whoa! 75% accuracy isn't bad considering we're only using a few simple features. But wait, isn't this dataset imbalanced?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_temp = train['label'].map(lambda val: 'spam' if \n",
    "                             val == 1 else 'not-spam').to_frame()\n",
    "\n",
    "plt.figure(figsize=(6, 3))\n",
    "plt.title('Number of Spam emails', fontsize=18)\n",
    "ax = sns.countplot(x='label', data=df_temp)\n",
    "ax.xaxis.set_tick_params(labelsize=14)\n",
    "ax.yaxis.set_tick_params(labelsize=14)\n",
    "ax.set_xlabel('', fontsize=18)\n",
    "ax.set_ylabel('Email Count', fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Yes, this dataset is clearly imbalanced. To be specific, ~25% of the data are `spam` and ~75% are `not-spam`. But wait, our accuracy was ~75% too? Something seems off here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Question 2c\n",
    "Let's create another classifier called `zero_predictor()` that takes our features `X_train` as input and always predicts `0` (never predicts positive `1`). `zero_predictor()` should output a 1D numpy array with length equal to the number of rows in `X_train`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def zero_predictor(X_train):\n",
    "    \n",
    "    \"\"\"\n",
    "    Returns a 1D numpy array with a length equal to the number of \n",
    "    rows in X_train.\n",
    "\n",
    "    Parameters:\n",
    "        X_train (DataFrame): DataFrame of training features.\n",
    "\n",
    "    Returns:\n",
    "        (1D numpy array): Zero predictions for every entry in X_train.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Write your code here\n",
    "    ...\n",
    "    \n",
    "    return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Next, use your function `accuracy()` to compute the accuracy of your model `zero_predictor()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "zero_predictor_accuracy = accuracy(y_true, ...)\n",
    "\n",
    "# Print zero predictor accuracy\n",
    "print('Zero Predictor Accuracy: {:.3f}'.format(zero_predictor_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Wow, we achieved almost the same accuracy as our fancy `LogisticRegression` model by simply predicting zeros.\n",
    "\n",
    "The take-away lesson here is that you shouldnâ€™t use accuracy on imbalanced problems. This is because it is easy to get a high accuracy score by simply classifying all observations as the majority class (`not-spam` in our case).\n",
    "\n",
    "Because our problem is imbalanced, we'll need to find some other metric to help us evaluate our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Evaluating Classifiers\n",
    "We just learned that our classifier isn't as good as the accuracy suggests. Firstly, we are evaluating accuracy on the training set, which may provide a misleading accuracy measure. Accuracy on the training set doesn't always translate to accuracy in the real world (on the test set). But, you'll remember we split out dataset into `train` and `val` and we'll evaluate our model on `val` later on. But, the main issue is that our dataset is imbalanced which accuracy is not well suited to.\n",
    "\n",
    "When thinking about which metric to use to evaluate our model, its essential to consider the use case. Our classifier will be used for filtering email. Our model will prevent messages labeled `spam` from reaching someone's inbox. \n",
    "\n",
    "There are two kinds of errors the model can make:\n",
    "\n",
    "**False Positive (FP):** a `not-spam` (negative,0) email gets flagged as `spam` (positive,1) and filtered out of the inbox. A `FP` is commonly refered to as `Type I Error`.\n",
    "\n",
    "**False Negative (FN):** a `spam` (positive,1) email gets mislabeled as `not-spam` (negative,0) and ends up in the inbox. A `FN` is commonly refered to as `Type II Error`.\n",
    "\n",
    "And there are two correct predictions the model can make:\n",
    "\n",
    "**True Positive (TP):** a `spam` (positive,1) email that is classified as `spam` (positive,1).\n",
    "\n",
    "**True Negative (TN):** a `not-spam` (negative,0) email that is classified as `not-spam` (negative,0).\n",
    "\n",
    "These definitions depend both on the true labels and the predicted labels. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Question 2d\n",
    "Let's revisit our `zero_predictor()` from the previous question. For this question, we need to compute how many `false positives`, `false negatives`, `true positives` and `true negatives` this classifier generates when evaluated on the training set and its results are compared to `y_train`. Do NOT use any sklearn functions.\n",
    "\n",
    "First, you need to create four functions: \n",
    "- `count_false_positives()`\n",
    "- `count_false_negatives()`\n",
    "- `count_true_positives()`\n",
    "- `count_true_negatives()`\n",
    "\n",
    "Hint: Check out the numpy function `np.logical_and()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def count_false_positives(y_true, y_pred):\n",
    "    \n",
    "    \"\"\"\n",
    "    Returns the number of false positives.\n",
    "\n",
    "    Parameters:\n",
    "        y_true (1D numpy array): 1D array of true binary labels \n",
    "                                 np.array([0, 1, 0, 0, ..]).\n",
    "        y_pred (1D numpy array): 1D array of predicted binary labels \n",
    "                                 np.array([1, 0, 0, 1, ..]).\n",
    "\n",
    "    Returns:\n",
    "        (int): The number of false positives detected.\n",
    "    \"\"\"\n",
    "\n",
    "    # Write your code here\n",
    "    ...\n",
    "    \n",
    "    return ...\n",
    "\n",
    "def count_false_negatives(y_true, y_pred):\n",
    "    \n",
    "    \"\"\"\n",
    "    Returns the number of false negatives.\n",
    "\n",
    "    Parameters:\n",
    "        y_true (1D numpy array): 1D array of true binary labels \n",
    "                                 np.array([0, 1, 0, 0, ..]).\n",
    "        y_pred (1D numpy array): 1D array of predicted binary labels \n",
    "                                 np.array([1, 0, 0, 1, ..]).\n",
    "\n",
    "    Returns:\n",
    "        (int): The number of false negatives detected.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Write your code here\n",
    "    ...\n",
    "    \n",
    "    return ...\n",
    "\n",
    "def count_true_positives(y_true, y_pred):\n",
    "    \n",
    "    \"\"\"\n",
    "    Returns the number of true positives.\n",
    "\n",
    "    Parameters:\n",
    "        y_true (1D numpy array): 1D array of true binary labels \n",
    "                                 np.array([0, 1, 0, 0, ..]).\n",
    "        y_pred (1D numpy array): 1D array of predicted binary labels \n",
    "                                 np.array([1, 0, 0, 1, ..]).\n",
    "\n",
    "    Returns:\n",
    "        (int): The number of true positives detected.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Write your code here\n",
    "    ...\n",
    "    \n",
    "    return ...\n",
    "\n",
    "def count_true_negatives(y_true, y_pred):\n",
    "    \n",
    "    \"\"\"\n",
    "    Returns the number of false negatives.\n",
    "\n",
    "    Parameters:\n",
    "        y_true (1D numpy array): 1D array of true binary labels \n",
    "                                 np.array([0, 1, 0, 0, ..]).\n",
    "        y_pred (1D numpy array): 1D array of predicted binary labels \n",
    "                                 np.array([1, 0, 0, 1, ..]).\n",
    "\n",
    "    Returns:\n",
    "        (int): The number of true negatives detected.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Write your code here\n",
    "    ...\n",
    "    \n",
    "    return ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Count predictions\n",
    "zero_predictor_fp = count_false_positives(y_true, zero_predictor(X_train))\n",
    "zero_predictor_fn = count_false_negatives(y_true, zero_predictor(X_train))\n",
    "zero_predictor_tp = count_true_positives(y_true, zero_predictor(X_train))\n",
    "zero_predictor_tn = count_true_negatives(y_true, zero_predictor(X_train))\n",
    "\n",
    "# Print results\n",
    "print('There are {} records in train.'.format(train.shape[0]))\n",
    "print('There are {} false positives.'.format(zero_predictor_fp))\n",
    "print('There are {} false negatives.'.format(zero_predictor_fn))\n",
    "print('There are {} true positives.'.format(zero_predictor_tp))\n",
    "print('There are {} true negatives.'.format(zero_predictor_tn))\n",
    "print('fp + fn + tp + tn = {}'.format(sum([zero_predictor_fp, \n",
    "                                           zero_predictor_fn,\n",
    "                                           zero_predictor_tp,\n",
    "                                           zero_predictor_tn])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "A quick check is to ensure that `fp + fn + tp + tn = train.shape[0]`.\n",
    "\n",
    "You'll see that there are no `false positives` or `true positives`, which makes sense because our simple model `zero_predictor()` predicts `0` for every record. There are no positive predictions, so there can be no `false positives` or `true positives`.\n",
    "\n",
    "`false positives` and `false negatives` may be of differing importance depending on the use case, leading us to consider more ways of evaluating a classifier. Beyond accuracy, `precision` and `recall` are common metrics for evaluating classifiers.\n",
    "\n",
    "### Precision\n",
    "**Precision** measures the proportion of emails flagged as `spam` that are actually `spam`. In other field, such as medicine, `Precision` is refered to as `Positive Predictive Value (PPV)`.\n",
    "\n",
    "**precision = $\\frac{\\text{TP}}{\\text{TP} + \\text{FP}}$**\n",
    "\n",
    "### Recall\n",
    "**Recall** measures the proportion of `spam` emails that were correctly flagged as `spam`. In other field, such as medicine, `Recall` is refered to as `Sensitivity`.\n",
    "\n",
    "**recall = $\\frac{\\text{TP}}{\\text{TP} + \\text{FN}}$**\n",
    "\n",
    "### False-Alarm Rate\n",
    "**False-alarm rate (far)** measures the proportion (**$\\frac{\\text{FP}}{\\text{FP} + \\text{TN}}$**) of `non-spam` emails that were incorrectly flagged as `spam`.\n",
    "\n",
    "**far = $\\frac{\\text{FP}}{\\text{FP} + \\text{TN}}$**\n",
    "\n",
    "The two graphic below may help you understand precision and recall visually.\n",
    "<br>\n",
    "<img src=\"images/precision_recall.png\" alt=\"drawing\" width=\"400\"/>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Question 2e\n",
    "For this question, we need to compute the `accuracy`, `precision`, `recall` and `false-alarm rate` for our simple `zero_predictor()` model when evaluated on the training set and its results are compared to `y_train`. Do NOT use any sklearn functions.\n",
    "\n",
    "We already have a function for `accuracy`. For this question, you'll need to create three functions: \n",
    "- `precision()`\n",
    "- `recall()`\n",
    "- `false_positive_rate()`\n",
    "\n",
    "Hint: Use your functions from the last question.\n",
    "- `count_false_positives()`\n",
    "- `count_false_negatives()`\n",
    "- `count_true_positives()`\n",
    "- `count_true_negatives()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def precision(y_true, y_pred):\n",
    "    \n",
    "    \"\"\"\n",
    "    Returns the precision score.\n",
    "\n",
    "    Parameters:\n",
    "        y_true (1D numpy array): 1D array of true binary labels \n",
    "                                 np.array([0, 1, 0, 0, ..]).\n",
    "        y_pred (1D numpy array): 1D array of predicted binary labels \n",
    "                                 np.array([1, 0, 0, 1, ..]).\n",
    "\n",
    "    Returns:\n",
    "        (float): The precision.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Write your code here\n",
    "    ...\n",
    "\n",
    "    return ...\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    \n",
    "    \"\"\"\n",
    "    Returns the recall score.\n",
    "\n",
    "    Parameters:\n",
    "        y_true (1D numpy array): 1D array of true binary labels \n",
    "                                 np.array([0, 1, 0, 0, ..]).\n",
    "        y_pred (1D numpy array): 1D array of predicted binary labels \n",
    "                                 np.array([1, 0, 0, 1, ..]).\n",
    "\n",
    "    Returns:\n",
    "        (float): The recall.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Write your code here\n",
    "    ...\n",
    "\n",
    "    return ...\n",
    "\n",
    "def false_alarm_rate(y_true, y_pred):\n",
    "    \n",
    "    \"\"\"\n",
    "    Returns the false alarm rate.\n",
    "\n",
    "    Parameters:\n",
    "        y_true (1D numpy array): 1D array of true binary labels \n",
    "                                 np.array([0, 1, 0, 0, ..]).\n",
    "        y_pred (1D numpy array): 1D array of predicted binary labels \n",
    "                                 np.array([1, 0, 0, 1, ..]).\n",
    "\n",
    "    Returns:\n",
    "        (float): The false positive rate.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Write your code here\n",
    "    ...\n",
    "\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Count metrics\n",
    "zero_predictor_accuracy = accuracy(y_true, zero_predictor(X_train))\n",
    "zero_predictor_precision = precision(y_true, zero_predictor(X_train))\n",
    "zero_predictor_recall = recall(y_true, zero_predictor(X_train))\n",
    "zero_predictor_far = false_alarm_rate(y_true, zero_predictor(X_train))\n",
    "\n",
    "# Print results\n",
    "print('Accuracy {:.3f}'.format(zero_predictor_accuracy))\n",
    "print('Precision: {:.3f}'.format(zero_predictor_precision))\n",
    "print('Recall: {:.3f}'.format(zero_predictor_recall))\n",
    "print('False Alarm Rate: {:.3f}'.format(zero_predictor_far))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Question 2f\n",
    "Comment on the results from Question 2e. For each of `accuracy`, `precision`, `recall`, and `false-alarm rate`, briefly explain why we see the result that we do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "*Type your answer here, replacing this text.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Question 2g\n",
    "Using the function you have created, compute the `accuracy`, `precision`, `recall`, and `false-alarm rate` of the `LogisticRegression` classifier created and trained in Question 2b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "lr_predictor_accuracy = ...\n",
    "lr_predictor_precision = ...\n",
    "lr_predictor_recall = ...\n",
    "lr_predictor_far = ...\n",
    "\n",
    "# Print results\n",
    "print('Accuracy {:.3f}'.format(lr_predictor_accuracy))\n",
    "print('Precision: {:.3f}'.format(lr_predictor_precision))\n",
    "print('Recall: {:.3f}'.format(lr_predictor_recall))\n",
    "print('False Alarm Rate: {:.3f}'.format(lr_predictor_far))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Question 2h\n",
    "Are there more false positives or false negatives when using the `logistic regression` classifier from Question 2b?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "*Type your answer here, replacing this text.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Question 2i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Given the words we gave you above `['drug', 'bank', 'prescription', 'memo', 'private']`, give one reason why our `Logistic Regression` classifier is performing so poorly. \n",
    "\n",
    "Hint: Think about how prevalent these words are in the email set. Maybe the bar chart from Question 1c could be helpful?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Write your code here (if you want)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "*Type your answer here, replacing this text.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Question 2j\n",
    "Which of these two classifiers would you prefer for a `spam` filter and why? Describe your reasoning and relate it to at least one of the evaluation metrics `accuracy`, `precision`, `recall`, and `false-alarm rate` we have computed so far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "*Type your answer here, replacing this text.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Build another Model\n",
    "We'll be comparing models in the next section, so let's create another one using the words from Question 1c `['body', 'business', 'html', 'money', 'offer', 'please']`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Define words\n",
    "words2 = ['body', 'business', 'html', 'money', 'offer', 'please']\n",
    "\n",
    "# Get train data\n",
    "X_train2 = word_detector(words2, train['email'])\n",
    "y_train2 = train[['label']]\n",
    "\n",
    "# Train model\n",
    "model2 = LogisticRegression()\n",
    "model2.fit(X_train2, y_train2)\n",
    "y_true2 = y_train2.values.flatten()\n",
    "\n",
    "# Print metrics\n",
    "print('Accuracy {:.3f}'.format(accuracy(y_true2, model2.predict(X_train2))))\n",
    "print('Precision: {:.3f}'.format(precision(y_true2, model2.predict(X_train2))))\n",
    "print('Recall: {:.3f}'.format(recall(y_true2, model2.predict(X_train2))))\n",
    "print('False Alarm Rate: {:.3f}'.format(false_alarm_rate(y_true2, model2.predict(X_train2))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 3. More Evaluation Metrics\n",
    "Ok, so we have `precision` and `recall`, which appear to be more useful for evaluating our model than `accuracy` when we have imbalanced data. In some applications, we may only care about `precision` or `recall`, however, what if we care about both? In this case, the `F-beta` metric is useful. `F-beta` is the harmonic mean of `precision` and `recall` and is expressed by the following equation.\n",
    "\n",
    "**$F_{\\beta} = (1 + \\beta^{2}) \\frac{precision * recall}{\\beta^{2} * precision + recall}$**\n",
    "\n",
    "When choosing beta in your `F-beta` score, the more you care about `recall` over `precision` the higher `beta` should be. For example, with the `F1` score, we care equally about `recall` and `precision`, however, with the `F2` score, `recall` is twice as important and `precision`. \n",
    "\n",
    "More generally, with 0 < `beta` < 1, we care more about `precision`and with `beta` > 1, we care more about `recall`.\n",
    "\n",
    "`F1` is the common form of `F-beta` and is widely used in many machine learning applications.\n",
    "\n",
    "**$F_{1} = 2 \\frac{precision * recall}{precision + recall}$**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Question 3a\n",
    "Create a function `f_beta` that computes the `F-beta` score we defined above. You may want to use some of the functions you have already created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def f_beta(y_true, y_pred, beta):\n",
    "    \n",
    "    \"\"\"\n",
    "    Returns the F-beta score.\n",
    "\n",
    "    Parameters:\n",
    "        y_true (1D numpy array): 1D array of true binary labels \n",
    "                                 np.array([0, 1, 0, 0, ..]).\n",
    "        y_pred (1D numpy array): 1D array of predicted binary labels \n",
    "                                 np.array([1, 0, 0, 1, ..]).\n",
    "        beta (float): The beta parameter for the F-beta metric.\n",
    "\n",
    "    Returns:\n",
    "        (float): The F-beta score.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Write your code here\n",
    "    ...\n",
    "\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Print results\n",
    "print(\"Model 1\\nwords = {}\\nF1 Score: {:.3f}\\n\".format(words, f_beta(y_true, model.predict(X_train), beta=1)))\n",
    "print(\"Model 2\\nwords = {}\\nF1 Score: {:.3f}\".format(words2, f_beta(y_true2, model2.predict(X_train2), beta=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Wow, thats a pretty significant difference in `F1` score between these two lists of words. Keep this in mind for `Project 1` where your task will be to improve the model.\n",
    "\n",
    "### Probability Thresholds\n",
    "You'll notice that when computing our metrics, we used the `model.predict()` method. For our model, it predicts a `0` (`not-spam`) or `1` (`spam`) for a given array of features. Let's check out the `model.predict()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "But, in your lecture, we learned that the output of the logistic model is a sigmoid function, which looks something like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x_sigmoid = np.arange(-10, 10, 0.001)\n",
    "y_sigmoid = sigmoid(x_sigmoid)\n",
    "plt.plot(x_sigmoid, y_sigmoid, '-')\n",
    "plt.plot([-10, 10], [0.5, 0.5], lw=5)\n",
    "plt.xlabel('X', fontsize=16)\n",
    "plt.ylabel(r'$P(Y = 1 | x)$', fontsize=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "When you use the `.predict()` method, `scikit-learn` automatically applied a threshold of `0.5`. So in the plot above, everything less than **X = 0** is classified as **y = 0** and everything greater than **X = 0** is classified as **y = 1**. However, the threshold is something we can tune to our specific use case.\n",
    "\n",
    "Take a minute and mess around with the interactive visualization below to gain an understanding for how the choice of your threshold can impact our metrics `accuracy`, `precision`, and `recall`. I have created a dummy dataset with one feature `the number of word counts for \"body\"` and a logistic model that predicts the probability of an email being `spam` given the `\"body\"` count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "threshold_prediction_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Depending on what is important to us, we can set the threhold. For example, if we want a model with high precision, a threshold of `0.8` would be work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "threshold_prediction_plot(0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "However, if recall is important, a threshold of `0.1` might work better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "threshold_prediction_plot(0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "So, how do we access these probabilities? We can use the `model.predict_proba()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model.predict_proba(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "`model.predict_proba()` returns the probability of the sample for each class in the model, where classes are ordered as they are in `model.classes_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "So, the first column of `model.predict_proba(X_train)` is the probability of **0** `not-spam` and the second column is the probability of **1** `spam`. You notice that because there are only two classes, the sum of the first and second columns equals 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model.predict_proba(X_train)[0, 0] + model.predict_proba(X_train)[0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Precision-Recall Curve\n",
    "A useful visualization for understanding the trade-off between `precision` and `recall` is the `Precision-Recall (PR) Curve`. The `PR Curve` is a curve that combines `precision` and `recall` in a single visualization. For every threshold, you calculate `precision` and `recall` and plot it. The closer to the upper-right corner, the better your more is.\n",
    "\n",
    "You can use this plot to make an educated decision when it comes to the `precision-recall` trade-off, where the higher the `recall` the lower the `precision`. Knowing at which `recall` your `precision` starts to fall fast can help you choose an optimal `threshold` and deliver a better model.\n",
    "\n",
    "Spend some time playing with the interactive visualization below which displays the `PR Curve` foe model 1 and model 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "# Initialize\n",
    "init_notebook_mode(connected=True) \n",
    "fig = go.Figure()\n",
    "\n",
    "# Model 1\n",
    "y_scores = model.predict_proba(X_train)[:, 1]\n",
    "model_precisions, model_recalls, model_thresholds = precision_recall_curve(y_true, y_scores)\n",
    "model_thresholds = np.append(model_thresholds, [1.])\n",
    "df1 = pd.DataFrame({'precision': model_precisions, 'recall': model_recalls, 'threshold': model_thresholds})\n",
    "trace1 = go.Scatter(x=df1['recall'], y=df1['precision'], name='Model 1', mode='lines', hovertext=df1['threshold'],\n",
    "                    hovertemplate='Precision=%{y}<br>Recall=%{x}<br>Threshold=%{hovertext}')\n",
    "\n",
    "# Model 2\n",
    "y_scores2 = model2.predict_proba(X_train2)[:, 1]\n",
    "model_precisions2, model_recalls2, model_thresholds2 = precision_recall_curve(y_true2, y_scores2)\n",
    "model_thresholds2 = np.append(model_thresholds2, [1.])\n",
    "df2 = pd.DataFrame({'precision': model_precisions2, 'recall': model_recalls2, 'threshold': model_thresholds2})\n",
    "trace2 = go.Scatter(x=df2['recall'], y=df2['precision'], name='Model 2', mode='lines', hovertext=df2['threshold'],\n",
    "                    hovertemplate='Precision=%{y}<br>Recall=%{x}<br>Threshold=%{hovertext}')\n",
    "\n",
    "layout = go.Layout(title='Precision-Recall Curve',\n",
    "                   xaxis=dict(title='Recall'),\n",
    "                   yaxis=dict(title='Precision'))\n",
    "\n",
    "fig = go.Figure(data=[trace1, trace2], layout=layout)\n",
    "fig.layout.update(hovermode='closest')\n",
    "\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Question 3b\n",
    "Below is a function that computes the `F1` score given a threshold. When we calculated the `F1` score in Question 3a, we used a default threshold value of `0.5`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def f1_threshold(y_true, y_pred_proba, threshold):\n",
    "    \n",
    "    \"\"\"\n",
    "    Returns the F-beta score.\n",
    "\n",
    "    Parameters:\n",
    "        y_true (1D numpy array): 1D array of true binary labels \n",
    "                                 np.array([0, 1, 0, 0, ..]).\n",
    "        y_pred_proba (1D numpy array): 1D array of prediction probabilities \n",
    "                                       for the positive class\n",
    "                                       (model.predict_proba(X)[:, 1])\n",
    "                                       np.array([0.12, 0.56, 0.23, 0.89, ..]).\n",
    "        threshold (float): The probability threshold, which is a number \n",
    "                           between 0 and 1.\n",
    "\n",
    "    Returns:\n",
    "        (float): The F1 score given a threshold.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate the binary predictions\n",
    "    y_pred = (y_pred_proba >= threshold).astype(int)\n",
    "    \n",
    "    return f_beta(y_true, y_pred, beta=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def plot_f1_threshold(X, y, model):\n",
    "    \n",
    "    \"\"\"\n",
    "    Returns the F-beta score.\n",
    "\n",
    "    Parameters:\n",
    "        y_true (1D numpy array): 1D array of true binary labels \n",
    "                                 np.array([0, 1, 0, 0, ..]).\n",
    "        y_pred_proba (1D numpy array): 1D array of prediction probabilities \n",
    "                                       for the positive class\n",
    "                                       (model.predict_proba(X)[:, 1])\n",
    "                                       np.array([0.12, 0.56, 0.23, 0.89, ..]).\n",
    "        threshold (float): The probability threshold, which is a number \n",
    "                           between 0 and 1.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create threshold array\n",
    "    thresholds = np.arange(0, 1, 0.01)\n",
    "    \n",
    "    # Compute F1 scores for each threshold\n",
    "    f1_scores = np.array([f1_threshold(y.values.flatten(), model.predict_proba(X)[:, 1], threshold) \n",
    "                          for threshold in np.arange(0, 1, 0.01)])\n",
    "    \n",
    "    # Get finite values\n",
    "    thresholds = thresholds[np.isfinite(f1_scores)]\n",
    "    f1_scores = f1_scores[np.isfinite(f1_scores)]\n",
    "    \n",
    "    # Optimal values\n",
    "    idx = np.argmax(f1_scores)\n",
    "    f1_score = f1_scores[idx]\n",
    "    threshold = thresholds[idx]\n",
    "\n",
    "    # Plot\n",
    "    fig = plt.figure(figsize=(12, 4))\n",
    "    fig.subplots_adjust(wspace=0.2, hspace=0)\n",
    "    ax1 = plt.subplot2grid((1, 2), (0, 0))\n",
    "    ax2 = plt.subplot2grid((1, 2), (0, 1))\n",
    "    ax1.set_title('Optimal F1 score is {:.2f} for a threshold of {:.2f}.'.format(f1_score, threshold), fontsize=12)\n",
    "    ax1.plot(thresholds, f1_scores, '.-')\n",
    "    ax1.plot([threshold, threshold], [0, 1], '-', label='Optimal F1')\n",
    "    ax1.legend()\n",
    "    ax1.set_xlabel('Threshold')\n",
    "    ax1.set_ylabel('F1 Score')\n",
    "    ax1.set_xlim([0, 1])\n",
    "    ax1.set_ylim([0, 1])\n",
    "    \n",
    "    # Get PR curve\n",
    "    prc_precisions, prc_recalls, prc_thresholds = precision_recall_curve(y.values.flatten(), model.predict_proba(X)[:, 1])\n",
    "    precision_opt = precision(y.values.flatten(), (model.predict_proba(X)[:, 1] >= threshold).astype(int))\n",
    "    recall_opt = recall(y.values.flatten(), (model.predict_proba(X)[:, 1] >= threshold).astype(int))\n",
    "    \n",
    "    # Plot\n",
    "    ax2.set_title('Optimal Precision is {:.2f} and recall is {:.2f}.'.format(precision_opt, recall_opt), fontsize=12)\n",
    "    ax2.plot(prc_recalls, prc_precisions)\n",
    "    ax2.plot(recall_opt, precision_opt, 'o', label='Optimal PR')\n",
    "    ax2.legend()\n",
    "    ax2.set_xlabel('Recall')\n",
    "    ax2.set_ylabel('Precision')\n",
    "    ax2.set_xlim([0, 1])\n",
    "    ax2.set_ylim([0, 1])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Above is a function called `plot_f1_threshold()`, which generates a plot of the `F1` score as a function of the threshold value and the `precision-recall` curve. Below, is an example if the function being used for model 1 with `X_train` and `y_train` ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plot_f1_threshold(X_train, y_train, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "For this question, you're task is to find the optimal `threshold`, `F1`, `precision` and `recall` for `model2`. You'll remember that `model2` uses features for these words `['body', 'business', 'html', 'money', 'offer', 'please']` (see variable `words2`).\n",
    "\n",
    "First, which datset should you use to pick the optimal threshold (`train` or `val`), please explain your decision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "*Type your answer here, replacing this text.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Next, use `plot_f1_threshold()` to find the optimal `threshold`, `F1`, `precision` and `recall` for `model2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "...\n",
    "\n",
    "plot_f1_threshold(..., ..., ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Next, let's print `F1`, `precision` and `recall` scores using the default `0.5` `threshold`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Precision Score: {:.3f}\".format(precision(y_val.values.flatten(), model2.predict(X_val))))\n",
    "print(\"Recall Score: {:.3f}\".format(recall(y_val.values.flatten(), model2.predict(X_val))))\n",
    "print(\"F1 Score: {:.3f}\".format(f_beta(y_val.values.flatten(), model2.predict(X_val), beta=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Lastly, reflect on the difference between the optimal values and those using the default `threshold` of `0.5`. Would you use the default on your next project?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Congratulation, you're done Assignment 8. Review your answers and clean up that code before submitting on Quercus. `#cleancode`**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}