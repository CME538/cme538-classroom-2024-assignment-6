{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CME538 - Introduction to Data Science\n",
    "## Assignment 3 - Text Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Objectives\n",
    "After completing this assignment, you should be comfortable:\n",
    "\n",
    "- Using `json` to import `json` files.\n",
    "- Wrangle `json` data into a DataFrame.\n",
    "- Use `matplotlib` and `seaborn` for data visualization.\n",
    "- use more advanced `Pandas` methods.\n",
    "- Gain an understanding of sentiment analysis.\n",
    "\n",
    "**Note:** You are free to add new cells to use as a scratch pad, but make sure to clean you code up and present your answer in the cell indicated with `# Write your code here`.\n",
    "\n",
    "### Marking Breakdown\n",
    "\n",
    "Question | Points\n",
    "--- | ---\n",
    "Question 1a | 1\n",
    "Question 1b | 1\n",
    "Question 2a | 1\n",
    "Question 2b | 1\n",
    "Question 2c | 1\n",
    "Question 3 | 1\n",
    "Question 4a | 1\n",
    "Question 4b | 1\n",
    "Question 4c | 1\n",
    "Question 4d | 1\n",
    "Question 4e | 1\n",
    "Question 5a | 1\n",
    "Question 5b | 1\n",
    "Question 5c | 1\n",
    "Question 5d | 1\n",
    "Question 6a | 1\n",
    "Question 6b | 1\n",
    "Question 6c | 1\n",
    "Question 6d | 1\n",
    "Question 6e | 1\n",
    "Question 6f | 1\n",
    "Question 6g | 1\n",
    "Question 7a | 1\n",
    "Question 7b | 1\n",
    "Total | 24\n",
    "\n",
    "One of the following marks below will be added to the **Total** above.\n",
    "\n",
    "### Code Quality\n",
    "\n",
    "| Rank | Points | Description |\n",
    "| :-- | :-- | :-- |\n",
    "| Youngling | 1 | Code is unorganized, variables names are not descriptive, redundant, memory-intensive, computationally-intensive, uncommented, error-prone, difficult to understand. |\n",
    "| Padawan | 2 | Code is organized, variables names are descriptive, satisfactory utilization of memory and computational resources, satisfactory commenting, readable. |\n",
    "| Jedi | 3 | Code is organized, easy to understand, efficient, clean, a pleasure to read. #cleancode |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3rd party imports\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "# local imports\n",
    "import utils\n",
    "\n",
    "# Configure Notebook\n",
    "%matplotlib inline\n",
    "plt.style.use('fivethirtyeight')\n",
    "sns.set_context(\"notebook\")\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset consists of tweets from Donald Trump between January 1st, 2016 and December 31st, 2017. This period spans the election campaign and election victory by Donald Trump on November 9th, 2016. All tweets registered to Donald Trump's Twitter account `@realDonaldTrump` during this time period were saved in 5 `json` files located in the assignment directory. The tweets have been separated according to which of Donald Trump's phones (iPhone or Android) was used to send the tweet.\n",
    "\n",
    "<br>\n",
    "<img src=\"images/trump.png\" alt=\"drawing\" width=\"400\"/>\n",
    "<br>\n",
    "<center>Courtesy of FiveThirtyEight</center>\n",
    "\n",
    "In this assignment we want to analyze Trump's tweets and explore possible differences in behavior across the devices (iPhone or Android) used to make the tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import Tweet Data\n",
    "First, you must explore the files and understand the data structure and content. Next, you must process the raw data and convert it into a more flexible format.\n",
    "## Question 1a\n",
    "Print the contents of the root directory of the assignment3.ipynb notebook. Create a variable  `assignment_files` and assign the output from `os.listdir()` to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here.\n",
    "assignment_files = ...\n",
    "\n",
    "# Print answer\n",
    "print(assignment_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1b\n",
    "Filter the `assignment_files` list so that it only constains files ending in `.json`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here.\n",
    "assignment_files = ...\n",
    "\n",
    "# Print answer\n",
    "print(assignment_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "Load the file `trump_tweets_01-01-2016_31-12-2016_android.json` and print its contents. Create a variable  `file_temp` and assign the opened file to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here.\n",
    "file_temp = ...\n",
    "\n",
    "# Print answer\n",
    "utils.json_print(file_temp[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File contents\n",
    "These `json` files contain a list of dictionaries. Each dictionary corresponds to a tweet from Donald Trump. There are six *keys* in each dictionary.\n",
    "\n",
    "- date:\n",
    "- favorites:\n",
    "- id:\n",
    "- isRetweet:\n",
    "- retweets:\n",
    "- text:\n",
    "\n",
    "In the space below, please describe the data corresponding to each *key* and their data types are. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Type your answers next to the keys below.*\n",
    "\n",
    "- date:\n",
    "    - Description:\n",
    "    - Data type:\n",
    "    \n",
    "- favorites:\n",
    "    - Description:\n",
    "    - Data type:\n",
    "    \n",
    "- id:\n",
    "    - Description:\n",
    "    - Data type:\n",
    "    \n",
    "- isRetweet:\n",
    "    - Description:\n",
    "    - Data type:\n",
    "\n",
    "- retweets:\n",
    "    - Description:\n",
    "    - Data type:\n",
    "\n",
    "- text:\n",
    "    - Description:\n",
    "    - Data type:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2a\n",
    "Next, we need to combine the json files into a more flexible format. A DataFrame would be great! First, we should combine the five lists of dictionaries into one big list of dictionaries. However, the file names contain a critical piece of information for our analysis that is not present in the dictionaries. That is, which device was used to make the Tweet, iPhone or Android.\n",
    "\n",
    "The file name strucuture is as follows: `trump_tweets_01-01-2016_31-12-2016_android.json`\n",
    "\n",
    "The device name is always the last word before `.json`.\n",
    "\n",
    "Create a variable `tweets` and assign a list to it that contains all the tweet-dictionaries from each file with the `'device'` added as a key. See an example of what `tweets` should look like.\n",
    "\n",
    "<br>\n",
    "<img src=\"images/tweet_device_json.png\" alt=\"drawing\" width=\"900\"/>\n",
    "<br>\n",
    "\n",
    "Devices should be lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here.\n",
    "tweets = ...\n",
    "\n",
    "# Print answer\n",
    "utils.json_print(tweets[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2b\n",
    "Next, convert the dictionary into a DataFrame. `tweets` should look something like this.\n",
    "\n",
    "<br>\n",
    "<img src=\"images/tweet_df.png\" alt=\"drawing\" width=\"750\"/>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here.\n",
    "tweets = ...\n",
    "\n",
    "# Print answer\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2c\n",
    "We've combined multiple files containing Trump tweets and its possible that there could be duplicates. To ensure there are no duplicates, remove any rows with an `'id'` appearing more than once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here.\n",
    "tweets = ...\n",
    "\n",
    "# Print answer\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Tweet Device Analysis (iPhone V.S. Android)\n",
    "In this section, we'll analysis Trump's tweeting habits and which devices were used to do the tweeting.\n",
    "\n",
    "First, let's see how many unique devices are used to tweet in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['device'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like during this time period, Trump was tweeting from an iPhone and Android."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "Create a bar chart showing the the frequency of tweets for each device. Your plot should look similar to the following: \n",
    "\n",
    "<br>\n",
    "<img src=\"images/device_count_bar_chart.png\" alt=\"drawing\" width=\"600\"/>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here.\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "Next, we'd like to analyze Trump's tweeting habits and how his two devices are used over the entire period of the dataset. Your final plot (Question 4e) should look similar to the plot below:\n",
    "\n",
    "<br>\n",
    "<img src=\"images/device_activity_by_year.png\" alt=\"drawing\" width=\"600\"/>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4a\n",
    "The first thing we need to do it convert from the raw `date` units to a UTC datetime. You'll remember we covered this in Lecture 10. The raw `date` appears to be a Unix timestamp. You'll recall that the Unix timestamp is a way to track time as a running total of seconds. This count starts at the Unix Epoch on January 1st, 1970 at UTC. Technically, the definition of a Unix time stamp is seconds since the Unix Epoch, however, depending on how a time stamp was produced, it may be in milliseconds or perhaps even microseconds. For example, Javascript uses the number of milliseconds since the Unix Epoch. In any case, we should confirm the units of our Unix timestamp. \n",
    "\n",
    "Check out this [website](https://currentmillis.com/) for a useful tool.\n",
    "\n",
    "Using this tool or any other tool or method you'd like, determine what the units of the Unix timestamps are in our dataset.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Type your answer here, replacing this text.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4b\n",
    "Now that we know the Unix timestamp units, use `pd.to_datetime()` to create a new column for the DataFrame `tweets` that contains a datetime object in UTC. Please assign the new column as follows `tweets['datetime_utc'] = ...`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here.\n",
    "tweets['datetime_utc'] = ...\n",
    "\n",
    "# View DataFrame\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4c\n",
    "Set the index of the `tweets` DataFrame to the `'id'` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here.\n",
    "tweets = ...\n",
    "\n",
    "# View DataFrame\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4d\n",
    "To examine the distribution of dates we need to convert the datetime to a fractional year that can be plotted as a distribution. We can use the `year_fraction` function below, which takes a datetime object as input and outputs the frational year. \n",
    "\n",
    "Your DataFrame should look something like this.\n",
    "\n",
    "<br>\n",
    "<img src=\"images/tweets_plus_year_frac.png\" alt=\"drawing\" width=\"600\"/>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def year_fraction(date):\n",
    "    \"\"\"\n",
    "    https://stackoverflow.com/questions/6451655/how-to-convert-python-datetime-dates-to-decimal-float-years\n",
    "    \"\"\"\n",
    "    start = datetime(date.year, 1, 1).toordinal()\n",
    "    year_length = datetime(date.year + 1, 1, 1).toordinal() - start\n",
    "    return date.year + float(date.toordinal() - start) / year_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here.\n",
    "tweets['year'] = ...\n",
    "\n",
    "# View DataFrame\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4e\n",
    "Now we can `sns.histplot()` to overlay the distributions of Android and iPhone tweets over the years. Your final plot should look similar to the plot below:\n",
    "\n",
    "<br>\n",
    "<img src=\"images/device_activity_by_year.png\" alt=\"drawing\" width=\"600\"/>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here.\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "In this assignment, we want to explore differences in bahavior across these devices. For Question 5, we want to explore daily patterns in Trump's tweets for each device.\n",
    "\n",
    "### Question 5a\n",
    "First, we need to convert from UTC to Eastern Standard Time (EST) because we want to explore his tweeting habits as a function of the time of day (morning, afternoon, evening) where he is tweeting from. EST is the time zone that Trump was most likely tweeting from in 2016 and 2017 given he spent most of his time in New York City, The Mar-a-Lago Club in Florida and Washington DC. Create a new column `tweets['datetime_est']` that contains datetime objects localized to EST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here.\n",
    "tweets['datetime_est'] = ...\n",
    "\n",
    "# View DataFrame\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5b\n",
    "For us to visualiza tweets as a function of the time of day they were posted, we need to add a new column `tweets['hour']`. For example, 6 am is hour 6 and 9 pm is hour 21.\n",
    "\n",
    "Hint:\n",
    "```python\n",
    "hour + minute/60 + second/60**2\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here.\n",
    "tweets['hour'] = ...\n",
    "\n",
    "# View DataFrame\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5c\n",
    "Use the `tweets['hour']` column along with the seaborn `kdeplot` function to examine the distribution over hours of the day that Trump tweets from each device. Your final plot should look similar to the following:\n",
    "\n",
    "<br>\n",
    "<img src=\"images/tweet_hours_all.png\" alt=\"drawing\" width=\"600\"/>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here.\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5d\n",
    "You're told that Trump switched from his Android to an iPhone sometime in March 2017. Therefore, Trump's tweeting habits may be represented in the iPhone distribution above. Let's try changing the plot to only include iPhone tweets from before January 1st, 2017. Your final plot should look similar to the following:\n",
    "<br>\n",
    "<img src=\"images/tweet_hours_2016.png\" alt=\"drawing\" width=\"600\"/>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here.\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Tweet Sentiment Analysis\n",
    "We can use the words in Trump's tweets to calculate a measure of the sentiment of the tweet. For example, the sentence \"I love Fox News!\" has positive sentiment, whereas the sentence \"I hate The New York Times!\" has a negative sentiment. Additionaly, some words have a stronger positive or negative sentiment than others. \"I love Fox News.\" is more positive than \"I like Fox News.\"\n",
    "\n",
    "We will use the [VADER](https://github.com/cjhutto/vaderSentiment) (Valence Aware Dictionary and Sentiment Reasoner) lexicon to analyze the sentiment of Trump's tweets. VADER is a lexicon and rule-based sentiment analysis tool that is specifically attuned to sentiments expressed in social media which is ideal for our usage.\n",
    "\n",
    "The VADER lexicon gives the sentiment of individual words and has been included as a text file in the assignment directory `vader_lexicon.txt`. Run the following cell to show the first few rows of the lexicon file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(''.join(open(\"vader_lexicon.txt\").readlines()[:20]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row contains 4 entries where the first entry is a word and the second entry is the polarity of that word, measuring how positive or negative the word is. Note: the VADER lexicon contains emojis too.\n",
    "\n",
    "The creators of VADER describe the tool’s assessment of polarity (compound score) in the following way:\n",
    "\n",
    "“The compound score is computed by summing the valence scores of each word in the lexicon, adjusted according to the rules, and then normalized to be between -1 (most extreme negative) and +1 (most extreme positive). This is the most useful metric if you want a single unidimensional measure of sentiment for a given sentence. Calling it a 'normalized, weighted composite score' is accurate.”\n",
    "\n",
    "### Question 6a\n",
    "Import `vader_lexicon.txt` as a DataFrame and assign it to a variable called `lexicon`. The DataFrame should have one column called `polarity`, which corresponds to the second entry (column) in `vader_lexicon.txt` and the index of the DataFrame should be the word, which corresponds to the first entry (column) in `vader_lexicon.txt`. The 3rd and 4th entries of `vader_lexicon.txt` should not be included in `lexicon`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here.\n",
    "lexicon = ...\n",
    "\n",
    "# View DataFrame\n",
    "lexicon.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6b\n",
    "Ultimately, we want to calculate the sentiment of each tweet. This is accomplised by using the lexicon to find the polarity of each word in a tweet and then taking the sum of these polarities. \n",
    "\n",
    "First, we need to clean up the text of each tweet before looking up the polarities of each word. Make the text in each tweet lowercase because the VADER lexicon is all lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Write your code here.\n",
    "tweets['text'] = ...\n",
    "\n",
    "# View DataFrame\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6c\n",
    "Next, we must remove all punctuation from the text of each tweet because it will cause us to fail to match words with the lexicon. This is because the VADER lexicon does not assign polarities to puntuation like `?, !, etc.`. There are more sophisticated ways to go about this using regular expressions, however, we will use a string of punctuation characters (see variable `punctuations` below). Create a new column in the `tweets` DataFrame called `'text_no_punc'` and assign to it the tweet text without any punctuation. In the text, replace punctuations with a single space. For example, `hello! my name is sebastian.` would become `hello  my name is sebastian `. Consider writing a function that loops through all the characters in a tweet, `.replace()`, and `.apply()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set punctuations\n",
    "punctuations = '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
    "\n",
    "# Write your code here.\n",
    "tweets['text_no_punc'] = ...\n",
    "\n",
    "# View DataFrame\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6d\n",
    "Next, we need to convert `tweets` to a look up table with the following format.\n",
    "<br>\n",
    "<img src=\"images/look_up.png\" alt=\"drawing\" width=\"250\"/>\n",
    "<br>\n",
    "The tweet `id` is the DataFrame index, which is repeated once for every word in the tweet. \n",
    "\n",
    "There are many different ways to arrive at the correct output, however, it is desirable to use the minimum amount of code. Try to avoide using for loops. Our solution uses a chain of 4 Pandas methods. Try looking at `str.expand()`, `str.split`, `stack()`, and `reset_index()`. Create a new variable called `word_lookup` and assign your new DataFrame to it.\n",
    "\n",
    "Validate your code with tweet `id = '814958820980039681'` above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here.\n",
    "word_lookup = ... \n",
    "\n",
    "# View DataFrame\n",
    "word_lookup.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6e\n",
    "Next, we need to add the lexicon polarity from `lexicon` to `word_lookup`. Add a column called `'polarity'` to `word_lookup`. You will need to merge `word_lookup` and `lexicon` tables. If certain words are not found in `lexicon`, set their polarities to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here.\n",
    "word_lookup = ...\n",
    "\n",
    "# View DataFrame\n",
    "word_lookup.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6f\n",
    "Next, we need to get the total polarity sum for each unique tweet `'id'`. Create a new variable called `polarity` and assign to it a new DataFrame where tweet `'id'` is the index and there is one column called `'polarity'` containing the sum of all the polarities from all the words for a particular tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here.\n",
    "polarity = ... \n",
    "\n",
    "# View DataFrame\n",
    "polarity.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6g\n",
    "Lastly, merge the summed polarity values from `polarity` to our `tweets` DataFrame. `tweets` should get a new column called `'polarity'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here.\n",
    "tweets = ...\n",
    "\n",
    "# View DataFrame\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a measure of the sentiment of each of his tweets! Now, run the cells below to see the most positive and most negative tweets from Trump in your dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Trump's most negative tweets:\")\n",
    "for text in tweets.sort_values('polarity').head()['text']:\n",
    "    print('\\n  ', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Trump's most positive tweets:\")\n",
    "for text in tweets.sort_values('polarity', ascending=False).head()['text']:\n",
    "    print('\\n  ', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7\n",
    "Now, let's try looking at the distributions of sentiments for tweets containing certain keywords.\n",
    "\n",
    "### Question 7a\n",
    "In the cell below, create a single plot showing both the distribution of tweet sentiments for tweets containing `'nytimes'`, as well as the distribution of tweet sentiments for tweets containing `'fox'`.\n",
    "\n",
    "Be sure to label your axes and provide a title and legend. Be sure to use different colors for `'fox'` and `'nytimes'`. Your plot shoud look like this:\n",
    "<br>\n",
    "<img src=\"images/7a.png\" alt=\"drawing\" width=\"600\"/>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here.\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7b\n",
    "Can you find another pair of keywords that lead to an interesting plot?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Congratulation, you're done Assignment 3. Review your answers and clean up that code before submitting on Quercus. `#cleancode`**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
