{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# CME538 - Introduction to Data Science\n",
    "## Assignment 8 - Data Science Life Cycle\n",
    "\n",
    "### Learning Objectives\n",
    "After completing this assignment, you should be comfortable:\n",
    "\n",
    "- Feature engineering\n",
    "- Using sklearn to build simple and more complex linear models\n",
    "- Building a data pipeline using pandas\n",
    "- Identifying informative variables through EDA\n",
    "- Feature engineering with categorical variables\n",
    "- Classification using logistic regression\n",
    "- Classification metrics\n",
    "\n",
    "### Marking Breakdown\n",
    "\n",
    "Question | Points\n",
    "--- | ---\n",
    "Question 1a | 1\n",
    "Question 1b | 1\n",
    "Question 1c | 1\n",
    "Question 1d | 1\n",
    "Question 2a | 1\n",
    "Question 2b | 1\n",
    "Question 2c | 1\n",
    "Question 3a | 1\n",
    "Question 3b | 1\n",
    "Question 3c | 1\n",
    "Question 3d | 1\n",
    "Question 4a | 1\n",
    "Question 4b | 1\n",
    "Question 4c | 1\n",
    "Question 4d | 1\n",
    "Question 4e | 1\n",
    "Question 4f | 1\n",
    "Total | 17\n",
    "\n",
    "One of the following marks below will be added to the **Total** above.\n",
    "\n",
    "### Code Quality\n",
    "\n",
    "| Rank | Points | Description |\n",
    "| :-- | :-- | :-- |\n",
    "| Youngling | 1 | Code is unorganized, variables names are not descriptive, redundant, memory-intensive, computationally-intensive, uncommented, error-prone, difficult to understand. |\n",
    "| Padawan | 2 | Code is organized, variables names are descriptive, satisfactory utilization of memory and computational resources, satisfactory commenting, readable. |\n",
    "| Jedi | 3 | Code is organized, easy to understand, efficient, clean, a pleasure to read. #cleancode |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Setup Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Import 3rd party libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Configure Notebook\n",
    "%matplotlib inline\n",
    "plt.style.use('fivethirtyeight')\n",
    "sns.set_context(\"notebook\")\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In this assignment, you will use all the tools and knowledge that you've learned in class to explore and model New York Taxi data. You will create a regression model that predicts the travel time of a taxi ride and you will also create a binary classifier to predict whether or not someone will tip at the end of their ride. This assignment is meant to guide you through the complete Data Science Life Cycle. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Features of all [yellow taxi](https://en.wikipedia.org/wiki/Taxicabs_of_New_York_City) trips in January 2016 are published by the [NYC Taxi and Limosine Commission](https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page).\n",
    "\n",
    "The full dataset is over 1 GB, so we've placed a simple random sample of the data into `taxi.csv`.\n",
    "\n",
    "Columns of the `taxi.csv` include:\n",
    "- `pickup_datetime`: date and time when the meter was engaged\n",
    "- `dropoff_datetime`: date and time when the meter was disengaged\n",
    "- `pickup_lon`: the longitude where the meter was engaged\n",
    "- `pickup_lat`: the latitude where the meter was engaged\n",
    "- `dropoff_lon`: the longitude where the meter was disengaged\n",
    "- `dropoff_lat`: the latitude where the meter was disengaged\n",
    "- `passengers`: the number of passengers in the vehicle (driver entered value)\n",
    "- `distance`: trip distance in miles\n",
    "- `payment_method`: 1=Credit card, 2=Cash, 3=No charge, 4=Dispute\n",
    "- `surcharge`: Improvement surcharge\n",
    "- `tax`: State and municipal taxes\n",
    "- `fare`: the time-and-distance fare calculated by the meter\n",
    "- `tip`: the amount of credit card tips, cash tips are not included"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 1. Data Selection and Cleaning\n",
    "\n",
    "In this part, you will limit the data to trips that began and ended on Manhattan Island ([map](https://www.google.com/maps/place/Manhattan,+New+York,+NY/@40.7590402,-74.0394431,12z/data=!3m1!4b1!4m5!3m4!1s0x89c2588f046ee661:0xa0b3281fcecc08c!8m2!3d40.7830603!4d-73.9712488)). \n",
    "\n",
    "## Question 1a\n",
    "Import `taxi.csv` as a DataFrame and assign it to a variable called `all_taxi`. \n",
    "\n",
    "Only include trips that have **both** pick-up and drop-off locations within the boundaries of New York City:\n",
    "\n",
    "- Longitude is between -74.03 and -73.75 (inclusive of both boundaries)\n",
    "- Latitude is between 40.6 and 40.88 (inclusive of both boundaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "all_taxi = ...\n",
    "\n",
    "# View DataFrame\n",
    "all_taxi.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Question 1b\n",
    "Create a plot of pickup locations using Folium and the `HeatMap` function. The `HeatMap` will show the density of pickup locations in different areas of the city, where red areas have relatively more pickups. As you can see, most of the pickups are in Manhattan in addition to two hot spots corresponding to airports. Your plot should look like this (radius = 10).\n",
    "<br>\n",
    "<img src=\"images/q1b.png\" alt=\"drawing\" width=\"500\"/>\n",
    "<br> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!pip install folium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import folium\n",
    "from folium import Circle\n",
    "from folium.plugins import HeatMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Create a map of New York\n",
    "map_1 = folium.Map(location=[40.7484, -73.9857], \n",
    "                   tiles='cartodbpositron', \n",
    "                   zoom_start=10)\n",
    "\n",
    "# Write your code here\n",
    "...\n",
    "\n",
    "# Display map\n",
    "map_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Question 1c\n",
    "We'll be trying to estimate the duration of a taxi ride and therefore, we will need to compute the duration of each ride. Create a new column in `all_taxi` called `'duration'` and assign to it the length of the taxi ride in seconds. The datatype for `'duration'` should be `int`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Convert to datetime and localize to EST\n",
    "all_taxi['pickup_datetime'] = pd.to_datetime(all_taxi['pickup_datetime'], errors='coerce')\n",
    "all_taxi['dropoff_datetime'] = pd.to_datetime(all_taxi['dropoff_datetime'], errors='coerce')\n",
    "all_taxi['pickup_datetime'] = all_taxi['pickup_datetime'].dt.tz_localize(tz='EST')\n",
    "all_taxi['dropoff_datetime'] = all_taxi['dropoff_datetime'].dt.tz_localize(tz='EST')\n",
    "\n",
    "# Write your code here\n",
    "all_taxi['duration'] = ...\n",
    "\n",
    "# View DataFrame\n",
    "all_taxi.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Question 1d\n",
    "Create a DataFrame called `clean_taxi` that only includes trips with a positive passenger count, a positive distance, a duration of at least 1 minute and at most 1 hour, and an average speed of at most 100 miles per hour. Inequalities should not be strict (e.g., `<=` instead of `<`) unless comparing to 0. You will need to first create a new column in `all_taxi` called `'speed'` and assign to it the average speed in miles per hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "all_taxi['speed'] = ...\n",
    "clean_taxi = ...\n",
    "\n",
    "# View DataFrame\n",
    "clean_taxi.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Next, we want to collect trips from `clean_taxi` that start and end within the boundaries of Manhattan Island.\n",
    "\n",
    "We're in luck because `GeoPandas` has a builtin dataset for the boundaries of New York Boroughs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "path = gpd.datasets.get_path('nybb')\n",
    "boroughs = gpd.read_file(path)\n",
    "boroughs.set_index('BoroCode', inplace=True)\n",
    "boroughs.sort_index(inplace=True)\n",
    "boroughs.to_crs(epsg=4326, inplace=True)\n",
    "boroughs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's plot these quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ax = boroughs.plot(figsize=(8, 8), column='BoroName', categorical=True, \n",
    "                   cmap='Accent', linewidth=.6, edgecolor='0.2', alpha=0.5,\n",
    "                   legend=True, legend_kwds={'loc': 2,'fontsize':16, \n",
    "                                             'frameon':False})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's create a new DataFrame called `manhattan_taxi` that only includes trips from `clean_taxi` that start and end within the boundaries of Manhattan Island."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!pip install shapely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from shapely.geometry import MultiPoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "First we have to create a `MultiPoint` geometry for each trip (pickup and dropoff)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "clean_taxi['geometry'] = clean_taxi.apply(lambda row: MultiPoint([(row['pickup_lon'], row['pickup_lat']), \n",
    "                                                                  (row['dropoff_lon'], row['dropoff_lat'])]), axis=1)\n",
    "clean_taxi = gpd.GeoDataFrame(clean_taxi)\n",
    "clean_taxi.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Next, for each trip we need to check if it started and ended within Manhattan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "manhattan_taxi = clean_taxi[clean_taxi.within(boroughs[boroughs['BoroName'] == 'Manhattan'].iloc[0].geometry)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's see if our filtering worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ax = boroughs.plot(figsize=(8, 8), column='BoroName', categorical=True, \n",
    "                   cmap='Accent', linewidth=.6, edgecolor='0.2', alpha=0.5,\n",
    "                   legend=True, legend_kwds={'loc': 2,'fontsize':16, \n",
    "                                             'frameon':False})\n",
    "manhattan_taxi.plot(ax=ax, markersize=0.00001, color='r');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "It worked! That filtering operation takes quite a bit of time, so let's save a `.csv` and create a checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "manhattan_taxi.to_csv('manhattan_taxi.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Question 1e (ungraded)\n",
    "In the following cell, print a summary of the data selection and cleaning you performed. For example, you should print something like: \"Of the original 1000 trips, 21 anomolous trips (2.1%) were removed through data cleaning, and then the 600 trips within Manhattan were selected for further analysis.\" (Note that the numbers in this example are not accurate.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Predicting Trip Duration\n",
    "In this section, you will develop a machine learning model to predict the duration of taxi trips in New York.\n",
    "\n",
    "# 2. Exploratory Data Analysis\n",
    "In this part, you'll choose which days to include as training data in your regression model. \n",
    "\n",
    "Your goal is to develop a general model that could potentially be used for future taxi rides. There is no guarantee that future distributions will resemble observed distributions, but some effort to limit training data to typical examples can help ensure that the training data are representative of future observations.\n",
    "\n",
    "January 2016 had some atypical days. New Years Day (January 1) fell on a Friday. MLK Day was on Monday, January 18. A [historic blizzard](https://en.wikipedia.org/wiki/January_2016_United_States_blizzard) passed through New York that month. Using this dataset to train a general regression model for taxi trip times must account for these unusual phenomena, and one way to account for them is to remove atypical days from the training data.\n",
    "\n",
    "## Question 2a\n",
    "Add a new column named `'date'` to `manhattan_taxi` that contains the date (but not the time) of pickup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "manhattan_taxi['date'] = ...\n",
    "\n",
    "# View DataFrame\n",
    "manhattan_taxi.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Question 2b\n",
    "Create a data visualization that allows you to identify which dates were affected by the historic blizzard of January 2016. Make sure that the visualization type is appropriate for the visualized data.\n",
    "\n",
    "Hint: How do you expect taxi usage to differ on blizzard days?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Question 2c\n",
    "We have generated a list of dates that should have a fairly typical distribution of taxi rides, which excludes holidays and blizzards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import calendar\n",
    "\n",
    "print('Typical dates:\\n')\n",
    "pat = '  [1-3]|18 | 23| 24|25 |26 '\n",
    "print(re.sub(pat, '   ', calendar.month(2016, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "atypical = [1, 2, 3, 18, 23, 24, 25, 26]\n",
    "typical_dates = [n for n in range(1, 32) if n not in atypical]\n",
    "print(typical_dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Create a new DataFrame that only contains `typical_dates` and assign it to a new variable called `final_taxi`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "final_taxi = ...\n",
    "\n",
    "# View DataFrame\n",
    "final_taxi.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "You are welcome to perform more exploratory data analysis, but your work will not be scored. Here's a blank cell to use if you wish. In practice, further exploration would be warranted at this point, but we won't require that of you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 3. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In this section, you'll create a feature set for your linear regression model. You decide to predict trip duration from the following inputs: start location, end location, trip distance, time of day, and day of the week (*Monday, Tuesday, etc.*). \n",
    "\n",
    "You will ensure that the process of transforming observations into a feature set is expressed as a Python function called `create_features`, so that it's easy to make predictions for different samples in later parts of the assignment.\n",
    "\n",
    "Because you are going to look at the data in detail in order to define features, it's best to split the data into training, validation and test sets now, then only inspect the training set. Remember what we learned in Lecture 21 about a form a data leakage called train/test contamination. Do not touch your test dataset until your final model is trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(final_taxi, train_size=0.7, \n",
    "                               test_size=0.3, random_state=0)\n",
    "val, test = train_test_split(test, train_size=0.5,\n",
    "                             test_size=0.5, random_state=0)\n",
    "print('Train:', train.shape, 'Val:', val.shape, 'Test:', test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Question 3a\n",
    "Create a box plot that compares the distributions of taxi trip durations for each day using `train` only. Individual dates should appear on the horizontal axis, and duration values should appear on the vertical axis. Your plot should look like the following.\n",
    "\n",
    "<br>\n",
    "<img src=\"images/q3a.png\" alt=\"drawing\" width=\"500\"/>\n",
    "<br> \n",
    "\n",
    "Hint: Use sns.boxplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Question 3b\n",
    "Add the following features to `train`, `val` and `test`. \n",
    "\n",
    "- `hour`: The integer hour of the pickup time. E.g., a 3:45pm taxi ride would have `15` as the hour. A 12:20am ride would have `0`.\n",
    "- `day`: The day of the week with Monday=0, Sunday=6.\n",
    "- `weekend`: 1 if and only if the `day` is Saturday or Sunday.\n",
    "- `period`: 1 for early morning (12am-6am), 2 for daytime (6am-6pm), and 3 for night (6pm-12pm). Hint: np.digitize()\n",
    "\n",
    "Because we have to do this for `train`, `val` and `test`, create a function called `add_features`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def add_features(df):\n",
    "    \"\"\"Augment a dataframe df with additional features.\"\"\"\n",
    "    df_temp = df.copy()\n",
    "    \n",
    "    # Write your code here\n",
    "    df_temp.loc[:, 'hour'] = ...\n",
    "    df_temp.loc[:, 'day'] = ...\n",
    "    df_temp.loc[:, 'weekend'] = ...\n",
    "    df_temp.loc[:, 'period'] = ...\n",
    "    \n",
    "    return df_temp\n",
    "\n",
    "# Add features to train , val and test\n",
    "train = add_features(train)\n",
    "val = add_features(val)\n",
    "test = add_features(test)\n",
    "\n",
    "# View train DataFrame\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Question 3c\n",
    "Use `sns.distplot` and `train` to create an overlaid histogram comparing the distribution of average speeds for taxi rides that start in the early morning (12am-6am), day (6am-6pm; 12 hours), and night (6pm-12am; 6 hours). Your plot should look like this.\n",
    "<br>\n",
    "<img src=\"images/q3c.png\" alt=\"drawing\" width=\"500\"/>\n",
    "<br> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Manhattan can roughly be divided into Lower, Midtown, and Upper regions. Instead of studying a map, let's approximate by finding the first principal component of the pick-up location (latitude and longitude). Before doing that, let's once again take a look at a scatterplot of trips in Manhattan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "plt.scatter(manhattan_taxi['pickup_lon'], \n",
    "            manhattan_taxi['pickup_lat'], s=0.1, alpha=0.2)\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.title('Pickup locations');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Add a `region` column to `train`, `val` and `test` that categorizes each pick-up location as 0, 1, or 2 based on the value of each point's first principal component, such that an equal number of points fall into each region. \n",
    "\n",
    "Read the documentation of [`pd.qcut`](https://pandas.pydata.org/pandas-docs/version/0.23.4/generated/pandas.qcut.html), which categorizes points in a distribution into equal-frequency bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "First, let's calculate the first principal component (PCA1) using `'pickup_lon'` and `'pickup_lat'` from `train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=1)\n",
    "pca.fit(train[['pickup_lon', 'pickup_lat']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print('Principal component 1 explains {:.2f} % of the variance in \"pickup_lon\" and \"pickup_lat\".'.format(pca.explained_variance_ratio_[0] * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Next, we need to transform `['pickup_lon', 'pickup_lat']` into principal component 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pc1 = pca.transform(train[['pickup_lon', 'pickup_lat']])\n",
    "print('Original shape: ', train[['pickup_lat', 'pickup_lon']].shape)\n",
    "print('Transformed shape of PC1: ', pc1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "As we can see, we have reduced from 2 dimensions (2 features) to 1 dimension and this one dimension (or Principal Component), explains 88% of the variance in the original two dimensions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ax = sns.distplot(pc1)\n",
    "ax.set_xlabel('Principal Component 1');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Below, you can see that our (latitude, longitude) locations have been collapsed to a single line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_new = pca.inverse_transform(pc1)\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.scatter(train['pickup_lon'], \n",
    "            train['pickup_lat'], s=0.1, alpha=0.2, label='Pickup Locations')\n",
    "plt.scatter(X_new[:, 0], X_new[:, 1], s=1, alpha=0.8, label='Pickup Locations\\nTransformed to PC1')\n",
    "plt.legend()\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.title('Pickup locations');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Next, we need to categorizes each pick-up location as 0, 1, or 2 based on the value of each point's first principal component `pc1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "out, bins = pd.qcut(pc1.flatten(), 3, labels=[1, 2, 3], retbins=True)\n",
    "print(out)\n",
    "print(bins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's see what these regions look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_new = pca.inverse_transform(pc1)\n",
    "plt.figure(figsize=(5, 5))\n",
    "sns.scatterplot(train['pickup_lon'], \n",
    "                train['pickup_lat'], \n",
    "                hue=out, s=1, alpha=0.2, label='Regions')\n",
    "plt.legend()\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.title('Pickup locations');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Next, let's create a function `add_region` so we can reuse this workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=1)\n",
    "pca.fit(train[['pickup_lon', 'pickup_lat']])\n",
    "pc1 = pca.transform(train[['pickup_lon', 'pickup_lat']])\n",
    "out, bins = pd.qcut(pc1.flatten(), 3, labels=[1, 2, 3], retbins=True)\n",
    "\n",
    "def add_region(df, pca, bins):\n",
    "    \"\"\"Add a \"region\" column to df_temp.\"\"\"\n",
    "    df_temp = df.copy()\n",
    "    \n",
    "    # Write your code here\n",
    "    pc1 = pca.transform(df_temp[['pickup_lon', 'pickup_lat']])\n",
    "    df_temp['region'] = pd.cut(pc1.flatten(), \n",
    "                               [-np.inf, bins[1], bins[2], np.inf], \n",
    "                               labels=[1, 2, 3])\n",
    "    \n",
    "    return df_temp\n",
    "\n",
    "# Now let's add 'region' to train, val and test\n",
    "train = add_region(train, pca, bins)\n",
    "val = add_region(val, pca, bins)\n",
    "test = add_region(test, pca, bins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Below, we compare the `region`'s calculated from the training dataset and applied to the validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "sns.scatterplot(data=train, x='pickup_lon', y='pickup_lat', hue='region',\n",
    "                s=1, alpha=0.2, label='Train Regions', ax=ax1)\n",
    "ax1.set_xlim([-74.02, -73.92])\n",
    "ax1.set_ylim([40.7, 40.875])\n",
    "ax1.set_xlabel('Longitude')\n",
    "ax1.set_ylabel('Latitude')\n",
    "ax1.legend()\n",
    "\n",
    "sns.scatterplot(data=val, x='pickup_lon', y='pickup_lat', hue='region',\n",
    "                s=1, alpha=0.2, label='Val Regions', ax=ax2)\n",
    "ax2.set_xlim([-74.02, -73.92])\n",
    "ax2.set_ylim([40.7, 40.875])\n",
    "ax1.set_xlabel('Longitude')\n",
    "ax1.set_ylabel('Latitude')\n",
    "ax2.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Pay attention to this important step in the workflow. You'll notice that `pc1` and the `region` bin limits where computed exclusively from the training dataset and then applied to the validation and test datasets. This is important for ensure no data leakage occurs. `#data-leakage`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Question 3d\n",
    "Use `sns.distplot` to create an overlaid histogram comparing the distribution of speeds for morning taxi rides (12am-6am) in the three different regions defined above using `train`. Ponder if there is an association between region and average speed during the night."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Lastly, let's create a feature set that includes our features of interest. Quantitative features are converted to standard units using `StandardScaler`, while categorical features are converted to dummy variables using `pd.get_dummies`. The `period` is not included because it is a linear combination of the `hour`. The `weekend` variable is not included because it is a linear combination of the `day`.  The `speed` is **NOT** included because it was computed from the `duration` and the `distance`. This would be an example of a type of data leakage called target leakage. This occurs when information about the training target leaks into the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Define numerical features to use for modelling\n",
    "num_features = ['pickup_lon', 'pickup_lat', 'dropoff_lon', 'dropoff_lat', \n",
    "                'distance']\n",
    "\n",
    "# Define categorical features to use for modelling\n",
    "cat_features = ['hour', 'day', 'region']\n",
    "\n",
    "# Fit scaler (basically get the mean and stdev) for the training data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train[num_features])\n",
    "\n",
    "def create_features(df):\n",
    "    \"\"\"Create a feature set from taxi ride dataframe df.\"\"\"\n",
    "    scaled = df[num_features].copy()\n",
    "    \n",
    "    # Convert numeric features to standard units\n",
    "    scaled.iloc[:, :] = scaler.transform(scaled) \n",
    "    \n",
    "    # Convert categorical features using dummy encoding\n",
    "    categoricals = [pd.get_dummies(df[s], prefix=s, drop_first=True) for s in cat_features]\n",
    "    \n",
    "    return pd.concat([scaled] + categoricals, axis=1)\n",
    "\n",
    "# Let's test our function\n",
    "create_features(train).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 4. Model Selection\n",
    "In this part, you will select a regression model to predict the duration of a taxi ride."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Question 4a\n",
    "Assign `constant_rmse` to the root mean squared error on the validation set for a constant model that always predicts the mean duration of all training set taxi rides. Its always benefitial to have a simple (naive) baseline to compare our more sophisticated models too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def rmse(errors):\n",
    "    \"\"\"Return the root mean squared error.\"\"\"\n",
    "    return np.sqrt(np.mean(errors ** 2))\n",
    "\n",
    "# Write your code here\n",
    "constant_rmse = ...\n",
    "\n",
    "# Print score\n",
    "print('Constant model validation RMSE: {} seconds'.format(constant_rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Question 4b\n",
    "Assign `simple_rmse` to the root mean squared error on the validation set for a simple linear regression model that uses only the distance of the taxi ride as a feature (and includes an intercept).\n",
    "\n",
    "Simple linear regression means that there is only one feature. Multiple linear regression means that there is more than one feature. In either case, you can use the `LinearRegression` model from `sklearn` to fit the parameters to data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Write your code here\n",
    "...\n",
    "simple_rmse = ...\n",
    "\n",
    "# Print score\n",
    "print('Simple linear regression model validation RMSE: {} seconds'.format(simple_rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Question 4c\n",
    "Assign `linear_rmse` to the root mean squared error on the validation set for a linear regression model fitted to the training set without regularization, using the feature set defined by the `create_features` function from Part 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "...\n",
    "linear_rmse = ...\n",
    "\n",
    "# Print score\n",
    "print('Multiple linear regression model validation RMSE: {} seconds'.format(linear_rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Question 4d\n",
    "For each possible value of `period`, fit an unregularized linear regression model to the subset of the training set in that `period`.  Assign `period_rmse` to the root mean squared error on the validation set for a model that first chooses linear regression parameters based on the observed `period` of the taxi ride, then predicts the duration using those parameters. Again, fit to the training set and use the `create_features` function for features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "model = LinearRegression(fit_intercept=True)\n",
    "val_errors = []\n",
    "\n",
    "for period in np.unique(train['period']):\n",
    "    \n",
    "    # Filter to period\n",
    "    period_train = ...\n",
    "    period_val = ...\n",
    "    \n",
    "    # Fit model\n",
    "    model.fit(...)\n",
    "    \n",
    "    # Compute period errors\n",
    "    period_errors = ...\n",
    "\n",
    "    # Collect errors\n",
    "    val_errors.extend(period_errors)\n",
    "\n",
    "# Compute val score\n",
    "period_rmse = rmse(np.array(val_errors))\n",
    "\n",
    "# Print score\n",
    "print('Period linear regression model validation RMSE: {} seconds'.format(period_rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This approach is a simple form of decision tree regression, where a different regression function is estimated for each possible choice among a collection of choices. In this case, the depth of the tree is only 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Question 4e\n",
    "Instead of predicting duration directly, an alternative is to predict the average speed of the taxi ride using linear regression, then compute an estimate of the duration from the predicted speed and observed distance for each ride.\n",
    "\n",
    "Assign `speed_rmse` to the root mean squared error in the duration predicted by a model that first predicts speed as a linear combination of features from the `create_features` function, fitted on the training set, then calculates duration from the predicted speed and observed distance for the validation set.\n",
    "\n",
    "Hint: Speed is in miles per hour, but duration is measured in seconds. You'll need the fact that there are 60 * 60 = 3,600 seconds in an hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "...\n",
    "speed_rmse = ...\n",
    "\n",
    "# Print score\n",
    "print('Speed multiple linear regression model validation RMSE: {} seconds'.format(speed_rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "At this point, think about why predicting speed leads to a more accurate regression model than predicting duration directly. Consider the figure below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "sns.distplot(train['duration'], ax=ax1)\n",
    "ax1.set_xlabel('Duration, seconds')\n",
    "\n",
    "sns.distplot(train['speed'], ax=ax2)\n",
    "ax2.set_xlabel('Speed, mph')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The `duration` has a large right-skew and a larger dynamics range, which can create challenges. `speed` is much closed to a symmetric distribution, which is the likely explanation for this improved performance. Another option to try would be to use a `log` transformation on `duration`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ax= sns.distplot(np.log(train['duration']))\n",
    "ax.set_xlabel('log Duration, seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Next, let's select the best model based on its validation score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "models = ['constant', 'simple', 'linear', 'period', 'speed']\n",
    "pd.DataFrame.from_dict({\n",
    "    'Model': models,\n",
    "    'Val RMSE': [eval(m + '_rmse') for m in models]\n",
    "}).set_index('Model').plot(kind='barh');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Based on the results presented above, we select the `speed` model, which has the lowest RMSE on the validation dataset.\n",
    "\n",
    "## Question 4f\n",
    "The last step is to unlock out test dataset and compute the RMSE for the `speed` model as our final evaluation of the model's generalization error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "speed_rmse_test = ...\n",
    "\n",
    "# Print score\n",
    "print('Speed multiple linear regression model test RMSE: {} seconds'.format(speed_rmse_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Congratulation, you're done Assignment 7. Review your answers and clean up that code before submitting on Quercus. `#cleancode`**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}