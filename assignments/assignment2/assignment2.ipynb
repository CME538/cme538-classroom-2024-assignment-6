{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CME538 - Introduction to Data Science\n",
    "## Assignment 2 - Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Objectives\n",
    "After completing this assignment, you should be comfortable:\n",
    "\n",
    "- Using `requests` and `BeautifulSoup` to scrape a simple web page.\n",
    "- Wrangle unstructured text data into a DataFrame format.\n",
    "- Use `Pandas` functions to extract information from a DataFrame.\n",
    "- Introduced to the `Folium` package.\n",
    "\n",
    "You are free to add new cells to use as a scratch pad, but make sure to clean you code up and present your answer in the cell indicated with `# Write your code here`.\n",
    "\n",
    "### Marking Breakdown\n",
    "\n",
    "Question | Points\n",
    "--- | ---\n",
    "Question 1a | 1\n",
    "Question 1b | 4\n",
    "Question 1c | 1\n",
    "Question 2a | 5\n",
    "Question 2b | 2\n",
    "Question 2c | 1\n",
    "Question 2d | 1\n",
    "Question 2e | 1\n",
    "Question 2f | 1\n",
    "Question 2g | 1\n",
    "Question 2h | 1\n",
    "Total | 19\n",
    "\n",
    "One of the following marks below will be added to the **Total** above.\n",
    "\n",
    "### Code Quality\n",
    "\n",
    "| Rank | Points | Description |\n",
    "| :-- | :-- | :-- |\n",
    "| Youngling | 1 | Code is unorganized, variables names are not descriptive, redundant, memory-intensive, computationally-intensive, uncommented, error-prone, difficult to understand. |\n",
    "| Padawan | 2 | Code is organized, variables names are descriptive, satisfactory utilization of memory and computational resources, satisfactory commenting, readable. |\n",
    "| Jedi | 3 | Code is organized, easy to understand, efficient, clean, a pleasure to read. #cleancode |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import 3rd party libraries\n",
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "# Import local libraries\n",
    "import utils\n",
    "\n",
    "# Configure Notebook\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's install a super cool geospatial plotting package `Folium`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install folium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "You start a new job at an engineering company and you join a team that is planning to build a new waterfront facility on Lake Ontario. A critical input to the team's design process is information about the swell height and its seasonality (We'll learn more about seasonality in Week 5). Your manager informs you that NOAA (National Oceanic and Atmospheric Administration) has an array of sensors installed throughout the great lakes that measure swell height and direction among other things. \n",
    "\n",
    "In this assignment, you'll be working with the [NOAA - Great Lakes Environmental Research Laboratory (GLERL)](https://www.glerl.noaa.gov/res/glcfs/) dataset which contains forecasts and measurements for Ice Cover, Wave Height, Current Direction, Wind Speed, and others. We have already worked with this dataset in Lecture 3.2. \n",
    "\n",
    "<br>\n",
    "<img src=\"images/noaa.gif\" alt=\"drawing\" width=\"500\"/>\n",
    "<br>\n",
    "\n",
    "Your managers asks you to programmatically pull all available wave data from the GLERL server and report to her the minimum, maximum, and average wave height from the three closest grid points in the dataset to the planned location of the facility (lat = 43.9593°, lon = -78.1677°).  \n",
    "\n",
    "The image below shows the grid point locations in black and the location of the planned facility in red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import 'grid_plot.csv'\n",
    "grid_plot = pd.read_csv('grid_plot.csv')\n",
    "\n",
    "# Create a map of Toronto\n",
    "map1 = folium.Map(location=[43.9593, -78.1677], \n",
    "                 tiles='cartodbpositron', \n",
    "                 zoom_start=8)\n",
    "\n",
    "# Add bike stations to the map\n",
    "for idx, row in grid_plot.iterrows():\n",
    "    folium.Circle(location=[row.lat, row.lon],\n",
    "                  radius=20,\n",
    "                  color='black').add_to(map1)\n",
    "\n",
    "# Add weather stations\n",
    "folium.Marker([43.6532, -79.3832], icon=folium.Icon(color='blue'), popup='UofT').add_to(map1)\n",
    "folium.Marker([43.9593, -78.1677], icon=folium.Icon(color='red'), popup='Facility').add_to(map1)\n",
    "\n",
    "# Display map\n",
    "map1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "### HTML & Web Scraping\n",
    "For Question 1, you'll be using your web scraping skillz to create a reference DataFrame with the names of files that contain wave data for Lake Ontario in 2021.The files can be found [here](https://www.glerl.noaa.gov/emf/waves/GLERL-Donelan-Archive/2021/').\n",
    "\n",
    "The filename format is:\n",
    "LYYYY_MM.F.NC\n",
    "\n",
    "where:\n",
    "- L = lake letter (c=St. Clair, s=Superior, m=Michigan, h=Huron, e=Erie, o=Ontario)\n",
    "- YYYY = year at start of simulation (GMT)\n",
    "- MM = month at start of simulation (GMT)\n",
    "- F = is either `in1` or `out1` (don't worry about this)  \n",
    "- NC = file extension\n",
    "\n",
    "### Question 1a\n",
    "Use `requests.get()` to grab the HTML from this link (https://www.glerl.noaa.gov/emf/waves/GLERL-Donelan-Archive/2021/). Create a new variable named `response` and assign the returned object from `requests.get()` to it. Sometimes the **GLERL** server doesn't return a vaid response, which produces a Python error. You may have to run this cell more than once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here.\n",
    "response = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use `BeautifulSoup` to parse the html object returned by `requests.get()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's use the `.findAll()` method to generate a list of HTML entries for each NOAA file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_rows = soup.findAll('tr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the table data `<td>` we're interested in only appears after line 3 so let's grab everything from the 3th row on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_rows = table_rows[3:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print the first 5 rows of `table_rows` to see what they contain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for table_row in table_rows[0:10]:\n",
    "    print(table_row)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(table_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that `table_rows` contains `<tr></tr>` and `<td></td>` html tags. Each list entry is wrapped in `<tr></tr>` tags and withing these `<tr></tr>` tags, the data we're interest in is wrapped in `<td></td>` tags."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1b\n",
    "Use `table_rows` to create a DataFrame that inludes a row entry for each list item in `table_rows`. There are 54 list items in `table_rows` and therefore, you'll be creating a DataFrame with 54 rows. Create a variable called `noaa_files` and assign the DataFrame to it. The DataFrame should have three columns `filename`, `upload_date`, and `file_size`, which can all be extracted from the HTML snippets in `table_rows`. Make sure to use string method to remove any excess white space. For example, there may be a filename string `'\t  o2021_12.in1.nc'` but it should be entered into the DataFrame as `'co2021_12.in1.nc'`. \n",
    "\n",
    "`noaa_file.head(10)` should return something like this but with different `filename`, `upload_date`, and `file_size`. The table below is just to give you an idea for is expected but yours will look different.\n",
    "<br>\n",
    "<img src=\"images/noaa_files.png\" alt=\"drawing\" width=\"350\"/>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here.\n",
    "...\n",
    "\n",
    "# View DataFrame\n",
    "noaa_files.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, `noaa_files` should have 54 rows. Let's check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('noaa_files has {} rows'.format(noaa_files.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1c\n",
    "Filter `noaa_files` so it only contains `.in1.nc` files for lake Ontario. After filtering, reset the index using `.reset_index(drop=True)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here.\n",
    "noaa_files = ...\n",
    "\n",
    "# View DataFrame\n",
    "noaa_files.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`noaa_files` should now have 8 rows. Let's check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('noaa_files has {} rows'.format(noaa_files.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "Now that we have a DataFrame containing the names of all the files we're interested in, we can start downloading these files and extracting the desired information. Because the **GLERL** server can be a bit unreliable and because they recently retired this database, we've gone ahead and downloaded a bunch of `.wav` for you, which are located in the assignment directory. \n",
    "\n",
    "For Question 2, we are working with a different file because of the recent database changes. For Question 2 we are working with `.wav` (Wave) files with the following naming conversion.\n",
    "\n",
    "The gridded fields filename format is:\n",
    "LYYYYDDDHH.N.WAV\n",
    "\n",
    "where:\n",
    "- L = lake letter (s=Superior, m=Michigan, h=Huron, e=Erie, o=Ontario)\n",
    "- YYYY = year at start of simulation (GMT)\n",
    "- DDD = Day Of Year at start of simulation (GMT)\n",
    "- HH = hr at start of simulation (GMT)\n",
    "- N = Site Number (don't worry about this, it will always be 0)\n",
    "\n",
    "Because we are focused on wave information for Lake Ontario, our files have the following format **oYYYYDDDHH.0.wav** (`o` for Lake Ontario and `.wav` for wave measurements).\n",
    "\n",
    "Let's import the `noaa_files` `.csv` that corresponds to the files we've already downloaded for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noaa_files = pd.read_csv('noaa_files.csv')\n",
    "noaa_files.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also take a look at the `.wav` files we've included for you in the assignment folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[file for file in os.listdir() if '.wav' in file]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 32 rows in the `noaa_files` DataFrame and 32 `.wav` files in the assignment folder. They correspond to the same 32 datetimes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2a\n",
    "Write a function `noaa_file_parser(filename)` that takes a filename as as an argument (e.g. filename = `o202101518.0.wav`) and returns a DataFrame with the following columns.\n",
    "\n",
    "- filename (string)\n",
    "- year (int)\n",
    "- day (int)\n",
    "- hour (int)\n",
    "- grid_number (int)\n",
    "- wave_height (float)\n",
    "- wave_direction (int)\n",
    "- wave_period (float)\n",
    "\n",
    "There are two types of rows in a noaa file (e.g. `o202101518.0.wav`):\n",
    "\n",
    "1. **Time stamp row** - this row indicates the date and time of the grid measurements to follow.\n",
    "    \n",
    "    Example: For file `o202101518.0.wav`, the first row is \n",
    "    \n",
    "    `'2021 015 19     /glcfs/bathy/ontario5km.dat    WAVES                   746'`\n",
    "    \n",
    "    There are six entries in a time stamp row.\n",
    "    \n",
    "    - Year (GMT)\n",
    "    - Day of the year (GMT)\n",
    "    - Hour of the day (GMT)\n",
    "    - Map file. There is a map file for each lake and it relates the grid numbers to latitudes and longitudes. `ontario5km.map` is the Lake Ontario map and its already in your assignment folder.\n",
    "    - File type (WAVES)\n",
    "    - Number of grid points for a particular lake (746 for lake ontario).\n",
    "\n",
    "2. **Measurement row** - this row contains the wave measurements we're interested in. Each measurement row contains wave measurements for a particular gird point. \n",
    "\n",
    "    Example: For file `o202101518.0.wav`, the second row is \n",
    "    \n",
    "    `'1   1.029  231  4.2'`\n",
    "    \n",
    "    There are four entries in a measurement row.\n",
    "    \n",
    "    - grid_number (int)\n",
    "    - wave height (meters) (float)\n",
    "    - wave direction (0 = toward north, 90 = toward east) (int)\n",
    "    - wave period (s) (float)\n",
    "    \n",
    "The NOAA files contain this repeating pattern:\n",
    "```\n",
    "2021 015 19     /glcfs/bathy/ontario5km.dat    WAVES                   746\n",
    "1   1.029  231  4.2\n",
    "2   0.932  228  4.0\n",
    "...\n",
    "745   0.656  312  3.4\n",
    "746   0.581  331  3.4\n",
    "2021 015 20     /glcfs/bathy/ontario5km.dat    WAVES                   746\n",
    "1   1.142  228  4.4\n",
    "2   1.042  227  4.3\n",
    "...\n",
    "745   0.642  316  3.4\n",
    "746   0.600  345  3.8\n",
    "2021 015 21     /glcfs/bathy/ontario5km.dat    WAVES                   746\n",
    "1   1.205  224  4.9\n",
    "2   1.109  223  4.7\n",
    "...\n",
    "745   0.635  317  3.4\n",
    "746   0.592  350  3.8\n",
    "```\n",
    "    \n",
    "Your function `noaa_file_parser(filename='o202101518.0.wav')` should return the following DataFrame (first 10 rows shown). \n",
    "<br>\n",
    "<img src=\"images/noaa_file_parser.png\" alt=\"drawing\" width=\"550\"/>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the code in the cell below to explore NOAA file `'o202101518.0.wav'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "open('o202101518.0.wav', 'r').read().split('\\n')[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here.\n",
    "def noaa_file_parser(filename):\n",
    "    ...\n",
    "\n",
    "# Print DataFrame head. Here I am using the first file in the assignment directory to print (o202101518.0.wav).\n",
    "file_grid_data = noaa_file_parser(filename=noaa_files.loc[0, 'filename'])\n",
    "file_grid_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`file_grid_data` should have 4476 rows. Let's check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('file_grid_data has {} rows'.format(file_grid_data.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2b\n",
    "Next, use the function you just built (`noaa_file_parser()`) to parse each noaa file in the DataFrame `noaa_files`. Run `noaa_file_parser()` in a `for` loop that loops through each file in `noaa_files['filename]`. At the end of each loop, add the DataFrame returned by `noaa_file_parser()` to a list. After looping through every file in the DataFrame `noaa_files`, use `pd.concat()` to combine all the DataFrames. The result will be one large DataFrame containing all the wave measurement data. Create a variable `grid_data` and assign this new DataFrame to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here.\n",
    "...\n",
    "\n",
    "# View DataFrame\n",
    "grid_data.head() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a quick check, `grid_data` should have 32 unique files and 143232 rows in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('There are {} unique files in grid_data and {} rows'.format(grid_data['filename'].nunique(), grid_data.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2c\n",
    "The map file `ontario5km.map` is located in the assignment directory and contains the latitude and longitude information for each grid_number in `grid_data`. Import the `ontario5km.map` as a DataFrame with the following column names:\n",
    "\n",
    "- grid_number\n",
    "- fortran column\n",
    "- fortran row\n",
    "- lat\n",
    "- lon\n",
    "- depth\n",
    "\n",
    "The ascii map file structure is:\n",
    "\n",
    "NNNNN III JJJ LL.LLLLL LL.LLLLL DDD\n",
    "\n",
    "where:\n",
    "- NNNNN = sequence number (grid_number)\n",
    "- III = fortran column\n",
    "- JJJ = fortran row\n",
    "- LL.LLLLL = lat (decimal degrees N)\n",
    "- LL.LLLLL = lon (decimal degrees W)\n",
    "- DDD = depth (m)\n",
    "\n",
    "**Hint:** The columns are white-space delimited.\n",
    "\n",
    "Create a variable `map_file` and assign this new DataFrame to it.\n",
    "\n",
    "Below is a quick view of what the file `'ontario5km.map'` contains. Notice there are no column names so we'll have to add them ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "open('ontario5km.map', 'r').read().split('\\n')[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here.\n",
    "map_file = ...\n",
    "\n",
    "# Because Longitude is in units of (decimal degrees W), let's convert these to negative values.\n",
    "map_file['lon'] = map_file['lon'] * -1\n",
    "\n",
    "# View DataFrame\n",
    "map_file.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2d\n",
    "We want our wave measurements in `grid_data` to have a geographic location so we can find the 3 closest grid points to the planned facility. To do this, we must use `pd.merge()` to map columns `lat` and `lon` from `map_file` to `grid_data`. Create a variable `grid_data_final` and assign this new DataFrame to it.\n",
    "\n",
    "`grid_data_final.head()` should look like this:\n",
    "<br>\n",
    "<img src=\"images/grid_data.png\" alt=\"drawing\" width=\"700\"/>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here.\n",
    "grid_data_final = ...\n",
    "\n",
    "# View DataFrame\n",
    "grid_data_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2e\n",
    "Save `grid_data_final` to the root path with file name `'grid_data_final.csv'`. Make sure to not include an index column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here.\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2f\n",
    "Next, we need to find the three grid points that are closest to the location of the planned facility. The facility will be located at (lat = 43.9593°, lon = -78.1677°). A helper function has been included to calculate the distance between any two points (lat1, lon1 and lat2, lon2). We’ll use the Haversine (or Great Circle) distance formula, which takes the latitude and longitude of two points, adjusts for Earth’s curvature, and calculates the straight-line distance between them. \n",
    "\n",
    "You can call this function as follows:\n",
    "\n",
    "`utils.haversine(lat1, lon1, lat2, lon2)`\n",
    "\n",
    "The distance is returned in kilometers.\n",
    "\n",
    "First, create a new column called `distance` for DataFrame `grid_data_final`. You can use the `.apply()` method to apply `utils.haversine(lat1, lon1, lat2, lon2)` to each row. \n",
    "\n",
    "`grid_data_final.head()` should look like this:\n",
    "<br>\n",
    "<img src=\"images/grid_data_distance.png\" alt=\"drawing\" width=\"700\"/>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here.\n",
    "grid_data_final['distance'] = ...\n",
    "\n",
    "# View DataFrame\n",
    "grid_data_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2g\n",
    "Next, create a new DataFrame called `closest_points` which only contains wave measurement data from the three closest grid points to the planned facility at (lat = 43.9593°, lon = -78.1677°)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here.\n",
    "... \n",
    "\n",
    "# View DataFrame\n",
    "closest_points.head() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a quick check, `closest_points` should have 576 rows and 3 unique `'grid_number'`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('closest_points has {} rows and {} unique grid_number.'.format(closest_points.shape[0], closest_points['grid_number'].nunique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validate that the three points you've found make sense visually with a plot. The three closest point are shown as red markers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import 'grid_plot.csv'\n",
    "grid_plot = pd.read_csv('grid_plot.csv')\n",
    "\n",
    "# Create a map of Toronto\n",
    "map2 = folium.Map(location=[43.9593, -78.1677], \n",
    "                 tiles='cartodbpositron', \n",
    "                 zoom_start=8)\n",
    "\n",
    "# Add bike stations to the map\n",
    "for idx, row in grid_plot.iterrows():\n",
    "    folium.Circle(location=[row.lat, row.lon],\n",
    "                  radius=20,\n",
    "                  color='black').add_to(map2)\n",
    "\n",
    "# Add bike stations to the map\n",
    "for idx, row in closest_points.iterrows():\n",
    "    folium.Circle(location=[row.lat, row.lon],\n",
    "                  radius=20,\n",
    "                  color='red').add_to(map2)\n",
    "    \n",
    "# Add weather stations\n",
    "folium.Marker([43.6532, -79.3832], icon=folium.Icon(color='blue'), popup='UofT').add_to(map2)\n",
    "folium.Marker([43.9593, -78.1677], icon=folium.Icon(color='red'), popup='Facility').add_to(map2)\n",
    "\n",
    "# Display map\n",
    "map2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2h\n",
    "The final step is to use the `closest_points` DataFrame to calculate the minimum, maximum and average wave height across the three closest points. Create variables `wave_height_min`, `wave_height_max`, `wave_height_mean` and assign the computed values to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here.\n",
    "wave_height_min = ...\n",
    "wave_height_max = ...\n",
    "wave_height_mean = ...\n",
    "\n",
    "# Print answers\n",
    "print('Wave height min: {} m\\nWave height max: {} m\\nWave height mean: {} m'.format(wave_height_min, \n",
    "                                                                                    wave_height_max, \n",
    "                                                                                    wave_height_mean)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Congratulation, you're done Assignment 2. Review your answers and clean up that code before submitting on Quercus. `#cleancode`** "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
