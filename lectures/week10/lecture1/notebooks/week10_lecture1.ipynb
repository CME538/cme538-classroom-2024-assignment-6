{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CME538 - Introduction to Data Science\n",
    "## Lecture 10.1 - Feature Extraction\n",
    "\n",
    "### Lecture Structure\n",
    "1. [Principal Component Analysis (PCA)](#section1)\n",
    "2. [Application 1: mtcars Dataset](#section2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Configure Notebook\n",
    "%matplotlib inline\n",
    "plt.style.use('fivethirtyeight')\n",
    "sns.set_context(\"notebook\")\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section1'></a>\n",
    "# Principal Component Analysis (PCA)\n",
    "### Import Trees Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trees = pd.read_csv('trees.csv', index_col=0)\n",
    "trees.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do these columns mean?\n",
    "- **Girth (numeric)**: Tree diameter in inches.\n",
    "- **Height (numeric)**: Height in feet.\n",
    "- **Volume (numeric)**: Volume of timber in cubic feet.\n",
    "\n",
    "Let's try plotting this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(trees);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's only consider `Girth` and `Volume` to keep things simple and easy to visualize (2D). Let's plot `Girth` vs `Volume`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.scatterplot(x='Girth', y='Volume', data=trees)\n",
    "ax.set_xlabel('Girth (Diameter), inches', fontsize=16)\n",
    "ax.set_ylabel('Volume, feet$^{3}$', fontsize=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try PCA using sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, let's try using sklearn to get the first two principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Define df for features of interest\n",
    "X = trees[['Girth', 'Volume']]\n",
    "\n",
    "# Scale input\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "# Initialize PCA object and set the number of \n",
    "# components to 2\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# Compute the components using .fit_transform()\n",
    "X_transformed = pca.fit_transform(X_scaled)\n",
    "X_transformed = pd.DataFrame(data=X_transformed, \n",
    "                             columns=['pc1', 'pc2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we apply our PCA transformation to the original data, we get two new columns. Column `0` is `PC1` and column `1` is `PC2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_transformed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's plot them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.scatterplot(x='pc1', y='pc2', data=X_transformed)\n",
    "ax.set_xlim([-4, 4])\n",
    "ax.set_ylim([-4, 4])\n",
    "ax.set_xlabel('Principal Component 1 (PC1)', fontsize=16)\n",
    "ax.set_ylabel('Principal Component 2 (PC2)', fontsize=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, how much variance does each component account for?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('PC1 total explained variance: {:0.6f}'.format(pca.explained_variance_[0]))\n",
    "print('PC2 total explained variance: {:0.6f}'.format(pca.explained_variance_[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('PC1 total explained variance: {:0.2f}%'.format(pca.explained_variance_ratio_[0]*100))\n",
    "print('PC2 total explained variance: {:0.2f}%'.format(pca.explained_variance_ratio_[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, using `Numpy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('PC1 total explained variance: {:0.6f}'.format(np.var(X_transformed['pc1'], ddof=1)))\n",
    "print('PC2 total explained variance: {:0.6f}'.format(np.var(X_transformed['pc2'], ddof=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's plot the distributions for `pc1` and `pc2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.histplot(data=X_transformed, kde=True, stat='density')\n",
    "ax.set_xlim([-4, 4]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.jointplot(data=X_transformed, \n",
    "                   x='pc1', y='pc2', \n",
    "                   xlim=(-4, 4), ylim=(-4, 4),\n",
    "                   marginal_kws=dict(binwidth=0.25, rug=True))\n",
    "ax.set_axis_labels('Principal Component 1 (PC1)', \n",
    "                   'Principal Component 2 (PC2)', fontsize=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try coding this up ourselves\n",
    "First, let's create `X` again, which is a DataFrame with two columns (`Girth` and `Volume`). Again, we'll keep things simple and easy to visualize by only using 2 features but this would work will all three (`Girth`, `Volume`, `height`) or more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = trees[['Girth', 'Volume']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's scale out fatures by substracting the mean and dividing by the standard deviation. This is called standardizing the data. Let's call this new array `Z`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = pd.DataFrame(data=StandardScaler().fit_transform(X), columns=X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does `Z` look like? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next thing we have to do is compute the covariance matrix of `Z`. There are a few ways we can compute the covariance matrix in `Python`. Let's try them all.\n",
    "\n",
    "#### Method 1\n",
    "We can take `Z`, transpose it, and multiply the transposed matrix by `Z`  like this $ZZ^{T}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_cov1 = np.matmul(Z.T, Z) / (len(Z) - 1)\n",
    "print(Z_cov1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 2\n",
    "And, of course, `Numpy` has a built-in function for this `np.cov()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_cov2 = np.cov(Z.T)\n",
    "print(Z_cov2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we have to compute the eigenvectors and their corresponding eigenvalues for the matrix `Z`. Let's do this using `Numpy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eig_val, eig_vec = np.linalg.eig(Z_cov2)\n",
    "print('Eigen Values:\\n{}\\n'.format(eig_val))\n",
    "print('Eigen Vectors:\\n{}'.format(eig_vec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, and let's compare this to the output from sklearn (`.explained_variance_`, `.components_`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Explained Variance:\\n{}\\n'.format(pca.explained_variance_))\n",
    "print('Principal Components:\\n{}'.format(pca.components_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmmmmmmmm, something doesn't look right when comparing `eig_vec` to `pca.components_`. While `PCA()` lists the entries of an eigenvectors row-wise, `np.linalg.eig()` lists the entries of the eigenvectors column-wise. A quick transpose to `eig_vec` solves this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eig_val, eig_vec = np.linalg.eig(Z_cov2)\n",
    "print('Eigen Values:\\n{}\\n'.format(eig_val))\n",
    "print('Eigen Vectors:\\n{}'.format(eig_vec.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can see how eigen decomposition can be used to compute the principal components of a feature matrix. \n",
    "\n",
    "The last thing we have to do is transform our data into this new coordinate system. This is what we did earlier using `pca.fit_transform()` from `sklearn` but how do we do it without relying on this very nice package?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_transformed = np.matmul(Z, eig_vec)\n",
    "Z_transformed.columns = ['pc1', 'pc2']\n",
    "Z_transformed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's plot to see if we get the same as when using `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.scatterplot(x='pc1', y='pc2', data=Z_transformed)\n",
    "ax.set_xlim([-4, 4])\n",
    "ax.set_ylim([-4, 4])\n",
    "ax.set_xlabel('Principal Component 1 (PC1)\\n{:.2f}% explained var'.format(eig_val[0] / sum(eig_val) * 100), fontsize=16)\n",
    "ax.set_ylabel('Principal Component 2 (PC2)\\n{:.2f}% explained var'.format(eig_val[1] / sum(eig_val) * 100), fontsize=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It worked!!!! But that was a lot of work. Moving, forward, we'll use the `sklearn` implementation.\n",
    "\n",
    "Laslty, let's plot our vectors on the original data and the transformed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_vector(v0, v1, pc, ax=None):\n",
    "    ax = ax or plt.gca()\n",
    "    arrowprops=dict(arrowstyle='->', linewidth=2,\n",
    "                    shrinkA=0, shrinkB=0, color='#fc4f30')\n",
    "    ax.annotate('', v1, v0, arrowprops=arrowprops)\n",
    "    ax.text(v1[0]+0.25, v1[1]+0.25, pc, color='#fc4f30', ha='center', va='center', fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot data\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 7))\n",
    "fig.subplots_adjust(wspace=0.2)\n",
    "\n",
    "# Plot Girth vs Volume\n",
    "ax[0].scatter(Z['Girth'], Z['Volume'])\n",
    "for pc, (length, vector) in enumerate(zip(pca.explained_variance_, \n",
    "                                          pca.components_)):\n",
    "    v = vector * 3 * np.sqrt(length)\n",
    "    draw_vector(pca.mean_, pca.mean_ + v, 'PC{}'.format(pc+1), ax[0])\n",
    "ax[0].axis('equal')\n",
    "ax[0].set_xlabel('Girth (Scaled)', fontsize=16)\n",
    "ax[0].set_ylabel('Volume (Scaled)', fontsize=16)\n",
    "ax[0].set_xlim([-4, 4])\n",
    "ax[0].set_ylim([-4, 4])\n",
    "\n",
    "# Plot PC1 vs PC2\n",
    "ax[1].scatter(X_transformed['pc1'], X_transformed['pc2'])\n",
    "for pc, (length, vector) in enumerate(zip(pca.explained_variance_, \n",
    "                                          pca.components_.T)):\n",
    "    v = vector * 3\n",
    "    draw_vector(pca.mean_, pca.mean_ + v, X.columns[pc], ax[1])\n",
    "ax[1].axis('equal')\n",
    "ax[1].set_xlabel('Principal Component 1 (PC1)\\n{:.2f}% explained var'.format(eig_val[0] / sum(eig_val) * 100), fontsize=16)\n",
    "ax[1].set_ylabel('Principal Component 2 (PC2)\\n{:.2f}% explained var'.format(eig_val[1] / sum(eig_val) * 100), fontsize=16)\n",
    "ax[1].set_xlim([-4, 4])\n",
    "ax[1].set_ylim([-4, 4]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section2'></a>\n",
    "# Application 1: mtcars Dataset\n",
    "## mtcars Dataset\n",
    "Let's move on to a slightly more complex dataset `mtcars`. This dataset consists of data on 32 models of car, taken from an issue of the 1974 Motor Trend magazine. There are 11 features expressed in varying imperial units.\n",
    "- `mpg`: Fuel consumption (Miles per (US) gallon): more powerful and heavier cars tend to consume more fuel.\n",
    "- `cyl`: Number of cylinders: more powerful cars often have more cylinders.\n",
    "- `disp`: Displacement (cu.in.): the combined volume of the engine's cylinders.\n",
    "- `hp`: Gross horsepower: this is a measure of the power generated by the car.\n",
    "- `drat`: Rear axle ratio: this describes how a turn of the drive shaft corresponds to a turn of the wheels. Higher values will decrease fuel efficiency.\n",
    "- `wt`: Weight (1000 lbs): pretty self-explanatory!\n",
    "- `qsec`: 1/4 mile time: the cars speed and acceleration.\n",
    "- `vs`: Engine block: this denotes whether the vehicle's engine is shaped like a \"V\", or is a more common straight shape.\n",
    "- `am`: Transmission: this denotes whether the car's transmission is automatic (0) or manual (1).\n",
    "- `gear`: Number of forward gears: sports cars tend to have more gears.\n",
    "- `carb`: Number of carburetors: associated with more powerful engines.\n",
    "\n",
    "Let's import the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtcars = pd.read_csv('mtcars.csv')\n",
    "mtcars.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's compute the principal components using `sklearn`. Because `PCA` works best with numerical data, let's exclude the categorical variables (`model`, `vs` and `am`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtcars.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get features\n",
    "mtcars_features = mtcars.drop(['model', 'vs', 'am', 'carb', 'country'], axis=1)\n",
    "\n",
    "# Initialize scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Scale features\n",
    "mtcars_features_scaled = pd.DataFrame(data=scaler.fit_transform(mtcars_features),\n",
    "                                      columns=mtcars_features.columns)\n",
    "\n",
    "# Initialize PCA object \n",
    "pca = PCA()\n",
    "\n",
    "# Compute principal components\n",
    "pca.fit(mtcars_features_scaled)\n",
    "\n",
    "# Transform features into new coordinate system\n",
    "X_transformed = pca.transform(mtcars_features_scaled)\n",
    "X_transformed = pd.DataFrame(data=X_transformed, \n",
    "                             columns=['pc{}'.format(comp+1) for \n",
    "                                      comp in range(mtcars_features.shape[1])])\n",
    "\n",
    "# View DataFrame\n",
    "X_transformed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I'll just create a DataFrame to easily summarize out principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_summary = pd.DataFrame(\n",
    "    {'Variance': pca.explained_variance_,\n",
    "     'Proportion of Variance': pca.explained_variance_ratio_,\n",
    "     'Cumulative Proportion': np.cumsum(pca.explained_variance_ratio_)}\n",
    ").T\n",
    "pca_summary.columns = ['PC{}'.format(comp+1) for comp in range(mtcars_features.shape[1])]\n",
    "pca_summary.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's create what is called a `scree plot`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(pca_summary.shape[1]), \n",
    "         pca_summary.loc['Proportion of Variance', :]*100, '-o', label='Variance')\n",
    "plt.plot(np.arange(pca_summary.shape[1]), \n",
    "         pca_summary.loc['Cumulative Proportion', :]*100, '-o', label='Cummulative Variance')\n",
    "plt.xticks(np.arange(pca_summary.shape[1]), \n",
    "           ['PC{}'.format(comp+1) for comp in range(mtcars_features.shape[1])]);\n",
    "plt.legend()\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Proportion of Explained Variance, %');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `scree` plot shows us that after `PC3`, are minimal gains in terms of the explained variance when adding additional principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "fig.subplots_adjust(wspace=0.15)\n",
    "ax.scatter(X_transformed['pc1'], X_transformed['pc2'], alpha=0.5)\n",
    "\n",
    "ax.axis('equal')\n",
    "ax.set_xlabel('Principal Component 1 (PC1)\\n{:.2f}% explained var'.format(pca_summary.iloc[1, 0]*100), fontsize=16)\n",
    "ax.set_ylabel('Principal Component 2 (PC2)\\n{:.2f}% explained var'.format(pca_summary.iloc[1, 1]*100), fontsize=16)\n",
    "ax.set_xlim([-4, 6])\n",
    "ax.set_ylim([-5, 5]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "fig.subplots_adjust(wspace=0.15)\n",
    "for pc, vector in enumerate(pca.components_.T):\n",
    "    v = vector[0:2] * 6\n",
    "    draw_vector([0, 0], [0, 0] + v, mtcars_features.columns[pc], ax)\n",
    "ax.scatter(X_transformed['pc1'], X_transformed['pc2'], alpha=0.5)\n",
    "\n",
    "ax.axis('equal')\n",
    "ax.set_xlabel('Principal Component 1 (PC1)\\n{:.2f}% explained var'.format(pca_summary.iloc[1, 0]*100), fontsize=16)\n",
    "ax.set_ylabel('Principal Component 2 (PC2)\\n{:.2f}% explained var'.format(pca_summary.iloc[1, 1]*100), fontsize=16)\n",
    "ax.set_xlim([-4, 6])\n",
    "ax.set_ylim([-5, 5]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "fig.subplots_adjust(wspace=0.15)\n",
    "for idx, txt in enumerate(mtcars['model'].tolist()):\n",
    "    ax.annotate(txt, (X_transformed.iloc[idx]['pc1'], \n",
    "                      X_transformed.iloc[idx]['pc2']), \n",
    "                color=[0.3, 0.3, 0.3], alpha=0.5)\n",
    "    \n",
    "for pc, vector in enumerate(pca.components_.T):\n",
    "    v = vector[0:2] * 6\n",
    "    draw_vector([0, 0], [0, 0] + v, mtcars_features.columns[pc], ax)\n",
    "ax.scatter(X_transformed['pc1'], X_transformed['pc2'], alpha=0.75)\n",
    "\n",
    "ax.axis('equal')\n",
    "ax.set_xlabel('Principal Component 1 (PC1)\\n{:.2f}% explained var'.format(pca_summary.iloc[1, 0]*100), fontsize=16)\n",
    "ax.set_ylabel('Principal Component 2 (PC2)\\n{:.2f}% explained var'.format(pca_summary.iloc[1, 1]*100), fontsize=16)\n",
    "ax.set_xlim([-4, 6])\n",
    "ax.set_ylim([-5, 5]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "fig.subplots_adjust(wspace=0.15)\n",
    "colors = {'Japan': \"#1a53ff\", 'US': \"#00bfa0\", 'Europe': \"#dc0ab4\"}\n",
    "for idx, (model, country) in enumerate(zip(mtcars['model'].tolist(), mtcars['country'].tolist())):\n",
    "    ax.annotate(model, \n",
    "                (X_transformed.iloc[idx]['pc1'], X_transformed.iloc[idx]['pc2']), \n",
    "                color=colors[country],\n",
    "                alpha=0.5)\n",
    "    \n",
    "for pc, vector in enumerate(pca.components_.T):\n",
    "    v = vector[0:2] * 6\n",
    "    draw_vector([0, 0], [0, 0] + v, mtcars_features.columns[pc], ax)\n",
    "ax.scatter(X_transformed['pc1'], X_transformed['pc2'], alpha=0.75)\n",
    "\n",
    "ax.axis('equal')\n",
    "ax.set_xlabel('Principal Component 1 (PC1)\\n{:.2f}% explained var'.format(pca_summary.iloc[1, 0]*100), fontsize=16)\n",
    "ax.set_ylabel('Principal Component 2 (PC2)\\n{:.2f}% explained var'.format(pca_summary.iloc[1, 1]*100), fontsize=16)\n",
    "ax.set_xlim([-4, 6])\n",
    "ax.set_ylim([-5, 5]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "fig.subplots_adjust(wspace=0.15)\n",
    "colors = {'Japan': \"#1a53ff\", 'US': \"#00bfa0\", 'Europe': \"#dc0ab4\"}\n",
    "for idx, (model, country) in enumerate(zip(mtcars['model'].tolist(), mtcars['country'].tolist())):\n",
    "    \n",
    "    ax.annotate('{} ({})'.format(model, country), \n",
    "                (X_transformed.iloc[idx]['pc3'], X_transformed.iloc[idx]['pc4']), \n",
    "                color=colors[country],\n",
    "                alpha=0.5)\n",
    "    \n",
    "for pc, vector in enumerate(pca.components_.T):\n",
    "    v = vector[0:2] * 2\n",
    "    draw_vector([0, 0], [0, 0] + v, mtcars_features.columns[pc], ax)\n",
    "ax.scatter(X_transformed['pc3'], X_transformed['pc4'], alpha=0.75)\n",
    "\n",
    "ax.axis('equal')\n",
    "ax.set_xlabel('Principal Component 1 (PC1)\\n{:.2f}% explained var'.format(pca_summary.iloc[1, 2]*100), fontsize=16)\n",
    "ax.set_ylabel('Principal Component 2 (PC2)\\n{:.2f}% explained var'.format(pca_summary.iloc[1, 3]*100), fontsize=16)\n",
    "ax.set_xlim([-6, 9])\n",
    "ax.set_ylim([-5, 5]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bugatti = pd.DataFrame([{'model': 'Bugatti Veyron', 'mpg': 13.3, 'cyl': 16, \n",
    "                         'disp': 487.8, 'hp': 987, 'drat': 3.643, 'wt': 4.387, \n",
    "                         'qsec': 9.9, 'vs': 2, 'am': 1, 'gear': 7, 'carb': 0, \n",
    "                         'country': 'Europe'}])\n",
    "mtcars = pd.concat([mtcars, bugatti]).reset_index(drop=True)\n",
    "mtcars.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get features\n",
    "mtcars_features = mtcars.drop(['model', 'vs', 'am', 'carb', 'country'], axis=1)\n",
    "\n",
    "# Scale features\n",
    "mtcars_features_scaled = pd.DataFrame(data=scaler.transform(mtcars_features),\n",
    "                                      columns=mtcars_features.columns)\n",
    "\n",
    "# Transform features into new coordinate system\n",
    "X_transformed = pca.transform(mtcars_features_scaled)\n",
    "X_transformed = pd.DataFrame(data=X_transformed, \n",
    "                             columns=['pc{}'.format(comp+1) for \n",
    "                                      comp in range(mtcars_features.shape[1])])\n",
    "\n",
    "# View DataFrame\n",
    "X_transformed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "fig.subplots_adjust(wspace=0.15)\n",
    "colors = {'Japan': \"#1a53ff\", 'US': \"#00bfa0\", 'Europe': \"#dc0ab4\"}\n",
    "for idx, (model, country) in enumerate(zip(mtcars['model'].tolist(), mtcars['country'].tolist())):\n",
    "    \n",
    "    ax.annotate(model, \n",
    "                (X_transformed.iloc[idx]['pc1'], X_transformed.iloc[idx]['pc2']), \n",
    "                color=colors[country],\n",
    "                alpha=0.5)\n",
    "    \n",
    "for pc, vector in enumerate(pca.components_.T):\n",
    "    v = vector[0:2] * 6\n",
    "    draw_vector([0, 0], [0, 0] + v, mtcars_features.columns[pc], ax)\n",
    "ax.scatter(X_transformed['pc1'], X_transformed['pc2'], alpha=0.75)\n",
    "\n",
    "ax.axis('equal')\n",
    "ax.set_xlabel('Principal Component 1 (PC1)\\n{:.2f}% explained var'.format(pca_summary.iloc[1, 0]*100), fontsize=16)\n",
    "ax.set_ylabel('Principal Component 2 (PC2)\\n{:.2f}% explained var'.format(pca_summary.iloc[1, 1]*100), fontsize=16)\n",
    "ax.set_xlim([-10, 10])\n",
    "ax.set_ylim([-6, 12]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dataset \n",
    "mtcars = pd.read_csv('mtcars.csv')\n",
    "\n",
    "# Get features\n",
    "mtcars_features = mtcars.drop(['model', 'vs', 'am', 'carb', 'country'], axis=1)\n",
    "\n",
    "# Scale features\n",
    "mtcars_features_scaled = pd.DataFrame(data=scaler.transform(mtcars_features),\n",
    "                                      columns=mtcars_features.columns)\n",
    "\n",
    "# Combine features and Principal Components\n",
    "combined_df = pd.concat([mtcars_features_scaled, X_transformed], axis=1)\n",
    "\n",
    "# Compute the correlation matrix\n",
    "correlation = combined_df.corr()\n",
    "\n",
    "# Plot correlatin between features and Principal Components.\n",
    "correlation_plot_data = correlation.loc[mtcars_features_scaled.columns, \n",
    "                                        X_transformed.columns]\n",
    "fig, ax = plt.subplots(figsize=(20, 7))\n",
    "sns.set(font_scale=2)\n",
    "sns.heatmap(correlation_plot_data, cmap='bwr', linewidths=.7, \n",
    "            annot=True, fmt='.2f', vmin=-1, vmax=1, ax=ax,\n",
    "            cbar_kws={'label': 'Correlation'})\n",
    "ax.xaxis.set_tick_params(labelsize=30)\n",
    "ax.yaxis.set_tick_params(labelsize=30, labelrotation=0)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
