{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CME538 - Introduction to Data Science\n",
    "## Lecture 10.2 - Clustering\n",
    "\n",
    "### Lecture Structure\n",
    "1. [Create Dummy Dataset](#section1)\n",
    "2. [Interactive k-means Visualizer](#section2)\n",
    "3. [Import Iris Dataset](#section3)\n",
    "4. [k-means in Scikit-Learn](#section4)\n",
    "5. [Feature Scaling](#section5)\n",
    "6. [Minimizing Inertia](#section6)\n",
    "7. [Hierarchical Clustering in Scikit-Learn](#section7)\n",
    "8. [Picking the Number of Clusters](#section8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from copy import deepcopy\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from ipywidgets import interact, fixed, IntSlider\n",
    "from sklearn.metrics import pairwise_distances_argmin\n",
    "\n",
    "# Configure Notebook\n",
    "%matplotlib inline\n",
    "plt.style.use('fivethirtyeight')\n",
    "sns.set_context(\"notebook\")\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section1'></a>\n",
    "# Create Dummy Dataset\n",
    "### Create Blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(centers, cluster_std, n_samples):\n",
    "\n",
    "    # Create Data\n",
    "    features, true_labels = make_blobs(n_samples=n_samples,\n",
    "                                       centers=centers,\n",
    "                                       cluster_std=cluster_std,\n",
    "                                       random_state=42)\n",
    "\n",
    "    # Create DataFrame\n",
    "    data = pd.DataFrame(data=features, columns=['Feature 1', 'Feature 2'])\n",
    "    data['Cluster ID'] = true_labels\n",
    "    data['Cluster ID'] = data['Cluster ID'].astype('str')\n",
    "\n",
    "    return data\n",
    "    \n",
    "data = create_dataset(centers=3, cluster_std=2.75, n_samples=200)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(7, 7))\n",
    "fig.subplots_adjust(wspace=0.15)\n",
    "sns.scatterplot(data=data, x='Feature 1', y='Feature 2', hue='Cluster ID')\n",
    "ax.set_xlabel('Feature 1', fontsize=16)\n",
    "ax.set_ylabel('Feature 2', fontsize=16)\n",
    "ax.set_xlim([-17, 17])\n",
    "ax.set_ylim([-17, 17]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section2'></a>\n",
    "# Interactive k-means Visualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initiate_centers(num_clusters):\n",
    "    x = np.random.randint(-10, 10, size=num_clusters)\n",
    "    y = np.random.randint(-10, 10, size=num_clusters)\n",
    "    return np.array(list(zip(x, y)), dtype=np.float32)\n",
    "\n",
    "def draw_centers(num_clusters, ax, centers, factor=1, alpha=1.0):\n",
    "    colors = ['#008fd5', '#fc4f30', '#e5ae38', '#6d904f', '#8b8b8b', '#810f7c']\n",
    "    ax.scatter(centers[:, 0], centers[:, 1], c=colors[0:num_clusters], s=200 * factor, alpha=alpha)\n",
    "    ax.scatter(centers[:, 0], centers[:, 1], c='black', s=50 * factor, alpha=alpha)\n",
    "    \n",
    "def distance(centers, center_old):\n",
    "    return np.linalg.norm(centers - center_old, axis=1)\n",
    "\n",
    "def ssd(center, data):\n",
    "    return np.sum(np.power(center - data, 2))\n",
    "\n",
    "def kmeans_step_plot(steps, operations, num_clusters, random_seed, data):\n",
    "    \n",
    "    np.random.seed(random_seed)\n",
    "    colors = ['#008fd5', '#fc4f30', '#e5ae38', '#6d904f', '#8b8b8b', '#810f7c']\n",
    "    centers = initiate_centers(num_clusters)\n",
    "    centers_old = np.zeros(centers.shape)\n",
    "    cluster_labels = np.zeros(data.shape[0], dtype=int)\n",
    "    diffs = []\n",
    "    losses = []\n",
    "    \n",
    "    for step, operation in zip(range(steps), operations):\n",
    "        \n",
    "        # Assigning each value to its closest cluster\n",
    "        for i in range(data.shape[0]):\n",
    "            distances = distance(data[i], centers)\n",
    "            cluster_labels[i] = np.argmin(distances)\n",
    "        \n",
    "        if operation == 1:\n",
    "            \n",
    "            # Store the old centers\n",
    "            centers_old = deepcopy(centers)\n",
    "\n",
    "            # Find the new centers and compute loss\n",
    "            loss = 0\n",
    "            for i in range(num_clusters):\n",
    "                points = [data[j] for j in range(data.shape[0]) if cluster_labels[j] == i]\n",
    "                if len(points) > 0:\n",
    "                    centers[i] = np.mean(points, axis=0)\n",
    "                    loss += ssd(centers[i], points)\n",
    "                else:\n",
    "                    loss += losses[-1]\n",
    "            losses.append(loss)\n",
    "            \n",
    "            # Update the error\n",
    "            diffs.append(np.sum(distance(centers, centers_old)))\n",
    "    \n",
    "    \n",
    "    # Setup figure\n",
    "    fig = plt.figure(figsize=(15, 7))\n",
    "    fig.subplots_adjust(wspace=0.22, hspace=0.3)\n",
    "    ax1 = plt.subplot2grid((2, 2), (0, 0), rowspan=2)\n",
    "    ax2 = plt.subplot2grid((2, 2), (0, 1))\n",
    "    ax3 = plt.subplot2grid((2, 2), (1, 1))\n",
    "    \n",
    "    # Scatter Plot\n",
    "    if steps == 0 or operation == 1:\n",
    "        sns.scatterplot(x=data[:, 0], y=data[:, 1], color='#8b8b8b', ax=ax1)\n",
    "    else:\n",
    "        sns.scatterplot(x=data[:, 0], y=data[:, 1], hue=cluster_labels, palette=colors[0:num_clusters], ax=ax1)\n",
    "    draw_centers(num_clusters, ax1, centers, factor=1, alpha=1.0)\n",
    "    ax1.set_xlabel('Feature 1', fontsize=20)\n",
    "    ax1.set_ylabel('Feature 2', fontsize=20)\n",
    "    ax1.set_xlim([-17, 17])\n",
    "    ax1.set_ylim([-17, 17])\n",
    "    ax1.xaxis.set_tick_params(labelsize=16)\n",
    "    ax1.yaxis.set_tick_params(labelsize=16)\n",
    "    \n",
    "    # Update difference\n",
    "    ax2.plot(np.arange(steps//2)+1, diffs, '-o')\n",
    "    ax2.set_xlim([0.8, 10.2])\n",
    "    ax2.set_ylim([-2, 25])\n",
    "    ax2.set_xlabel('Step', fontsize=20)\n",
    "    ax2.set_ylabel('Difference', fontsize=20)\n",
    "    ax2.xaxis.set_tick_params(labelsize=16)\n",
    "    ax2.yaxis.set_tick_params(labelsize=16)\n",
    "    \n",
    "    # Update loss\n",
    "    ax3.plot(np.arange(steps//2)+1, losses, '-o')\n",
    "    ax3.set_xlim([0.8, 10.2])\n",
    "    ax3.set_ylim([0, 15000])\n",
    "    ax3.set_xlabel('Step', fontsize=20)\n",
    "    ax3.set_ylabel('Sum of Squared Differences', fontsize=20)\n",
    "    ax3.xaxis.set_tick_params(labelsize=16)\n",
    "    ax3.yaxis.set_tick_params(labelsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch widget\n",
    "interact(kmeans_step_plot,\n",
    "         steps=IntSlider(value=0, min=0, max=21, step=1),\n",
    "         operations=fixed([0, 1] * 10 + [0]),\n",
    "         num_clusters=IntSlider(value=3, min=1, max=4, step=1),\n",
    "         random_seed=IntSlider(value=0, min=1, max=25, step=1),\n",
    "         data=fixed(data[['Feature 1', 'Feature 2']].to_numpy()));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section3'></a>\n",
    "# Import Iris Dataset\n",
    "This data sets consists of 3 different types of irises’ (`Setosa`, `Versicolour`, and `Virginica`) petal and sepal length, stored in a 150x4 numpy.ndarray.\n",
    "\n",
    "Let's import the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "iris = pd.DataFrame(data=iris['data'], columns=iris['feature_names'])\n",
    "iris.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, Let's plot the `'Petal Length'` and `'Petal Width'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "fig.subplots_adjust(wspace=0.15)\n",
    "sns.scatterplot(data=iris, x='petal length (cm)', y='petal width (cm)')\n",
    "ax.set_xlabel('Petal Length (cm)', fontsize=16)\n",
    "ax.set_ylabel('Petal Width (cm)', fontsize=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section4'></a>\n",
    "# k-means in Scikit-Learn\n",
    "Let's work with the `Iris` dataset.\n",
    "\n",
    "First, we'll need to scale our input features. We've done this previously when training linear and logistic regression models. We'll explain in the next section why this is important specifically for `k-means`. You'll notice that there are four features in `iris`. We'll just work with two features `'Petal Length'` and `'Petal Width'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "iris_scaled = scaler.fit_transform(iris[['petal length (cm)', \n",
    "                                         'petal width (cm)']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's fit the modeling using Scikit-Learn's `KMeans`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=2)\n",
    "kmeans.fit(iris_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's look at some of the attributes/outputs of the model.\n",
    "\n",
    "`.inertia_`: Sum of squared distances of samples to their closest cluster center."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.inertia_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`cluster_centers_`: Coordinates of cluster centers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`labels_`: Labels of each point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`n_iter_`: Number of iterations run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.n_iter_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, let's plot the data and the cluster labels assigned by `KMeans`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "fig.subplots_adjust(wspace=0.15)\n",
    "sns.scatterplot(data=iris, x='petal length (cm)', y='petal width (cm)', hue=kmeans.labels_)\n",
    "ax.legend(loc=2, fontsize=14)\n",
    "ax.set_xlabel('Petal Length (cm)', fontsize=16)\n",
    "ax.set_ylabel('Petal Width (cm)', fontsize=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section5'></a>\n",
    "# Feature Scaling\n",
    "As you notices in the previous section, we used the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy data\n",
    "x = np.random.randn(1000) * 100  \n",
    "y = np.concatenate([np.random.randn(500), np.random.randn(500) + 5])\n",
    "\n",
    "# Plot data\n",
    "fig = plt.figure(figsize=(6, 10))\n",
    "fig.subplots_adjust(hspace=0.4)\n",
    "ax1 = plt.subplot2grid((3, 1), (0, 0))\n",
    "ax2 = plt.subplot2grid((3, 1), (1, 0))\n",
    "ax3 = plt.subplot2grid((3, 1), (2, 0))\n",
    "\n",
    "ax1.set_title('Dataset', fontsize=16)\n",
    "sns.scatterplot(x=x, y=y, ax=ax1)\n",
    "ax1.set_xlabel('x')\n",
    "ax1.set_ylabel('y')\n",
    "\n",
    "clusters = KMeans(2).fit_predict(np.array([x, y]).T)\n",
    "ax2.set_title('Non-normalised k-means', fontsize=16)\n",
    "sns.scatterplot(x=x, y=y, hue=clusters, ax=ax2)\n",
    "ax2.set_xlabel('x')\n",
    "ax2.set_ylabel('y')\n",
    "\n",
    "clusters = KMeans(2).fit_predict(np.array([x / 100, y]).T)\n",
    "ax3.set_title('Normalised k-means', fontsize=16)\n",
    "sns.scatterplot(x=x, y=y, hue=clusters, ax=ax3)\n",
    "ax3.set_xlabel('x')\n",
    "ax3.set_ylabel('y');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section6'></a>\n",
    "# Minimizing Inertia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples=200\n",
    "blobs1 = 4\n",
    "blobs_std1 = 1.5\n",
    "clusters1 = 6\n",
    "blobs2 = 4\n",
    "blobs_std2 = 5\n",
    "clusters2 = 2\n",
    "colors = ['#008fd5', '#fc4f30', '#e5ae38', '#6d904f', '#8b8b8b', '#810f7c']\n",
    "runs = 200\n",
    "\n",
    "# Setup figure\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "fig.subplots_adjust(wspace=0.22, hspace=0.3)\n",
    "ax1 = plt.subplot2grid((3, 2), (0, 0))\n",
    "ax2 = plt.subplot2grid((3, 2), (1, 0), rowspan=2)\n",
    "ax3 = plt.subplot2grid((3, 2), (0, 1))\n",
    "ax4 = plt.subplot2grid((3, 2), (1, 1), rowspan=2)\n",
    "\n",
    "# Model 1\n",
    "data1 = create_dataset(centers=blobs1, cluster_std=blobs_std1, n_samples=n_samples)\n",
    "inertias1 = []\n",
    "ax1.set_title('Clusters: {}\\nBlobs: {}'.format(clusters1, blobs1), fontsize=14, loc='right')\n",
    "sns.scatterplot(data=data1, x='Feature 1', y='Feature 2', color='#8b8b8b', ax=ax2)\n",
    "for _ in range(runs):\n",
    "    model1 = KMeans(n_clusters=clusters1).fit(data1)\n",
    "    inertias1.append(model1.inertia_)\n",
    "    draw_centers(clusters1, ax2, model1.cluster_centers_, factor=1, alpha=1.0)\n",
    "ax2.set_xlabel('Feature 1', fontsize=14)\n",
    "ax2.set_ylabel('Feature 2', fontsize=14)\n",
    "sns.distplot(inertias1, ax=ax1, kde=False)\n",
    "ax1.set_xlabel('Sum of Squared Differences', fontsize=14)\n",
    "ax1.set_ylabel('Probability Density', fontsize=14)\n",
    "\n",
    "# Model 2\n",
    "data2 = create_dataset(centers=blobs2, cluster_std=blobs_std2, n_samples=n_samples)\n",
    "inertias2 = []\n",
    "ax3.set_title('Clusters: {}\\nBlobs: {}'.format(clusters2, blobs2), fontsize=14, loc='right')\n",
    "sns.scatterplot(data=data2, x='Feature 1', y='Feature 2', color='#8b8b8b', ax=ax4)\n",
    "for _ in range(runs):\n",
    "    model2 = KMeans(n_clusters=clusters2).fit(data2)\n",
    "    inertias2.append(model2.inertia_)\n",
    "    draw_centers(clusters2, ax4, model2.cluster_centers_, factor=1, alpha=1.0)\n",
    "ax4.set_xlabel('Feature 1', fontsize=14)\n",
    "ax4.set_ylabel('Feature 2', fontsize=14)\n",
    "sns.distplot(inertias2, ax=ax3, kde=False)\n",
    "ax3.set_xlabel('Sum of Squared Differences', fontsize=14)\n",
    "ax3.set_ylabel('Probability Density', fontsize=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luckily, the good folks at `Scikit-Learn` have thought about this. The `KMeans` object has a parameter called `n_init`, which is the number of time the `k-means` algorithm will be run with different centroid seeds. The final results will be the best output of `n_init` consecutive runs in terms of inertia. The default is 10, but as we saw above, you may want to increase this to ensure you get the optimal output.\n",
    "\n",
    "Here we are running 100 `k-means` cluster runs with different random initializations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=2,\n",
    "                n_init=100)\n",
    "kmeans.fit(iris_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section7'></a>\n",
    "# Hierarchical Clustering in Scikit-Learn\n",
    "Unlike k-means, hierarchical clustering doesn’t require the user to specify the number of clusters beforehand. Instead it returns an output (typically as a dendrogram), from which the user can decide the appropriate number of clusters (either manually or algorithmically).\n",
    "\n",
    "Let's work with the `Iris` dataset.\n",
    "\n",
    "First, we'll need to scale our input features. We've done this previously when training linear and logistic regression models. We'll explain in the next section why this is important specifically for `k-means`. You'll notice that there are four features in `iris`. We'll just work with two features `'Petal Length'` and `'Petal Width'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "iris_scaled = scaler.fit_transform(iris[['petal length (cm)', \n",
    "                                         'petal width (cm)']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's fit the modeling using Scikit-Learn's `AgglomerativeClustering`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agglomerative = AgglomerativeClustering(n_clusters=2, \n",
    "                                        affinity='euclidean', \n",
    "                                        linkage='average')\n",
    "agglomerative.fit(iris_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's look at some of the attributes/outputs of the model.\n",
    "\n",
    "`labels_`: Labels of each point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agglomerative.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`n_iter_`: Number of iterations run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, let's plot the data and the cluster labels assigned by `AgglomerativeClustering`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "fig.subplots_adjust(wspace=0.15)\n",
    "sns.scatterplot(data=iris, x='petal length (cm)', y='petal width (cm)', hue=agglomerative.labels_)\n",
    "ax.legend(loc=2, fontsize=14)\n",
    "ax.set_xlabel('Petal Length (cm)', fontsize=16)\n",
    "ax.set_ylabel('Petal Width (cm)', fontsize=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section8'></a>\n",
    "# Picking the Number of Clusters\n",
    "## Elbow Method\n",
    "Let's work with the Iris data again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "iris_scaled = scaler.fit_transform(iris[['petal length (cm)', \n",
    "                                         'petal width (cm)']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's loop through clusters from 1 to 10 and run k-means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "for clusters in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=clusters, n_init=100)\n",
    "    kmeans.fit(iris_scaled)\n",
    "    models.append(kmeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(1, 11), [model.inertia_ for model in models], 'o-')\n",
    "plt.xlabel('Number of Clusters', fontsize=16)\n",
    "plt.ylabel('Inertia', fontsize=16);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
