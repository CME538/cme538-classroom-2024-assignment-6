{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CME538 - Introduction to Data Science\n",
    "## Lecture 2.3 - Pandas III\n",
    "### New Concepts\n",
    "* lambda functions.\n",
    "* Different ways to iterate through a DataFrame and performance consideration.\n",
    "* different methods for combining multiple DataFrames (merge, append, concatenate, join).\n",
    "* Working with time series data in Pandas\n",
    "\n",
    "### Lecture Structure\n",
    "1. [lamba functions](#section1)\n",
    "2. [Iterating over DataFrames](#section2)\n",
    "3. [Combining DataFrames](#section3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Notebook\n",
    "At the start of a notebook, we need to import the Python packages we plan to use.\n",
    "* [os](https://docs.python.org/3/library/os.html) - Built in miscellaneous operating system interfaces.\n",
    "* [Time](https://docs.python.org/3/library/time.html) - This module provides various time-related functions. \n",
    "* [NumPy](https://numpy.org/) - A library for the Python programming language, adding support for large, multi-dimensional arrays and matrices, along with a large collection of high-level mathematical functions to operate on these arrays. NumPy was introcuded in Lecture 3 and we will learn more about its functionality in this lecture. It is customary to `import numpy as np`.\n",
    "* [Pandas](https://pandas.pydata.org/) - pandas is a fast, powerful, flexible and easy to use open source data analysis and manipulation tool, built on top of the Python programming language. Lecture 4, 5, and 6 will do a deep dive into the core functionality of Pandas. It is customary to `import pandas as pd`. \n",
    "* [Seaborn](https://seaborn.pydata.org/) - Seaborn is a Python data visualization library based on matplotlib. It provides a high-level interface for drawing attractive and informative statistical graphics. We will use Seaborn throughout CIV1498 for data visualization. It is customary to `import seaborn as pd`.  \n",
    "* [Maplotlib](https://matplotlib.org//) - Matplotlib is a comprehensive library for creating static, animated, and interactive visualizations in Python. We will use Matplotlib throughout CIV1498 for data visualization. It is customary to `import matplotlib.pyplot as plt`. \n",
    "\n",
    "Next, we want to configure the Jupyter Notebook.\n",
    "* `%matplotlib inline` - This code configured the notebook to display all plots, from Seaborn or Matplotlib, in the Notebook as opposed to in a separate pop-up window.\n",
    "* `plt.style.use('fivethirtyeight')` - This code configured the plots with the \"fivethirtyeight\" styling, which tries to replicate the styles from the website [FiveThirtyEight](https://fivethirtyeight.com/).\n",
    "* `sns.set_context(\"notebook\")` - This sets the plotting context parameters to be optimized for a Notebook. This affects things like the size of the labels, lines, and other elements of the plot, but not the overall style."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import 3rd party libraries\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Configure Notebook\n",
    "%matplotlib inline\n",
    "plt.style.use('fivethirtyeight')\n",
    "sns.set_context(\"notebook\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install descartes package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install descartes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section1'></a>\n",
    "## 1. Lambda Functions\n",
    "As we learned in Week 1 - Lecture 3, you can write your very own Python functions using the `def` keyword. \n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def raise_to_power(number, power):\n",
    "    return number ** power\n",
    "\n",
    "# Raise the number 2 to the power of 5\n",
    "raise_to_power(number=2, power=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, for simpler function definitions (`raise_to_power`) they can be converted to a `lambda` function. A `lambda` function is a small anonymous function that can take any number of arguments, but can only have one expression. The benefits of using `lambda` functions are (1) you will write fewer lines of code and (2) you can create functions on the fly without assigning them a name.\n",
    "\n",
    "See the cell below where we've convert the `raise_to_power()` function to a `lambda` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise_to_power = lambda number, power: number ** power\n",
    "\n",
    "raise_to_power(number=2, power=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The structure of a `lambda` function is:\n",
    "\n",
    "```python\n",
    "lambda arguments : expression\n",
    "```\n",
    "\n",
    "For example,\n",
    "\n",
    "```python\n",
    "lambda argument1, argument2, (argument1 + argument2) / 2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll see later how lambda functions can be used with the **Pandas** `.apply()` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section2'></a>\n",
    "## 2. Iterating over DataFrames\n",
    "There are multiple ways to iterate through DataFrames and when those DataFrame become large and the desired computation become complex, these different methods can have major impacts on compute times. In some cases, it could mean the different bewteen seconds, tens of minutes and even hours.\n",
    "\n",
    "Let's start by creating a dataset of random latitude and longitude values.\n",
    "* Latitude : max/min +90 to -90\n",
    "* Longitude : max/min +180 to -180"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_locations = 100000\n",
    "locations = pd.DataFrame({'lat': np.random.uniform(-90, 90, num_locations),\n",
    "                          'lon': np.random.uniform(-180, 180, num_locations),\n",
    "                          'distance': np.zeros(num_locations)})\n",
    "locations.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to calculate the straight-line distance between two points on the earth's surface (latitude and Longitude).  \n",
    "\n",
    "We will calculate the distance between Toronto **(lat: 43.651070 lon: -79.347015)** and every point in `locations`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toronto_lat = 43.651070\n",
    "toronto_lon = -79.347015"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create geopgrpahic plot of Toronto using **GeoPanfas** (We'll get into GeoPandas later on in the course)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create geopgrpahic plot of Toronto using GeoPanfas\n",
    "df = pd.DataFrame({'City': ['Toronto'], 'Country': ['Canada'], \n",
    "                    'Latitude': [toronto_lat], 'Longitude': [toronto_lon]})\n",
    "\n",
    "gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.Longitude, df.Latitude))\n",
    "world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
    "\n",
    "# We restrict to South America.\n",
    "ax = world[world.continent == 'North America'].plot(color='white', edgecolor='black')\n",
    "\n",
    "# We can now plot our ``GeoDataFrame``.\n",
    "gdf.plot(ax=ax, color='red')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ll use the Haversine (or Great Circle) distance formula, which takes the latitude and longitude of two points, adjusts for Earth’s curvature, and calculates the straight-line distance between them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"Defines a basic Haversine distance formula.\"\"\"\n",
    "    MILES = 3959\n",
    "    lat1, lon1, lat2, lon2 = map(np.deg2rad, [lat1, lon1, lat2, lon2])\n",
    "    dlat = lat2 - lat1 \n",
    "    dlon = lon2 - lon1 \n",
    "    a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
    "    c = 2 * np.arcsin(np.sqrt(a)) \n",
    "    total_miles = MILES * c\n",
    "    return total_miles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple `for` loop over `range()`\n",
    "Just about every Pandas beginner I’ve ever worked with (Including myself) has, at some point, attempted to apply a custom function by looping over DataFrame rows one at a time. The advantage of this approach is that it is consistent with the way one would interact with other iterable Python objects, however, crude looping in Pandas does not take advantage of any built-in optimizations, making it extremely inefficient by comparison.\n",
    "\n",
    "If you're only looping over a few rows, then perhaps this approach will suffice, however, its a good idea to understand the limitations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_for_loop_method(locations):\n",
    "\n",
    "    distance_list = []\n",
    "\n",
    "    # Loop through rows in locations DataFrame\n",
    "    for row_index in range(locations.shape[0]):\n",
    "\n",
    "        # Get lat and lon and row_index\n",
    "        lat = locations.loc[row_index, 'lat']\n",
    "        lon = locations.loc[row_index, 'lon']\n",
    "\n",
    "        # Compute Haversine distance\n",
    "        distance = haversine(lat1=toronto_lat, \n",
    "                             lon1=toronto_lon, \n",
    "                             lat2=lat, \n",
    "                             lon2=lon)\n",
    "\n",
    "        # Collect distance\n",
    "        distance_list.append(distance)\n",
    "\n",
    "    # Add distance values\n",
    "    locations['distance'] = distance_list\n",
    "    \n",
    "    return locations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use some Jupyter `%magic` to time the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit simple_for_loop_method(locations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a simple for loop and the `range()` function, it took **3.6 seconds** to iterate through **100,000 rows**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = simple_for_loop_method(locations)\n",
    "locations.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple for loop using `.iterrows()`\n",
    "`.iterrows()` is a generator that iterates over the rows of the dataframe and returns the index of each row, in addition to an object containing the row itself. `.iterrows()` is optimized to work with Pandas dataframes, however, it’s often the least efficient way to run most standard functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterrows_method(locations):\n",
    "    \n",
    "    distance_list = []\n",
    "    # Loop through rows in locations DataFrame\n",
    "    for index, row in locations.iterrows():\n",
    "\n",
    "        # Get lat and lon and row_index\n",
    "        lat = row['lat']\n",
    "        lon = row['lon']\n",
    "\n",
    "        # Compute Haversine distance\n",
    "        distance = haversine(lat1=toronto_lat, \n",
    "                             lon1=toronto_lon, \n",
    "                             lat2=lat, \n",
    "                             lon2=lon)\n",
    "\n",
    "        # Collect distance\n",
    "        distance_list.append(distance)\n",
    "\n",
    "    # Add distance values\n",
    "    locations['distance'] = distance_list\n",
    "\n",
    "    return locations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use some Jupyter `%magic` to time the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit iterrows_method(locations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the Pandas `.iterrows()` function, it took **9.45 seconds** to iterate through **100,000 rows**, which is over twice as long as the simpler method.\n",
    "\n",
    "### Simple for loop using `.to_dict()`\n",
    "The Pandas `.to_dict()` method converts a DataFrame to a dictionary. Below, we specify `orient='row'`, which returns a list of dictionaries where each dictionary corresponds to a row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_dict_for_loop_method(locations):\n",
    "\n",
    "    distance_list = []\n",
    "    # Loop through rows in locations DataFrame\n",
    "    for row in locations.to_dict(orient='row'):\n",
    "\n",
    "        # Get lat and lon and row_index\n",
    "        lat = row['lat']\n",
    "        lon = row['lon']\n",
    "\n",
    "        # Compute Haversine distance\n",
    "        distance = haversine(lat1=toronto_lat, \n",
    "                             lon1=toronto_lon, \n",
    "                             lat2=lat, \n",
    "                             lon2=lon)\n",
    "\n",
    "        # Collect distance\n",
    "        distance_list.append(distance)\n",
    "\n",
    "    # Add distance values\n",
    "    locations['distance'] = distance_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use some Jupyter `%magic` to time the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit to_dict_for_loop_method(locations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the Pandas `.to_dict()` function, it took **2.32 seconds** to iterate through **100,000 rows**, which is almost five times faster than `.iterrows()`.\n",
    "\n",
    "### Using Pandas `.apply()`\n",
    "The `.apply()` method, which applies a function along a specific axis (meaning, either rows or columns) of a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_method(locations):\n",
    "\n",
    "    locations['distance'] = locations.apply(\n",
    "        lambda row: haversine(lat1=toronto_lat, \n",
    "                              lon1=toronto_lon, \n",
    "                              lat2=row['lat'], \n",
    "                              lon2=row['lon']), \n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    return locations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use some Jupyter `%magic` to time the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit apply_method(locations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the Pandas .apply() function takes roughly the same amount of time as the simple loop but the code is more compact.\n",
    "\n",
    "### Vectorization over Pandas `Series`\n",
    "Vectorization is the process of executing operations on entire arrays rather than by iterating over individual units. Recall that the fundamental units of Pandas, DataFrames and Series, are both based on NumPy arrays. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorized_series_method(locations):\n",
    "\n",
    "    locations['distance'] = haversine(lat1=toronto_lat, \n",
    "                                      lon1=toronto_lon, \n",
    "                                      lat2=locations['lat'], \n",
    "                                      lon2=locations['lon'])\n",
    "    \n",
    "    return locations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use some Jupyter `%magic` to time the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit vectorized_series_method(locations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By vectorizing over Pandas Series, we see a **x165** improvement over the `.to_dict()` method.\n",
    "\n",
    "### Vectorization over NumPy `Array`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorized_array_method(locations):\n",
    "\n",
    "    locations['distance'] = haversine(lat1=toronto_lat, \n",
    "                                      lon1=toronto_lon, \n",
    "                                      lat2=locations['lat'].to_numpy(), \n",
    "                                      lon2=locations['lon'].to_numpy())\n",
    "    \n",
    "    return locations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use some Jupyter `%magic` to time the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit vectorized_array_method(locations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By vectorizing over NumPy Arrays, we see a **x252** improvement over the `.to_dict()` method.\n",
    "\n",
    "**Note:** These times are for my compute and only a single exection. These time will vary from machine to machine and slightly from iteration to iteration.\n",
    "\n",
    "From this quick exercise, you should know that there are many different ways to iterate through a DataFrame and that the different methods have very different performance considerations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section3'></a>\n",
    "## 3. Combining DataFrames\n",
    "\n",
    "When conducting exploratory data analysis (EDA), its common that the data we want to use comes in multiple files and will need to be combined. Let's look at an example of this.\n",
    "\n",
    "In the **Lecture 6** folder, there are six `.csv` files from **Uber** showing monthly ridership numbers from April 2014 to September 2014.\n",
    "\n",
    "Let's take a look at these files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given what we've learned aleady in Lectures 4 and 5, we know how to import these `.csv` files to **Pandas** DataFrames. Lets try that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "april_data = pd.read_csv('uber-raw-data-apr14.csv')\n",
    "may_data = pd.read_csv('uber-raw-data-may14.csv')\n",
    "june_data = pd.read_csv('uber-raw-data-jun14.csv')\n",
    "july_data = pd.read_csv('uber-raw-data-jul14.csv')\n",
    "aug_data = pd.read_csv('uber-raw-data-aug14.csv')\n",
    "sept_data = pd.read_csv('uber-raw-data-sep14.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what the April data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "april_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this DataFrame, each row is an Uber trip\n",
    "\n",
    "Suppose we're asked to plot the number of trips per hour from April 2014 to September 2014. To tackle this problem, it would be much easier if all the data was in one DataFrame.\n",
    "\n",
    "In the following section, you'll be introduces to two Pandas methods for combining DataFrames: `.concatenate()` and `.merge()`. The figure below is helpful for figuring out which method to use.\n",
    "\n",
    "<br>\n",
    "<img src=\"images/merging_dataframes.png\" alt=\"drawing\" width=\"750\"/>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate\n",
    "\n",
    "We use the `.concat()` function to append either columns or rows from one DataFrame to another. This happens to be the functionality we need to handle the Uber data we import above.\n",
    "\n",
    "[`pd.concat()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.concat.html) has many features, which you're encouraged to explore, but the basic function is demonstrated below.\n",
    "\n",
    "If we look at the flow diagram above, if we want to stack multiple DataFrames side-by-side, then we set `axis=1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack the DataFrames on top of each other\n",
    "uber_data = pd.concat([april_data, \n",
    "                       may_data, \n",
    "                       june_data, \n",
    "                       july_data, \n",
    "                       aug_data, \n",
    "                       sept_data], axis=1)\n",
    "\n",
    "# View combined DataFrame\n",
    "uber_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this is not what we want to do with the Uber data. We'd like to stack the data from each month, one on top of each other from April to September. To accomplish this, we need to set `axis=0`. Note that the order of the months in the DataFrame (top to bottom) follows the order of months in the `.concat()` method, left to right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack the DataFrames on top of each other\n",
    "uber_data = pd.concat([april_data, \n",
    "                       may_data, \n",
    "                       june_data, \n",
    "                       july_data, \n",
    "                       aug_data, \n",
    "                       sept_data], axis=0)\n",
    "\n",
    "# View combined DataFrame\n",
    "uber_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see April data dat the top of the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and September data at the bottom.\n",
    "\n",
    "Next, let's plot the index of our new DataFrame `uber_data` and inspect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(uber_data.index)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see from the plot that when concatenating the DataFrames, the original indexes have been preserved, meaning that we have duplicates, which will be an issue moving forward. \n",
    "\n",
    "To adjust the row index automatically, we have to set the argument `ignore_index` as `True` while calling the `.concat()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_data = pd.concat([april_data, \n",
    "                       may_data, \n",
    "                       june_data, \n",
    "                       july_data, \n",
    "                       aug_data, \n",
    "                       sept_data], \n",
    "                      axis=0,\n",
    "                      ignore_index=True)\n",
    "\n",
    "plt.plot(uber_data.index)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now each row has a unique index!\n",
    "\n",
    "The `.concat()` function has many more features that you should definitely [check out](https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html).\n",
    "\n",
    "\n",
    "### Merge\n",
    "`.merge()` is another **Pandas** function for combining DataFrames. Generally speaking, the difference between `.merge()` and `.concat()` is that `.merge()` is used to combine two (or more) DataFrames on the basis of values of common columns while `.concat()` is used to append one (or more) DataFrames one below the other or one next to the other.\n",
    "\n",
    "<br>\n",
    "<img src=\"images/joins.jpg\" alt=\"drawing\" width=\"550\"/>\n",
    "<br>\n",
    "\n",
    "`.merge()` allows you to execute the following join operations: inner join, full outer join, left outer join, right outer join (see Figure above).\n",
    "\n",
    "Before learning more about `.merge()`, let's create some test data. First, let's create a DataFrame with the names of participants in a test and their participant id number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({'participant_id': ['1', '6', '33', '42', '65', '8', '20', '13', '14'],\n",
    "                    'first_name': ['Shoshanna', 'Marianne', 'Karl', 'Brent', 'John', 'Marcus', \n",
    "                                   'Bruce', 'Judi', 'Denzel'], \n",
    "                    'last_name': ['Saxe', 'Touchie', 'Peterson', 'Sleep', 'Harrison', 'Aurelius', \n",
    "                                  'Wayne', 'Dench', 'Washington']})\n",
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's create another DataFrame showing the participant's test scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame({'participant_id': ['22', '98', '71', '33', '42', '65', '8', '20', '13', \n",
    "                                       '14', '34', '54'],\n",
    "                    'score': [80, 76, 72, 66, 77, 64, 59, 60, 62, 89, 67, 58]})\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, use the `.merge()` method to work through the four paths outlined in the figure below. In this case, we're using `.merge()` because the contents of the DataFrame are required for combining the DataFrames. The common column **participant_id** will be used to merge **df1** and **df2**.\n",
    "<br>\n",
    "<img src=\"images/merging_dataframes.png\" alt=\"drawing\" width=\"750\"/>\n",
    "<br>\n",
    "#### Path 1: \n",
    "* I want to keep the full content of both DataFrames. \n",
    "* Solution: Outer Join\n",
    "\n",
    "The merge operation takes the form of `left_df.merge(right=right_df)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_outer_join = df1.merge(right=df2, \n",
    "                          how='outer', \n",
    "                          on='participant_id')\n",
    "df_outer_join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Path 2: \n",
    "* I want to keep the full content of the left DataFrame and merge any matching data from the right DataFrame. \n",
    "* Solution: Left Outer Join\n",
    "\n",
    "Practically, this means that we want a DataFrame with all of the participants from **df1** and we want to merge their scores from **df2**. We do not want any participants from **df2** that are not in **df1**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_left_outer_join = df1.merge(right=df2, \n",
    "                               how='left', \n",
    "                               on='participant_id')\n",
    "df_left_outer_join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Path 3: \n",
    "* I want to keep the full content of the right DataFrame and merge any matching data from the left DataFrame. \n",
    "* Solution: Right Outer Join\n",
    "\n",
    "Practically, this means that we want a DataFrame with all of the scores from **df2** and we want to merge their names from **df1**. We do not want any participants from **df1** that are not in **df2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_right_outer_join = df1.merge(right=df2, \n",
    "                                how='right', \n",
    "                                on='participant_id')\n",
    "df_right_outer_join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Path 4: \n",
    "* I want to keep the only contents of the right and left DataFrame only where overlap. \n",
    "* Solution: Inner Join\n",
    "\n",
    "Practically, this means that we want a DataFrame with all of the scores from **df2** and names from **df1** where they have **participant_id** in common."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inner_join = df1.merge(right=df2, \n",
    "                          how='inner', \n",
    "                          on='participant_id')\n",
    "df_inner_join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out Quercus for more LinkedinLearning Boosters and Pandas documentation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
