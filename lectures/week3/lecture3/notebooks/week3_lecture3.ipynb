{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CME538 - Introduction to Data Science\n",
    "## Lecture 3.3 - Cleaning Data\n",
    "\n",
    "---\n",
    "\n",
    "### Analyzing Canada's Government Travel Expenses\n",
    "\n",
    "In today‚Äôs connected world, where Zoom calls and online collaboration platforms dominate, government officials still seem to have a love for travel. Why spend thousands on trips when technology allows us to have a meeting without ever leaving our desks? It turns out, travel is sometimes unavoidable‚Äîwhether for meeting key stakeholders, building international relations, or attending summits. However, with taxpayer money at stake, it's important to examine the costs.\n",
    "\n",
    "This notebook dives deep into the data of Canadian government travel expenses, based on the Proactive Disclosure - Travel Expenses dataset (see [here](https://open.canada.ca/data/en/dataset/009f9a49-c2d9-4d29-a6d4-1a228da335ce/resource/8282db2a-878f-475c-af10-ad56aa8fa72c)).\n",
    "\n",
    "**Fun Fact:** Did you know that the CEO of a government agency once spent over $14,000 attending a conference in Australia? We‚Äôll be exploring whether this is just the tip of the iceberg.\n",
    "\n",
    "---\n",
    "\n",
    "### Lecture 3.3 Overview\n",
    "\n",
    "As part of CME538's focus on data science, Lecture 3.3 covered a range of important data cleaning techniques. Here's a summary of what was discussed in the lecture and what you'll see applied in this notebook:\n",
    "\n",
    "- **Type Conversion**: Converting data types to ensure they match the values they represent.\n",
    "- **Duplicates**: Removing any repeated entries.\n",
    "- **Missing Data**: Handling gaps in the dataset through removal or imputation.\n",
    "- **Implausible Data**: Flagging values that seem incorrect or out of bounds.\n",
    "- **Irrelevant Data**: Removing unnecessary columns to streamline our dataset.\n",
    "- **Character Encoding**: Fixing encoding issues.\n",
    "- **Datetime Parsing**: Ensuring dates are formatted consistently.\n",
    "- **Outliers**: Identifying and potentially removing extreme values.\n",
    "- **Inconsistent Data Entry**: Standardizing any inconsistent input formats (e.g., addresses).\n",
    "- **Unit Conversion**: Standardizing any discrepancies between units (like miles vs kilometers).\n",
    "\n",
    "This notebook will walk through these concepts step by step as we clean and analyze Canada's government travel expense data. Get ready for a mix of interesting insights, and maybe a surprise or two about how tax dollars are being spent!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Setup the Notebook\n",
    "\n",
    "Before diving into the data, it's important to set up our working environment. We'll be using Python‚Äôs powerful data analysis libraries, primarily `pandas` for data manipulation and `numpy` for numerical operations. Additionally, we‚Äôll configure `pandas` to display all columns and rows for better visibility, especially when analyzing data later.\n",
    "\n",
    "Let's get started by importing the necessary libraries and setting up display options for ease of analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install pandas numpy matplotlib seaborn\n",
    "\n",
    "# Step 0: Setup the Notebook\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Display options for better readability\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "# Other necessary imports\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load the Data\n",
    "\n",
    "In this step, we will load the dataset into our notebook. The dataset consists of travel expenses disclosed by the Canadian government. This data will help us understand government spending on travel and explore potential inefficiencies or unusual expenses.\n",
    "\n",
    "We will load the CSV file using `pandas` and display the first few rows to familiarize ourselves with its structure and content.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load the Data\n",
    "data_file_path = 'travelq.csv'\n",
    "data = pd.read_csv(data_file_path)\n",
    "\n",
    "# Display the first few rows of the dataset to get an overview\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's the Data Telling Us?\n",
    "The dataset contains several columns, including information about the purpose of travel, destination, expenses for airfare, lodging, meals, and other associated costs. This gives us a comprehensive view of travel-related expenses. \n",
    "\n",
    "Notice that some columns contain both English and French translations, which is common for Canadian datasets. We‚Äôll need to handle these appropriately when cleaning the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Data Exploration and Initial Checks\n",
    "\n",
    "In this step, we are going to explore the data and get a better understanding of it. We‚Äôll check for the basics like data types, missing values, and any possible anomalies. This is where we lay the groundwork for deeper analysis and cleaning.\n",
    "\n",
    "#### What Are We Looking For?\n",
    "\n",
    "- **Data Types:** Are the columns properly formatted (e.g., dates as `datetime`, numbers as `float` or `int`)?\n",
    "- **Missing Values:** Are there any gaps in the data that we need to handle?\n",
    "- **Duplicates:** Are there any duplicate rows that are unnecessarily inflating the data?\n",
    "\n",
    "Time to dive in and see what we‚Äôre dealing with!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data types and missing values\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More explicit check for missing values\n",
    "missing_values = data.isnull().sum()\n",
    "print('Missing values per column:')\n",
    "print(missing_values)\n",
    "\n",
    "# Check for duplicates\n",
    "duplicate_rows = data.duplicated().sum()\n",
    "print(f'Total duplicate rows: {duplicate_rows}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's Happening?\n",
    "\n",
    "- **Data Types:** `data.info()` gives a quick overview of the data types, but we might need to adjust some (e.g., converting `start_date` and `end_date` to proper `datetime` types).\n",
    "- **Missing Values:** The `isnull().sum()` function provides an explicit count of missing values for each column.\n",
    "- **Duplicates:** We can see the number of duplicate rows, which we‚Äôll deal with shortly if needed.\n",
    "\n",
    "Getting cleaner data is like prepping for a road trip ‚Äî you don‚Äôt want any bumps in the road!\n",
    "\n",
    "#### What's Missing?\n",
    "\n",
    "Here, we're checking how many missing values each column has. It's essential to get a sense of how severe the missing data issue is before deciding what to do. Let's take a look at the result above to see which columns are affected and how many entries are missing. \n",
    "\n",
    "Once we know that, we'll decide on the best way to handle it: do we drop the rows, fill in the blanks, or take another approach? Keep your cleaning tools ready!\n",
    "\n",
    "#### Discussion: ‚ÄúDid Someone Forget to Fill This In?‚Äù\n",
    "\n",
    "If the number of missing values is small, we might just drop the rows. If it‚Äôs a significant portion, we‚Äôll consider filling in the blanks using an appropriate method, such as filling with a median for numerical data or the most frequent value for categorical data. Let‚Äôs move ahead and clean up the missing values!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Dropping Rows with Missing Values (Bye-Bye, Gaps!)\n",
    "\n",
    "Now that we've identified the missing values, we're going to drop the rows with missing data. Since our dataset has synthetic issues added, it's a good opportunity to practice this important data cleaning step. Removing rows with missing values ensures we don't run into errors or misleading results during analysis or modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing values\n",
    "data_cleaned = data.dropna()\n",
    "\n",
    "# Check the shape of the cleaned dataset to see how much data was dropped\n",
    "data_cleaned.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Cleaned and Ready to Roll!\n",
    "\n",
    "We've successfully dropped the rows containing missing values. You can see above how much data we have left. This is a common cleaning step when missing values are scattered across the dataset.\n",
    "\n",
    "In the next step, we'll tackle another common data cleaning task: dealing with duplicates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Detecting and Removing Duplicates (Seeing Double?)\n",
    "\n",
    "Sometimes, datasets contain duplicate rows, which can occur due to errors during data entry or when datasets are merged. Duplicate entries can skew results and lead to incorrect conclusions. In this step, we‚Äôll identify and remove any duplicate rows to ensure our dataset is unique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect duplicates\n",
    "duplicates_count = data_cleaned.duplicated().sum()\n",
    "\n",
    "print(f\"Number of duplicate rows: {duplicates_count}\")\n",
    "\n",
    "# Remove duplicate rows\n",
    "data_no_duplicates = data_cleaned.drop_duplicates()\n",
    "\n",
    "# Check the shape of the dataset after removing duplicates\n",
    "data_no_duplicates.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Duplicate Data, Gone!\n",
    "\n",
    "We've identified and removed any duplicate rows in the dataset. By doing this, we ensure that our analysis won't be affected by repeated entries that could skew our results.\n",
    "\n",
    "In the next step, we'll deal with implausible data ‚Äî things like negative expenses or unrealistic numbers. Time to do a reality check!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Dealing with Implausible Data (Reality Check Time!)\n",
    "\n",
    "In some cases, datasets can contain implausible or impossible values, such as negative amounts for travel expenses. These errors can seriously mess up your analysis if not corrected. Here, we'll identify any such cases and fix them by replacing implausible values with `NaN`, so we can decide later how to handle them.\n",
    "\n",
    "Let's start by checking for negative values in key numeric columns like airfare, lodging, and meals. We'll then replace them with `NaN` for further action.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect implausible data (negative values)\n",
    "implausible_columns = ['airfare', 'lodging', 'meals', 'other_expenses']\n",
    "implausible_data = data_no_duplicates[implausible_columns] < 0\n",
    "\n",
    "# Sum the number of implausible entries in each column\n",
    "implausible_data_sum = implausible_data.sum()\n",
    "print(f\"Number of implausible entries:\\n{implausible_data_sum}\")\n",
    "\n",
    "# Replace implausible values with NaN using .loc and .map for each column\n",
    "for column in implausible_columns:\n",
    "    data_no_duplicates.loc[data_no_duplicates[column] < 0, column] = np.nan\n",
    "\n",
    "# Verify that the implausible values are handled\n",
    "data_no_duplicates[implausible_columns].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Cleaning Inconsistent Data Entries (Name Game Fix!)\n",
    "\n",
    "Sometimes, inconsistent data entries, such as variations in names or addresses, can creep into our dataset. This could be something like \"Toronto\" vs \"Toronto, Ontario\" or \"Vancouver\" vs \"Vancouver, BC.\" We can fix such inconsistencies by using the `fuzzywuzzy` library, which helps match similar strings based on token similarity.\n",
    "\n",
    "Let's apply this to the `destination_en` column of our dataset and clean up inconsistent destination names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fuzzywuzzy\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "# Example of common inconsistent names for demonstration. You could add more here.\n",
    "common_names = ['Toronto', 'Toronto, Ontario', 'Vancouver', 'Vancouver, BC', 'Montreal', 'Montr√©al']\n",
    "\n",
    "# Create a function to replace inconsistent names\n",
    "def replace_inconsistent_names(column, common_names):\n",
    "    corrected_names = []\n",
    "    \n",
    "    for name in column:\n",
    "        # Find the closest match for the current name\n",
    "        match = process.extractOne(name, common_names, scorer=fuzzywuzzy.fuzz.token_sort_ratio)\n",
    "        # If the match score is high enough, replace with the most common name\n",
    "        if match and match[1] > 80:  # Threshold set to 80 for better accuracy\n",
    "            corrected_names.append(match[0])\n",
    "        else:\n",
    "            corrected_names.append(name)  # Keep the name as it is if no good match is found\n",
    "    \n",
    "    return corrected_names\n",
    "\n",
    "# Apply the function to the 'destination_en' column using .loc to avoid the warning\n",
    "data_no_duplicates.loc[:, 'destination_en'] = replace_inconsistent_names(data_no_duplicates['destination_en'], common_names)\n",
    "\n",
    "# Check the cleaned column\n",
    "data_no_duplicates['destination_en'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Consistency is Key!\n",
    "\n",
    "We've cleaned up the inconsistent city names in the 'destination_en' column. Now, instead of multiple variations of the same location, we have consistent and standardized names. This makes analysis much easier and ensures that our insights are accurate.\n",
    "\n",
    "Coming up next, we'll handle outliers ‚Äî those data points that might be way outside the norm. Let's see how to identify and deal with them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Outlier Detection üßê\n",
    "\n",
    "Now, we're going to identify outliers using the **Interquartile Range (IQR)** method. Outliers are values that are significantly higher or lower than most of the data and can skew the analysis.\n",
    "\n",
    "We'll calculate the IQR for columns such as `airfare`, `lodging`, `meals`, and `other_expenses`. Outliers will be defined as any value outside 1.5 times the IQR.\n",
    "\n",
    "### Why?\n",
    "Detecting outliers can help us clean up extreme values that might distort our analysis or misrepresent spending patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the columns where we want to detect outliers\n",
    "numeric_columns = ['airfare', 'lodging', 'meals', 'other_expenses']\n",
    "\n",
    "# Function to detect and remove outliers using IQR\n",
    "def detect_and_remove_outliers(df, columns):\n",
    "    for col in columns:\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        # Replace outliers with NaN\n",
    "        df.loc[(df[col] < lower_bound) | (df[col] > upper_bound), col] = np.nan\n",
    "    return df\n",
    "\n",
    "# Apply the function to detect and remove outliers\n",
    "data_cleaned = detect_and_remove_outliers(data_no_duplicates, numeric_columns)\n",
    "\n",
    "# Check a summary of the cleaned data\n",
    "data_cleaned[numeric_columns].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What just happened? ü§î\n",
    "We detected and replaced the outliers with `NaN` for columns like airfare, lodging, meals, and other expenses. This helps ensure that extreme values don‚Äôt skew our results in later analysis.\n",
    "\n",
    "We‚Äôll next decide whether to **drop** or **impute** these outliers in Step 8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Handling Outliers üöÄ\n",
    "\n",
    "Now that we‚Äôve identified the outliers and replaced them with `NaN`, we need to decide how to handle them. The two main strategies are:\n",
    "\n",
    "- **Imputation**: Replace the missing values (outliers) with something like the median or mean of the column.\n",
    "- **Dropping**: Remove rows that contain these outliers.\n",
    "\n",
    "Since we want to maintain as much data as possible, we'll go with **imputation** using the median for each column. The median is a robust measure because it‚Äôs not affected by extreme values.\n",
    "\n",
    "### Why?\n",
    "Imputing with the median will help preserve the data while preventing outliers from distorting our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing (outlier) values with the median of each column\n",
    "def impute_outliers(df, columns):\n",
    "    for col in columns:\n",
    "        median_value = df[col].median()\n",
    "        df[col] = df[col].fillna(median_value)\n",
    "    return df\n",
    "\n",
    "# Apply imputation to outlier-affected columns\n",
    "data_imputed = impute_outliers(data_cleaned, numeric_columns)\n",
    "\n",
    "# Check if outliers are handled\n",
    "data_imputed[numeric_columns].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What happened here? ü§ì\n",
    "We replaced all `NaN` values (previously identified as outliers) with the median of their respective columns. This allows us to retain the rows without the influence of extreme values.\n",
    "\n",
    "Next, we'll move on to parsing and standardizing **datetime** values to ensure consistency. Let's move on! üéØ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Parsing Dates**.\n",
    "\n",
    "### Parsing Dates and Standardizing Formats\n",
    "\n",
    "We often encounter inconsistent date formats in real-world datasets. In this step, we'll ensure that all dates are consistently formatted by converting them to `datetime` objects. This allows for easier filtering, manipulation, and analysis later.\n",
    "\n",
    "---\n",
    "\n",
    "### **üîç Time Travel with Date Parsing!**\n",
    "We‚Äôre about to fix the dates. Imagine sorting travel plans for an organization that has meetings all across the world. Now, let‚Äôs make sure all the date columns are in the same format so we can analyze the data efficiently!\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Parse the 'start_date' and 'end_date' columns\n",
    "data_imputed['start_date'] = pd.to_datetime(data_imputed['start_date'], errors='coerce')\n",
    "data_imputed['end_date'] = pd.to_datetime(data_imputed['end_date'], errors='coerce')\n",
    "\n",
    "# Let's check if there are any issues with the conversion\n",
    "print(data_imputed[['start_date', 'end_date']].dtypes)\n",
    "print(data_imputed[['start_date', 'end_date']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **üï∞Ô∏è What Happened?**\n",
    "In this block:\n",
    "- We used `pd.to_datetime()` to convert the `start_date` and `end_date` columns into `datetime` objects.\n",
    "- The `errors='coerce'` argument ensures that any invalid dates are converted into `NaT` (Not a Time), so we can handle those later.\n",
    "- The result shows whether our conversion was successful and gives us a preview of the dates.\n",
    "\n",
    "---\n",
    "\n",
    "After parsing dates, we can proceed with exploring our travel durations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Character Encoding\n",
    "\n",
    "Sometimes datasets may contain special characters or non-UTF-8 encoding that can cause issues when reading or processing the data. In this step, we will:\n",
    "- Identify any encoding issues using Python‚Äôs `chardet` package.\n",
    "- Convert the data to UTF-8 format if necessary and handle errors during conversion.\n",
    "\n",
    "---\n",
    "\n",
    "### **üßô‚Äç‚ôÇÔ∏è Encoding Magic!**\n",
    "Our data may contain hidden encoding issues. Let's detect and fix them before they cause any headaches!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the chardet package if you haven't already\n",
    "# %pip install chardet\n",
    "\n",
    "import chardet\n",
    "\n",
    "# Function to detect encoding issues\n",
    "def detect_encoding(file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        result = chardet.detect(file.read())\n",
    "        return result\n",
    "\n",
    "# Detect encoding of the CSV file\n",
    "encoding_info = detect_encoding('travelq.csv')\n",
    "print(f\"Detected Encoding: {encoding_info}\")\n",
    "\n",
    "# Read the file using the detected encoding\n",
    "data_clean = pd.read_csv('travelq.csv', encoding=encoding_info['encoding'])\n",
    "\n",
    "# Convert to UTF-8 encoding and overwrite the file\n",
    "data_clean.to_csv('travelq_cleaned.csv', encoding='utf-8', index=False)\n",
    "\n",
    "# Verify the data is now in UTF-8\n",
    "print(\"Data successfully converted to UTF-8 encoding!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **üîß What Happened?**\n",
    "- We used the `chardet` package to detect the encoding of the file.\n",
    "- If any encoding issues were detected, we read the file using the appropriate encoding and converted it to UTF-8.\n",
    "- This ensures all characters, especially special characters, are correctly interpreted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12: Conclusion\n",
    "\n",
    "Now that we've successfully cleaned the data through multiple steps, it‚Äôs time to visualize the result and highlight why clean data is essential for any analysis. We‚Äôll create some cool charts to visualize the cleaned data and emphasize how clean data allows us to create meaningful visualizations and insights.\n",
    "\n",
    "---\n",
    "\n",
    "### **üéâ Clean Data, Cool Insights!**\n",
    "Data cleaning isn‚Äôt just about fixing errors‚Äîit unlocks the power to gain valuable insights from your data. Here are some cool charts that show how the Canadian government is spending on travel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Top 5 most expensive travel purposes\n",
    "top_expenses = data_imputed.groupby('purpose_en')['total'].sum().sort_values(ascending=False).head(5)\n",
    "\n",
    "# Plotting the top 5 most expensive purposes\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=top_expenses.values, y=top_expenses.index, hue=top_expenses.index, dodge=False, palette=\"viridis\")\n",
    "plt.title('Top 5 Most Expensive Travel Purposes')\n",
    "plt.xlabel('Total Cost ($)')\n",
    "plt.ylabel('Purpose')\n",
    "plt.legend([],[], frameon=False)  # Hide the legend \n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Distribution of total expenses\n",
    "filtered_expenses = data_imputed[data_imputed['total'] < 50000] # filtered to remove extreme values\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data_imputed['total'], bins=30, kde=True, color=\"blue\")\n",
    "plt.title('Distribution of Total Travel Expenses')\n",
    "plt.xlabel('Total Expense ($)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Travel expenses by destination\n",
    "top_destinations = data_imputed.groupby('destination_en')['total'].sum().nlargest(10).index\n",
    "filtered_data = data_imputed[data_imputed['destination_en'].isin(top_destinations)]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='destination_en', y='total', data=filtered_data)\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Distribution of Travel Expenses by Top 10 Destinations')\n",
    "plt.xlabel('Destination')\n",
    "plt.ylabel('Total Expense ($)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **üìä What We See:**\n",
    "- **Top 5 Travel Purposes:** A bar plot showing which purposes cost the most.\n",
    "- **Expense Distribution:** A histogram of total travel expenses, showing the spread of travel costs.\n",
    "- **Expenses by Destination:** A boxplot showing the variation of travel expenses by destination.\n",
    "\n",
    "---\n",
    "\n",
    "### **üìù Final Thoughts:**\n",
    "In this notebook, we‚Äôve gone through several crucial steps to clean and preprocess the data, including:\n",
    "- Handling missing values, duplicates, and implausible data.\n",
    "- Parsing dates and resolving inconsistencies.\n",
    "- Addressing encoding issues.\n",
    "\n",
    "Clean data allows us to generate powerful insights through visualizations and analysis. Without proper cleaning, the analysis could lead to incorrect conclusions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
