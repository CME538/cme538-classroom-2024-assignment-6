{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ece98d3",
   "metadata": {},
   "source": [
    "# CME538 Tutorial 12: Introduction to SQL\n",
    "\n",
    "Content by Katia\n",
    "\n",
    "# Overview of Notebook\n",
    "\n",
    "## Tutorial Structure:\n",
    "\n",
    "1. Introduction to Databases and SQL\n",
    "\n",
    "2. Writing SQL Queries using Magic SQL (`%%sql`)\n",
    "- `SELECT`\n",
    "- `WHERE`\n",
    "- `LIMIT`\n",
    "- `ORDER BY`\n",
    "- `OFFSET`\n",
    "- `JOIN`/`ON`\n",
    "- `HAVING`\n",
    "- Complex Queries\n",
    "\n",
    "3. SQL Queries to Python, Creating Databases, Python to SQL Connections\n",
    "- Write a SQL table to Pandas dataframe\n",
    "- Create a SQL Database\n",
    "    - Tables from Pandas\n",
    "    - Tables written in SQL\n",
    "\n",
    "## References\n",
    "\n",
    "- SQL Alchemy Documentation: https://docs.sqlalchemy.org/en/20/intro.html\n",
    "- Introduction to SQL, W3 Schools: https://www.w3schools.com/sql/\n",
    "- References https://database.guide/2-sample-databases-sqlite/\n",
    "\n",
    "*Prepared in November 2023*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8504b826",
   "metadata": {},
   "source": [
    "## What is a database?\n",
    "\n",
    "A database is an organized collection of structured information, or data, typically stored electronically in a computer system. A database is usually controlled by a database management system (DBMS). Together, the data and the DBMS, along with the applications that are associated with them, are referred to as a database system, often shortened to just database.\n",
    "\n",
    "Data within the most common types of databases in operation today is typically modeled in rows and columns in a series of tables to make processing and data querying efficient. The data can then be easily accessed, managed, modified, updated, controlled, and organized. Most databases use structured query language (SQL) for writing and querying data.\n",
    "\n",
    "Common database types include:\n",
    "- `SQLite`: database engine written in C programming language, self-contained (no extra server requirements), and used for smaller systems. Commonly used in Python SQL applications for this reason!\n",
    "- `MS Access`: Used for small or in-home business applications. Similar to SQLite.\n",
    "- `Postresql`: Open source relational database, used for many web, mobile, and geospatial applications.\n",
    "- `MySQL`: open-source relational database system, comes with a user management interface. Good for more advanced applications, with many users accessing the information.\n",
    "- `MS SQL`: Microsoft SQL Server. Integrates well with the Office Suite, and higher storage/compute compatability.\n",
    "- `Oracle DB`: Multi-model database management system, commonly employed when multiple environments for data storage and validation, with common data processing and refreshing.\n",
    "- `NoSQL`: database approach design that prioritizes retreiving data not in a tabular format (for example, a key-value system like dictionaries, but on a much larger scale)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa412ff",
   "metadata": {},
   "source": [
    "## What is SQL (Structured Query Language)?\n",
    "\n",
    "SQL is a programming language used by nearly all relational databases to query, manipulate, and define data, and to provide access control. SQL was first developed at IBM in the 1970s with Oracle as a major contributor, which led to implementation of the SQL ANSI standard, SQL has spurred many extensions from companies such as IBM, Oracle, and Microsoft. Although SQL is still widely used today, new programming languages are beginning to appear."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d07180",
   "metadata": {},
   "source": [
    "## What is a relational database? How is this different from what we studied so far?\n",
    "\n",
    "In contrast to individually saving data files, with SQL we are able to have multiple data sources saved in a computationally efficient way, preserve relationships between them and maintain data integrity.\n",
    "\n",
    "We will be working with the SQLite database file `chinook.db`. Data source: https://www.sqlitetutorial.net/sqlite-sample-database/\n",
    "\n",
    "This image shows all the tables in the database (name on top of each box), as well as the columns available (within the box) and keys (i.e. columns) where the joins happen between tables:\n",
    "![Database Tables](sqllite_tables.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f92e8d0",
   "metadata": {},
   "source": [
    "## SQL vs. Pandas?\n",
    "\n",
    "So far in this course, we have worked with individual data sources, but not with a database. SQL natively works better with distributed computing systems, and has fewer key-words than built in (compared to how many Pandas functions there are). SQL directly connects to local and remote database servers, and is meant with handling large volumes of information - SQL is good to batch this data, and performing aggregate functions in memory, in a much cheaper and cost-efficent way compared to Python/Pandas.\n",
    "\n",
    "Pandas on the other hand is very good for visualization and machine learning tasks.\n",
    "\n",
    "Almost all the commands that are native to SQL can be replicated in Pandas as well (but as you will see in tutorial, there are a few things simplified from using SQL!)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f228b39e",
   "metadata": {},
   "source": [
    "### Install packages, do updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577e6771",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# install the sql package\n",
    "!pip install ipython-sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6913fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#alternatively, we can use a tool like sqlalchemy to explicity define the connection\n",
    "!pip install SQLAlchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bca13bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: sometimes an older version of Pandas can throw errors\n",
    "pip install --upgrade pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ff9edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define imports\n",
    "\n",
    "# main library we will be using is sqlite3\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# alternative library to work with SQL directly in Pandas\n",
    "import sqlalchemy\n",
    "\n",
    "# Configure Notebook\n",
    "%matplotlib inline\n",
    "plt.style.use('fivethirtyeight')\n",
    "sns.set_context(\"notebook\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b348e582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ipython magic command - load the SQL extension.\n",
    "# Needs to be done for sqlite!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9e3e1d",
   "metadata": {},
   "source": [
    "If you already loaded SQL during your kernel run, you would use `%reload_ext sql`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ac53fd",
   "metadata": {},
   "source": [
    "### Writing SQL in Python\n",
    "\n",
    "`%%sql%%` is a magic Python commond (iPython) that will allow you to write and execute SQL code in a Jupyter notebook cell. One side note, the entire cell will be treated as SQL code when you write this command, including comments!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08b9009",
   "metadata": {},
   "source": [
    "First, we want to load the data, we do this by calling `sqlite:///` and then giving the path of the `.db` file relative to the notebook (for example `sqlite:///chinook.db`), or by directly providing a server connection string here (`postgresql://postgres:password123@localhost/dvdrental`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb7fed0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ad02f3cf",
   "metadata": {},
   "source": [
    "**Important Note:** In magic SQL, the database connections will remain open until you restart your kernel! Can explicitly close connections to, which we will show later in the tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a3ef72",
   "metadata": {},
   "source": [
    "Great! Now that we are connected, we can start running queries. What tables do we have in `chinook.db`? One side note, comments are delineated by `--` in SQL. (in some versions of SQL, `\\\\` also works, but `--` is pretty universal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4939f70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "select * from sqlite_master where type='table'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6264759",
   "metadata": {},
   "source": [
    "Let's explain this table - the `tbl_name` has the names of the different tables, and the `sql` column directly shows us the SQL code that was used to construct the table. In the `sql` column, we can see the different column names, as well as the variable types that are allowed within the columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c604d99b",
   "metadata": {},
   "source": [
    "Let's write our first SQL query, using the `SELECT` keyword (which will bring us back a view-only form of the table). In SQL, we always specify the column names (which is what `SELECT` will apply too and the table name\n",
    "\n",
    "*Syntax*: `SELECT` **columns** `FROM` **table** `;` \n",
    "\n",
    "When we want to select **all** the columns in a table, we use the `*` symbol. For instance, say we wanted to select all the columns from the table `media_types`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d23c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc857266",
   "metadata": {},
   "source": [
    "Another common practice is to indent the code and start new parts of the SQL command on new lines, as well as capitalize the key SQL commands (in this case, `SELECT` and `FROM` - case sensitivity is not an issue in SQL!). This is to improve the readibility of the query (as they can get quite complicated and long!). Below is our same code reformatted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946edd8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e127ee54",
   "metadata": {},
   "source": [
    "One note: by default the SQL query will bring back the whole table, so we use the `LIMIT` to bring back only the first selected numbers of rows (equivalent to `df.head()` from Pandas!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c031c946",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9c284316",
   "metadata": {},
   "source": [
    "## Common Expressions and Operations in SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09301ca6",
   "metadata": {},
   "source": [
    "If we wanted to just bring back the columns `Address`, `City` and `State` columns, our query would change like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82053ba0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ed45655a",
   "metadata": {},
   "source": [
    "As well, we can also `ORDER BY` a particular column. If we wanted to do the equivalent in pandas, our command would look something like this (and **case-sensitivity** would also be important in Python for column/table names, notice that this is not important in SQL):\n",
    "\n",
    "`df = df[[Address, City]].sort_values(by=['City'])`\n",
    "\n",
    "`df.head()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44871b02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a4c1af1c",
   "metadata": {},
   "source": [
    "A row-level filter we can apply is the `WHERE` key-word, applied after the table is selected using the `FROM` key-word. Mnemonically, this is much simpler to remember compared to pandas filtering functions. The ordering in SQL also guarantees less anomalous behaviour, while also being computationally efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcae1b99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f74f8ee9",
   "metadata": {},
   "source": [
    "Let's look at the `Invoices` table now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e2cb8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "04ce6438",
   "metadata": {},
   "source": [
    "It is important to note the different data types in SQL:\n",
    "- `INT`: same as in Python\n",
    "- `REAL`: equivalent to float in Python\n",
    "- `TEXT`: umbrella for nvarchar, usually has limitation on length (for example, nvarchar48 can be a string of up to 48 characters).\n",
    "- `BLOB`: Binary Large Objects, can be any type of file (mp3 files, pdfs, other database tables) or any type of data entry.\n",
    "- `DATETIME`: same as Python\n",
    "\n",
    "It is important to be mindful of the data-type when exeucuting the `WHERE` condition. Let's do a numerical `WHERE` condition instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0b73cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "25411e0b",
   "metadata": {},
   "source": [
    "If we treated the `Total` column like a string instead, the string query would still execute, notice the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88f4708",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebdf0ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c6abc4dd",
   "metadata": {},
   "source": [
    "You can also have more than one filter condition available! Notice that the filtering below is occuring on different columns (`total` and `BillingCity`) than those returned in the table (`invoiceId` and `total`). If we tried to do something like this in pandas, our query would look like so:\n",
    "\n",
    "`df_filtered = df.loc[(df['total'] > 1.5) | (df['BillingState'] == 'AB'),['invoiceId','total','CustomerId']]`\n",
    "\n",
    "while you get the same result, in the SQL query it is a bit easier to understand the operations and the way the queried results will be returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9672ac2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2615328c",
   "metadata": {},
   "source": [
    "Let's sort these results using the `ORDER BY` keyword that we learned earlier, we can also add additional arguments - ascending `ASC` or descending `DESC`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c530579",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f7146b27",
   "metadata": {},
   "source": [
    "Note that there is no equivalent to the Pandas `.tail()`, but we can use a combination of `ORDER BY` and `LIMIT` keywords to acheive the same result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbe0dff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d3970cbf",
   "metadata": {},
   "source": [
    "Let's say you wanted to omit the first (or last) few results when you presented your ordered dataframe. You can use the `OFFSET` command to skip over a few entries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c4b952",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "02d27dc5",
   "metadata": {},
   "source": [
    "Especially when working with a given dataset, you might have long column or table names! To simplify creating SQL queries, you can renaming entities with the `AS` keyword, when the object is first called/defined. In this example:\n",
    "- the `invoices` table becomes `i`\n",
    "- the columns are renamed:\n",
    "    - `invoiceId` as `inv`\n",
    "    - `total` as `tot`\n",
    "    - `CustomerId` as `cust` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f65d48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "007cb17b",
   "metadata": {},
   "source": [
    "To convert a data-type, the `CAST` SQL command is used, with the `AS` keyword also applied to specify the desired data-type. It can be a good practice to introduce casting when joining tables if not sure that they share the same data-type, or importing data from a source like a csv (that traditionally keeps data-types as strings):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ebe697",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1baffeca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d6c21c12",
   "metadata": {},
   "source": [
    "What is we wanted to get customers only, or perform an aggregation to get some statistics? Similar to pandas, we can use the `GROUP BY` key-words to help us out here.\n",
    "\n",
    "First, let's use both `SELECT` and `GROUP BY` to see all the `CustomerId` entries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8035d0e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fb8ad925",
   "metadata": {},
   "source": [
    "When we `SELECT` all the columns, the default value returned from the `GROUP BY` for a column will be the top \"rolled up\" result (i.e. the latest entry that has that specified group value):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef0e652",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f833cd8b",
   "metadata": {},
   "source": [
    "We can have multiple advanced aggregation optionms, like so (and we can always rename using `AS` same as before, to layer our queries):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992cab31",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "487151b2",
   "metadata": {},
   "source": [
    "No simple equivalent for something like this in Pandas! Additionally, multiple columns can be grouped by, and an aggregation can be specified too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c59db39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c8f4e12e",
   "metadata": {},
   "source": [
    "Another useful aggregation we can include is `COUNT` - there are a few ways that we can use this:\n",
    "- Number of records in a table or for a specific condition\n",
    "- Aggregated count (equivalent to use `.len()` functions in Python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93c1614",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d074a86b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514744f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0983f04f",
   "metadata": {},
   "source": [
    "Two other common aggregate functions in addition to `COUNT`, are `SUM` and `AVG` (just be careful of using numeric data-types, otherwise might get an error or unexpected results):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0f2598",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa69cd0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cd519e92",
   "metadata": {},
   "source": [
    "You can also do a `GROUPBY` by multiple columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e22a59",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cfe4bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "260daa17",
   "metadata": {},
   "source": [
    "You may have noticed that SQL operators follow a specifc order in a query. When we do a `GROUP BY`, an important observation is that `WHERE` can no longer be applied to for filtering.\n",
    "\n",
    "Instead, we need to rely on a new key-word, `HAVING`, which will do this row-based filtering on the groups (this would be similar to writing something like `.groupby(\"type\").filter(lambda f: max(f[\"cost\"]) < 8)` in Pandas).\n",
    "\n",
    "To summarize, for **rows we use `WHERE`, for groups we use `HAVING`**.\n",
    "\n",
    "<img src=\"order_operations.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1aafcf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c56f26",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "836ae63a",
   "metadata": {},
   "source": [
    "If we want to return distinct results over specific columns, we can `DISTINCT` keyword:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738cdd5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "03612d10",
   "metadata": {},
   "source": [
    "Here is the difference from adding the `DISTINCT` keyword:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c622a8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fca361",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99d018b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b5e521",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1d6c590e",
   "metadata": {},
   "source": [
    "A `JOIN` clause is used to combine rows from two or more tables, based on a related column between them, with the additional `ON` keyword. Let's combine two tables based on the figure below, `tracks` and `media_types`.\n",
    "\n",
    "![Database Tables](sqllite_tables.jpeg)\n",
    "\n",
    "But first, let's comment on the different types of joins available in SQL. The syntax will look different in the query, but the terminology is the same as what we've used in `pd.merge`:\n",
    "- `(INNER) JOIN`: Returns records that have matching values in both tables\n",
    "- `LEFT (OUTER) JOIN`: Returns all records from the left table, and the matched records from the right table\n",
    "- `RIGHT (OUTER) JOIN`: Returns all records from the right table, and the matched records from the left table\n",
    "- `FULL (OUTER) JOIN`: Returns all records when there is a match in either left or right table\n",
    "\n",
    "![Join Types](join_types.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6baee23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cfef2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "944b89cb",
   "metadata": {},
   "source": [
    "We want to join our table on the `MediaTypeId` column found in both tables. One more neat trick we can do in SQL - we can prefix the table name to indicate which columns we want to bring back in our view, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169797ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8374ff71",
   "metadata": {},
   "source": [
    "**ADVANCED SQL: `GROUP BY` and `JOIN`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590c8b80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6647320a",
   "metadata": {},
   "source": [
    "Other things we can see in SQL: \n",
    "- Views, \n",
    "- Stored Procedures\n",
    "- Schemas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b126dd",
   "metadata": {},
   "source": [
    "Two important summary points:\n",
    "\n",
    "<img src=\"generic_query.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee7ffa1",
   "metadata": {},
   "source": [
    "# Storing Results into Python\n",
    "\n",
    "Use the `<<` syntax, in the `%%sql` magic command. All SQL queries will return a table or a series with an index, which can then be converted to a `Pandas` or `NumPy` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13f524f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93cda32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93706fde",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "756f92ce",
   "metadata": {},
   "source": [
    "Let's try to use some `Pandas` commands on our dataframe!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7008ee5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# maybe let's first try to print out top 4 entries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7769d26",
   "metadata": {},
   "source": [
    "Oops! We need to explicity create a Pandas dataframe object to use the Pandas functions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba3a2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# need to explicitly define that you want to make a pandas dataframe! Note that an index will be created \n",
    "# but you can specify other arguments in pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a661ece",
   "metadata": {},
   "source": [
    "Looking good!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3caca086",
   "metadata": {},
   "source": [
    "## Pandas knows SQL!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb567a03",
   "metadata": {},
   "source": [
    "You can pass an entire SQL query using doc-string comment format ` \"\"\" \"\"\"`. Alternatively, Pandas has a `pd.read_sql()` command that can be used!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a460e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "select CustomerId, BillingCountry, InvoiceDate, sum(total), max(total), min(total)\n",
    "    from invoices\n",
    "group by CustomerId\n",
    "limit 20;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a85fe54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f48c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#alternatively, we can use a tool like sqlalchemy to explicity define the connection\n",
    "!pip install SQLAlchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a1b851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: sometimes an older version of Pandas can throw errors\n",
    "!pip install --upgrade pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d74158",
   "metadata": {},
   "source": [
    "Here is an alternative way using `pd.read_sql()`. The advantage of this is that your output is already put into a pandas dataframe for you, which let's you combine SQL and Pandas functions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb3ff21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# define your connection in sqlalchemy\n",
    "engine = sqlalchemy.create_engine(\"sqlite:///chinook.db\")\n",
    "connection = engine.connect()\n",
    "\n",
    "# write your query in a doc-string\n",
    "query = \"\"\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# pass both the query and the database connection into pd.read_sql\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777ef184",
   "metadata": {},
   "source": [
    "## Closing a SQL Connection\n",
    "\n",
    "Connections are an expensive compute resource, and you might also risk losing your data if you unknowingly leave a connection open. As well, an open connection can cause serious performance issues in the cases that you have several users trying to access a database, and also be a potential for a data breach, through other users being able to access the original table code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a122f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save available connections to a dictionary\n",
    "connection_dict = %sql --connections \n",
    "connection_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16402ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the first connection string from the dictionary\n",
    "connection_string = list(connection_dict.keys())[0] \n",
    "\n",
    "# get the connection object from the dictionary using the connection string\n",
    "connection_object = connection_dict[connection_string]\n",
    "\n",
    "# print connection\n",
    "print(connection_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ff54ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# close the connection using the connection object's url attribute\n",
    "%sql --close $connection_object.url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53699367",
   "metadata": {},
   "source": [
    "Let's check!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6208f668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check by rerunning the same dictionary from step 1\n",
    "connection_dict = %sql --connections \n",
    "connection_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6192062d",
   "metadata": {},
   "source": [
    "# Let's Create Our Own Database!\n",
    "\n",
    "Let's revisit the example from Week 3 Lecture 2 - the IMDB Web-Scraping example. Imagine you wanted to scrape this data, and then save this into a database, rather than individual csv files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbadaee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import 3rd party libraries\n",
    "import os\n",
    "import json \n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from bs4 import BeautifulSoup\n",
    "import matplotlib.pyplot as plt\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "url = \"https://www.goodreads.com/list/show/2681.Time_Magazine_s_All_Time_100_Novels\"\n",
    "\n",
    "response = requests.get(url)\n",
    "print(response.text[0:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49dee74",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# define our class that stores the html object\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "books = soup\n",
    "\n",
    "# retreive the book and authors\n",
    "titles = books.find_all('a', 'bookTitle')\n",
    "authors = books.find_all('a', 'authorName')\n",
    "\n",
    "# define lists to store cleaned information\n",
    "book_titles = [book.text.strip('\\n') for book in titles]\n",
    "author_names = [author.text for author in authors]\n",
    "rating = [(i+1) for i in range(len(book_titles))]\n",
    "\n",
    "# create dataframe with the lists\n",
    "goodreads_df = pd.DataFrame({\n",
    "    'titles': book_titles,\n",
    "    'authors': author_names,\n",
    "    'rank': rating\n",
    "})    \n",
    "\n",
    "goodreads_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de93f4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "goodreads_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164502a5",
   "metadata": {},
   "source": [
    "First, let's create our database object! When we connect to a new database object, it will automatically create this empty file for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0079011",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc914923",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef96df49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the first connection string from the dictionary\n",
    "connection_string = list(connection_dict.keys())[0] \n",
    "\n",
    "# get the connection object from the dictionary using the connection string\n",
    "connection_object = connection_dict[connection_string]\n",
    "\n",
    "# print connection\n",
    "print(connection_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee76994",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ae0da40e",
   "metadata": {},
   "source": [
    "Let's check that this table exists in our database!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d624765",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4ae69e75",
   "metadata": {},
   "source": [
    "Let's create a second table to write into, our database, this time using the top 100 books according to the Guardian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555bdde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get url\n",
    "url = \"https://www.theguardian.com/world/2002/may/08/books.booksnews\"\n",
    "\n",
    "# preview results\n",
    "response = requests.get(url)\n",
    "print(response.text[0:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b8b382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define our class that stores the html object\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "books = soup\n",
    "\n",
    "# retreive the book and authors\n",
    "titles = books.find_all('p', 'dcr-19m3vvb')\n",
    "\n",
    "print(titles[1].text)\n",
    "\n",
    "# define lists to store cleaned information\n",
    "# book_titles = [book.text.split('by')[0].strip() for book in titles[:99]]\n",
    "# author_names = [book.text.split('by')[-1].split(',')[0].strip() for book in titles[:99]]\n",
    "# years = [book.text.split('(')[-1].split(')')[0].strip() for book in titles[:99]]\n",
    "# rating = [(i+1) for i in range(len(book_titles))]\n",
    "\n",
    "# # create dataframe with the lists\n",
    "# guardian_df = pd.DataFrame({\n",
    "#     'titles': book_titles,\n",
    "#     'authors': author_names,\n",
    "#     'years': years,\n",
    "#     'rank': rating\n",
    "# })    \n",
    "\n",
    "# guardian_df.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674c6796",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "df563c3d",
   "metadata": {},
   "source": [
    "Let's preview the `guardian` table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab22fead",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "be60c354",
   "metadata": {},
   "source": [
    "Now let's join our two tables! Let's see what books are in common between the Guardian and Goodreads:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996573de",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea11f07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "41712970",
   "metadata": {},
   "source": [
    "Interesting - only 9 of the titles are shared between the two sources. Let's create a new table from this join!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4599c683",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9b7ff2ac",
   "metadata": {},
   "source": [
    "Let's try to preview our table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df80ffee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f019f9a8",
   "metadata": {},
   "source": [
    "Let's make one more simple table for genres. This time, we will create a table by defining column names, then using the `INSERT` keyword to add tuples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac75ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "CREATE TABLE genres(id integer, genre varchar(200));\n",
    "INSERT into genres(id, genre) values(1, \"Horror\");\n",
    "INSERT into genres(id, genre) values(2, \"Classical\");\n",
    "INSERT into genres(id, genre) values(3, \"Romance\");\n",
    "INSERT into genres(id, genre) values(4, \"History\");\n",
    "INSERT into genres(id, genre) values(5, \"Fiction\");\n",
    "INSERT into genres(id, genre) values(6, \"Adventure\");\n",
    "\n",
    "SELECT * FROM genres;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c616ac94",
   "metadata": {},
   "source": [
    "Let's check all the tables that are in our database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03584ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT * \n",
    "FROM sqlite_master \n",
    "WHERE type='table';"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1692e917",
   "metadata": {},
   "source": [
    "And finally, close the connection, same as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb69ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# close the connection using the connection object's url attribute\n",
    "%sql --close $connection_object.url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d0ad1c",
   "metadata": {},
   "source": [
    "The End!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
